# SOME DESCRIPTIVE TITLE.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: https://github.com/belphegor-belbel/doc-opensuse-ja\n"
"POT-Creation-Date: 2026-01-16 22:16+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <someuser@example.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/x-po; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. Tag: title
#: tuning_network.xml:20
#, no-c-format
msgid "Tuning the network"
msgstr ""

#. Tag: meta
#: tuning_network.xml:22
#, no-c-format
msgid "Learn to tune Linux networking by configuring kernel socket buffers, TCP autotuning, Netfilter and RPS settings"
msgstr ""

#. Tag: date
#: tuning_network.xml:30
#, no-c-format
msgid "2024-06-25"
msgstr ""

#. Tag: para
#: tuning_network.xml:37
#, no-c-format
msgid "The network subsystem is complex and its tuning highly depends on the system use scenario and on external factors such as software clients or hardware components (switches, routers, or gateways) in your network. The Linux kernel aims more at reliability and low latency than low overhead and high throughput. Other settings can mean less security, but better performance."
msgstr ""

#. Tag: title
#: tuning_network.xml:46
#, no-c-format
msgid "Configurable kernel socket buffers"
msgstr ""

#. Tag: para
#: tuning_network.xml:48
#, no-c-format
msgid "Most of modern networking is based on the TCP/IP protocol and a socket interface for communication; for more information about TCP/IP, see <xref linkend=\"cha-network\"/> . The Linux kernel handles data it receives or sends via the socket interface in socket buffers. These kernel socket buffers are tunable."
msgstr ""

#. Tag: title
#: tuning_network.xml:58
#, no-c-format
msgid "TCP autotuning"
msgstr ""

#. Tag: para
#: tuning_network.xml:59
#, no-c-format
msgid "Since kernel version 2.6.17 full autotuning with 4 MB maximum buffer size exists. This means that manual tuning does not improve networking performance considerably. It is often the best not to touch the following variables, or, at least, to check the outcome of tuning efforts carefully."
msgstr ""

#. Tag: para
#: tuning_network.xml:66
#, no-c-format
msgid "If you update from an older kernel, it is recommended to remove manual TCP tunings in favor of the autotuning feature."
msgstr ""

#. Tag: para
#: tuning_network.xml:72
#, no-c-format
msgid "The special files in the <filename>/proc</filename> file system can modify the size and behavior of kernel socket buffers; for general information about the <filename>/proc</filename> file system, see <xref linkend=\"sec-util-proc\"/> . Find networking related files in:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:79
#, no-c-format
msgid "/proc/sys/net/core\n"
      "/proc/sys/net/ipv4\n"
      "/proc/sys/net/ipv6"
msgstr ""

#. Tag: para
#: tuning_network.xml:83
#, no-c-format
msgid "General <systemitem>net</systemitem> variables are explained in the kernel documentation ( <filename>linux/Documentation/sysctl/net.txt</filename> ). Special <systemitem>ipv4</systemitem> variables are explained in <filename>linux/Documentation/networking/ip-sysctl.txt</filename> and <filename>linux/Documentation/networking/ipvs-sysctl.txt</filename> ."
msgstr ""

#. Tag: para
#: tuning_network.xml:92
#, no-c-format
msgid "In the <filename>/proc</filename> file system, for example, it is possible to either set the Maximum Socket Receive Buffer and Maximum Socket Send Buffer for all protocols, or both these options for the TCP protocol only (in <filename>ipv4</filename> ) and thus overriding the setting for all protocols (in <filename>core</filename> )."
msgstr ""

#. Tag: term
#: tuning_network.xml:102
#, no-c-format
msgid "<filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:105
#, no-c-format
msgid "If <filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename> is set to <literal>1</literal> , autotuning is active and buffer size is adjusted dynamically."
msgstr ""

#. Tag: term
#: tuning_network.xml:113
#, no-c-format
msgid "<filename>/proc/sys/net/ipv4/tcp_rmem</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:116
#, no-c-format
msgid "The three values setting the minimum, initial, and maximum size of the Memory Receive Buffer per connection. They define the actual memory usage, not only TCP window size."
msgstr ""

#. Tag: term
#: tuning_network.xml:124
#, no-c-format
msgid "<filename>/proc/sys/net/ipv4/tcp_wmem</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:127
#, no-c-format
msgid "The same as <filename>tcp_rmem</filename> , but for Memory Send Buffer per connection."
msgstr ""

#. Tag: term
#: tuning_network.xml:134
#, no-c-format
msgid "<filename>/proc/sys/net/core/rmem_max</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:137
#, no-c-format
msgid "Set to limit the maximum receive buffer size that applications can request."
msgstr ""

#. Tag: term
#: tuning_network.xml:144
#, no-c-format
msgid "<filename>/proc/sys/net/core/wmem_max</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:147
#, no-c-format
msgid "Set to limit the maximum send buffer size that applications can request."
msgstr ""

#. Tag: para
#: tuning_network.xml:155
#, no-c-format
msgid "Via <filename>/proc</filename> it is possible to disable TCP features that you do not need (all TCP features are switched on by default). For example, check the following files:"
msgstr ""

#. Tag: term
#: tuning_network.xml:163
#, no-c-format
msgid "<filename>/proc/sys/net/ipv4/tcp_timestamps</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:166
#, no-c-format
msgid "TCP time stamps are defined in RFC1323."
msgstr ""

#. Tag: term
#: tuning_network.xml:172
#, no-c-format
msgid "<filename>/proc/sys/net/ipv4/tcp_window_scaling</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:175
#, no-c-format
msgid "TCP window scaling is also defined in RFC1323."
msgstr ""

#. Tag: term
#: tuning_network.xml:181
#, no-c-format
msgid "<filename>/proc/sys/net/ipv4/tcp_sack</filename>"
msgstr ""

#. Tag: para
#: tuning_network.xml:184
#, no-c-format
msgid "Select acknowledgments (SACKS)."
msgstr ""

#. Tag: para
#: tuning_network.xml:191
#, no-c-format
msgid "Use <command>sysctl</command> to read or write variables of the <filename>/proc</filename> file system. <command>sysctl</command> is preferable to <command>cat</command> (for reading) and <command>echo</command> (for writing), because it also reads settings from <filename>/etc/sysctl.conf</filename> and, thus, those settings survive reboots reliably. With <command>sysctl</command> you can read all variables and their values easily; as &rootuser; use the following command to list TCP related settings:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:202
#, no-c-format
msgid "&prompt.sudo;sysctl -a | grep tcp"
msgstr ""

#. Tag: title
#: tuning_network.xml:208
#, no-c-format
msgid "Side effects of tuning network variables"
msgstr ""

#. Tag: para
#: tuning_network.xml:209
#, no-c-format
msgid "Tuning network variables can affect other system resources such as CPU or memory use. <!-- (p.124)--> <!-- Also see \"Tuning TCP behavior\", ibm p. 130 -->"
msgstr ""

#. Tag: title
#: tuning_network.xml:220
#, no-c-format
msgid "Detecting network bottlenecks and analyzing network traffic"
msgstr ""

#. Tag: para
#: tuning_network.xml:222
#, no-c-format
msgid "Before starting with network tuning, it is important to isolate network bottlenecks and network traffic patterns. There are certain tools that can help you with detecting those bottlenecks."
msgstr ""

#. Tag: para
#: tuning_network.xml:228
#, no-c-format
msgid "The following tools can help analyzing your network traffic: <command>netstat</command> , <command>tcpdump</command> , and <command>wireshark</command> . Wireshark is a network traffic analyzer."
msgstr ""

#. Tag: title
#: tuning_network.xml:243
#, no-c-format
msgid "Netfilter"
msgstr ""

#. Tag: para
#: tuning_network.xml:245
#, no-c-format
msgid "The Linux firewall and masquerading features are provided by the Netfilter kernel modules. This is a highly configurable rule based framework. If a rule matches a packet, Netfilter accepts or denies it or takes special action ( <quote>target</quote> ) as defined by rules such as address translation."
msgstr ""

#. Tag: para
#: tuning_network.xml:253
#, no-c-format
msgid "There are many properties Netfilter can take into account. Thus, the more rules are defined, the longer packet processing may last. Also advanced connection tracking could be rather expensive and, thus, slowing down overall networking."
msgstr ""

#. Tag: para
#: tuning_network.xml:262
#, no-c-format
msgid "When the kernel queue becomes full, all new packets are dropped, causing existing connections to fail. The 'fail-open' feature allows a user to temporarily disable the packet inspection and maintain the connectivity under heavy network traffic. For reference, see <link xlink:href=\"https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/\"/> ."
msgstr ""

#. Tag: para
#: tuning_network.xml:269
#, no-c-format
msgid "For more information, see the home page of the Netfilter and iptables project, <link xlink:href=\"https://www.netfilter.org\"/> ."
msgstr ""

#. Tag: title
#: tuning_network.xml:275
#, no-c-format
msgid "Improving the network performance with receive packet steering (RPS)"
msgstr ""

#. Tag: para
#: tuning_network.xml:277
#, no-c-format
msgid "Modern network interface devices can move so many packets that the host can become the limiting factor for achieving maximum performance. To keep up, the system must be able to distribute the work across multiple CPU cores."
msgstr ""

#. Tag: para
#: tuning_network.xml:284
#, no-c-format
msgid "Some modern network interfaces can help distribute the work to multiple CPU cores through the implementation of multiple transmission and multiple receive queues in hardware. However, others are only equipped with a single queue and the driver must deal with all incoming packets in a single, serialized stream. To work around this issue, the operating system must <quote>parallelize</quote> the stream to distribute the work across multiple CPUs. On &productname; this is done via Receive Packet Steering (RPS). RPS can also be used in virtual environments."
msgstr ""

#. Tag: para
#: tuning_network.xml:295
#, no-c-format
msgid "RPS creates a unique hash for each data stream using IP addresses and port numbers. The use of this hash ensures that packets for the same data stream are sent to the same CPU, which helps to increase performance."
msgstr ""

#. Tag: para
#: tuning_network.xml:301
#, no-c-format
msgid "RPS is configured per network device receive queue and interface. The configuration file names match the following scheme:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:306
#, no-c-format
msgid "/sys/class/net/<replaceable>&lt;device&gt;</replaceable>/queues/<replaceable>&lt;rx-queue&gt;</replaceable>/rps_cpus"
msgstr ""

#. Tag: para
#: tuning_network.xml:308
#, no-c-format
msgid "<replaceable>&lt;device&gt;</replaceable> stands for the network device, such as <literal>eth0</literal> , <literal>eth1</literal> . <replaceable>&lt;rx-queue&gt;</replaceable> stands for the receive queue, such as <literal>rx-0</literal> , <literal>rx-1</literal> ."
msgstr ""

#. Tag: para
#: tuning_network.xml:315
#, no-c-format
msgid "If the network interface hardware only supports a single receive queue, only <literal>rx-0</literal> exists. If it supports multiple receive queues, there is an rx- <replaceable>N</replaceable> directory for each receive queue."
msgstr ""

#. Tag: para
#: tuning_network.xml:322
#, no-c-format
msgid "These configuration files contain a comma-delimited list of CPU bitmaps. By default, all bits are set to <literal>0</literal> . With this setting, RPS is disabled and therefore the CPU that handles the interrupt also processes the packet queue."
msgstr ""

#. Tag: para
#: tuning_network.xml:328
#, no-c-format
msgid "To enable RPS and enable specific CPUs to process packets for the receive queue of the interface, set the value of their positions in the bitmap to <literal>1</literal> . For example, to enable CPUs 0-3 to process packets for the first receive queue for eth0, set the bit positions 0-3 to 1 in binary: <literal>00001111</literal> . This representation then needs to be converted to hex&mdash;which results in <literal>F</literal> in this case. Set this hex value with the following command:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:338
#, no-c-format
msgid "&prompt.sudo;echo \"f\" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus"
msgstr ""

#. Tag: para
#: tuning_network.xml:340
#, no-c-format
msgid "If you wanted to enable CPUs 8-15:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:344
#, no-c-format
msgid "1111 1111 0000 0000 (binary)\n"
      "15     15    0    0 (decimal)\n"
      "F       F    0    0 (hex)"
msgstr ""

#. Tag: para
#: tuning_network.xml:348
#, no-c-format
msgid "The command to set the hex value of <literal>ff00</literal> would be:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:352
#, no-c-format
msgid "&prompt.sudo;echo \"ff00\" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus"
msgstr ""

#. Tag: para
#: tuning_network.xml:354
#, no-c-format
msgid "On NUMA machines, best performance can be achieved by configuring RPS to use the CPUs on the same NUMA node as the interrupt for the interface's receive queue."
msgstr ""

#. Tag: para
#: tuning_network.xml:360
#, no-c-format
msgid "On non-NUMA machines, all CPUs can be used. If the interrupt rate is high, excluding the CPU handling the network interface can boost performance. The CPU being used for the network interface can be determined from <filename>/proc/interrupts</filename> . For example:"
msgstr ""

#. Tag: screen
#: tuning_network.xml:366
#, no-c-format
msgid "&prompt.sudo;cat /proc/interrupts\n"
      "            CPU0       CPU1       CPU2       CPU3\n"
      "...\n"
      "  51:  113915241          0          0          0      Phys-fasteoi   eth0\n"
      "..."
msgstr ""

#. Tag: para
#: tuning_network.xml:372
#, no-c-format
msgid "In this case, <literal>CPU 0</literal> is the only CPU processing interrupts for <literal>eth0</literal> , since only <literal>CPU0</literal> contains a non-zero value."
msgstr ""

#. Tag: para
#: tuning_network.xml:378
#, no-c-format
msgid "On &x86; and &x86-64; platforms, <command>irqbalance</command> can be used to distribute hardware interrupts across CPUs. See <command>man 1 irqbalance</command> for more details."
msgstr ""

