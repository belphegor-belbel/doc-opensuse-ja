# translation of tuning_memory.xml.po to Japanese
# Japanese translations for PACKAGE package
# PACKAGE パッケージに対する英訳.
#
# Automatically generated, 2018.
# Yasuhiko Kamata <belphegor@belbel.or.jp>, 2019, 2021, 2023.
msgid ""
msgstr ""
"Project-Id-Version: tuning_memory.xml\n"
"Report-Msgid-Bugs-To: https://github.com/belphegor-belbel/doc-opensuse-ja\n"
"POT-Creation-Date: 2023-04-19 22:11+0000\n"
"PO-Revision-Date: 2023-04-20 07:14+0900\n"
"Last-Translator: Yasuhiko Kamata <belphegor@belbel.or.jp>\n"
"Language-Team: Japanese <opensuse-ja@opensuse.org>\n"
"Language: ja\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Generator: KBabel 1.11.4\n"

#. Tag: title
#: tuning_memory.xml:45
#, no-c-format
msgid "Tuning the memory management subsystem"
msgstr "メモリ管理サブシステムのチューニング"

#. Tag: para
#: tuning_memory.xml:53
#, no-c-format
msgid ""
"To understand and tune the memory management behavior of the kernel, it is "
"important to first have an overview of how it works and cooperates with "
"other subsystems."
msgstr ""
"カーネルのメモリ管理動作を理解してチューニングするには、まず動作に関する概要"
"と、他のサブシステムとの連携を理解するところから始めるのがよいでしょう。"

#. Tag: para
#: tuning_memory.xml:58
#, no-c-format
msgid ""
"<remark>sknorr, 2014-08-21: Using VM for \"virtual memory manager\" is "
"really dangerous - everyone knows that VM means \"virtual machine\" and half "
"of the people reading this chapter might skip this prargraph and be "
"completely confused. At least use VMM or shorten \"memory management "
"subsystem\" to \"memory subsystem\", the latter of which I think should be "
"short enough. </remark> The memory management subsystem, also called the "
"virtual memory manager, will subsequently be called <quote>VM</quote> . The "
"role of the VM is to manage the allocation of physical memory (RAM) for the "
"entire kernel and user programs. It is also responsible for providing a "
"virtual memory environment for user processes (managed via POSIX APIs with "
"Linux extensions). Finally, the VM is responsible for freeing up RAM when "
"there is a shortage, either by trimming caches or swapping out "
"<quote>anonymous</quote> memory."
msgstr ""
"<remark>sknorr, 2014-08-21: Using VM for \"virtual memory manager\" is "
"really dangerous - everyone knows that VM means \"virtual machine\" and half "
"of the people reading this chapter might skip this prargraph and be "
"completely confused. At least use VMM or shorten \"memory management "
"subsystem\" to \"memory subsystem\", the latter of which I think should be "
"short enough. </remark> メモリ管理サブシステムは仮想メモリマネージャとも呼ば"
"れ、下記では <quote>VM</quote> と略しています。 VM の役割は、カーネル全体と"
"ユーザプログラムに対する物理メモリ (RAM) の割り当て管理です。それだけではな"
"く、ユーザプロセスに対して、仮想メモリ環境を提供する責任も負っています "
"(Linux 拡張付きの POSIX API を介して管理します) 。最後に、 VM はメモリが枯渇"
"した場合に、キャッシュを解放したり <quote>匿名</quote> メモリをスワップアウト"
"したりすることで、メモリを空ける責任も負っています。"

#. Tag: para
#: tuning_memory.xml:74
#, no-c-format
msgid ""
"The most important thing to understand when examining and tuning VM is how "
"its caches are managed. The basic goal of the VM's caches is to minimize the "
"cost of I/O as generated by swapping and file system operations (including "
"network file systems). This is achieved by avoiding I/O completely, or by "
"submitting I/O in better patterns."
msgstr ""
"VM の調査やチューニングに際して理解しておくべき最重要事項は、キャッシュの管理"
"方法についてです。 VM のキャッシュの基本的なゴールは、スワップやファイルシス"
"テムの操作(ネットワークファイルシステムを含みます) を行うことによって生成され"
"る I/O のコストを、最小化することにあります。これは I/O を完全に排除するか、"
"もしくはよりよいパターンで I/O を送信することによって達成します。"

#. Tag: para
#: tuning_memory.xml:81
#, no-c-format
msgid ""
"Free memory will be used and filled up by these caches as required. The more "
"memory is available for caches and anonymous memory, the more effectively "
"caches and swapping will operate. However, if a memory shortage is "
"encountered, caches will be trimmed or memory will be swapped out."
msgstr ""
"空きメモリは、必要に応じてキャッシュとして使用されます。キャッシュや匿名メモ"
"リとして使用できるメモリ領域が大きければ大きいほど、より効率的にキャッシュや"
"スワップを制御できるようになります。しかしながら、いったんメモリが枯渇してし"
"まった場合は、キャッシュを解放するか、メモリをスワップアウトする必要がありま"
"す。"

#. Tag: para
#: tuning_memory.xml:88
#, no-c-format
msgid ""
"For a particular workload, the first thing that can be done to improve "
"performance is to increase memory and reduce the frequency that memory must "
"be trimmed or swapped. The second thing is to change the way caches are "
"managed by changing kernel parameters."
msgstr ""
"適切な負荷範囲であれば、性能を改善するにあたって最初にやるべきことは、メモリ"
"を増やしてキャッシュの効率を上げ、キャッシュの解放やスワップアウトの頻度を減"
"らすのがよいでしょう。次にやるべきことは、カーネルのパラメータを変更して、"
"キャッシュの管理方法を変えることです。"

#. Tag: para
#: tuning_memory.xml:94
#, no-c-format
msgid ""
"Finally, the workload itself should be examined and tuned as well. If an "
"application is allowed to run more processes or threads, effectiveness of VM "
"caches can be reduced, if each process is operating in its own area of the "
"file system. Memory overheads are also increased. If applications allocate "
"their own buffers or caches, larger caches will mean that less memory is "
"available for VM caches. However, more processes and threads can mean more "
"opportunity to overlap and pipeline I/O, and may take better advantage of "
"multiple cores. Experimentation will be required for the best results."
msgstr ""
"最後に、負荷それ自身についても調査してチューニングする必要があります。アプリ"
"ケーション側で多くのプロセスやスレッドを動作させることができる場合、各プロセ"
"スがファイルシステム内の独自の領域を利用していると、 VM の効率が落ちてしまい"
"ます。また、メモリによるオーバーヘッドも増えてしまいます。アプリケーション側"
"で独自のバッファやキャッシュを持っている場合、そのキャッシュを大きくしてしま"
"うと、 VM 側に割り当てることのできるサイズが小さくなってしまいます。しかしな"
"がら、プロセスやスレッドの数を増やすことで、 I/O の重複度合いやパイプライン処"
"理の機会が増えることがありますので、これによってマルチコア環境で性能が向上す"
"ることもあります。従って、最適な結果を得るためには、実験が必要ということにな"
"ります。"

#. Tag: title
#: tuning_memory.xml:106
#, no-c-format
msgid "Memory usage"
msgstr "メモリの用途"

#. Tag: para
#: tuning_memory.xml:108
#, no-c-format
msgid ""
"Memory allocations in general can be characterized as <quote>pinned</quote> "
"(also known as <quote>unreclaimable</quote> ), <quote>reclaimable</quote> or "
"<quote>swappable</quote> ."
msgstr ""
"メモリの割り当ては一般に、 <quote>ピン済み</quote> (<quote>回収不可</quote> "
"と呼ばれることもあります), <quote>回収可</quote>, <quote>スワップ可</quote> "
"に分類することができます。"

#. Tag: title
#: tuning_memory.xml:115
#, no-c-format
msgid "Anonymous memory"
msgstr "匿名メモリ"

#. Tag: para
#: tuning_memory.xml:116
#, no-c-format
msgid ""
"Anonymous memory tends to be program heap and stack memory (for example, "
"<literal>&gt;malloc()</literal> ). It is reclaimable, except in special "
"cases such as <literal>mlock</literal> or if there is no available swap "
"space. Anonymous memory must be written to swap before it can be reclaimed. "
"Swap I/O (both swapping in and swapping out pages) tends to be less "
"efficient than pagecache I/O, because of allocation and access patterns."
msgstr ""
"匿名メモリはプログラムのヒープやスタックメモリ (例: <literal>&gt;malloc()</"
"literal>) として使用されるメモリで、 <literal>mlock</literal> のような特殊"
"ケースや、利用可能なスワップ領域が存在しないような場合を除いて、回収可能なメ"
"モリとして位置づけられます。また、匿名メモリは回収される前にスワップに書き込"
"まなければなりません。また、スワップの I/O (ページのスワップインおよびスワッ"
"プアウト) は、割り当てとアクセスのパターンにより、ページキャッシュの I/O より"
"も効率が落ちる傾向があります。"

#. Tag: title
#: tuning_memory.xml:128
#, no-c-format
msgid "Pagecache"
msgstr "ページキャッシュ"

#. Tag: para
#: tuning_memory.xml:129
#, no-c-format
msgid ""
"A cache of file data. When a file is read from disk or network, the contents "
"are stored in pagecache. No disk or network access is required, if the "
"contents are up-to-date in pagecache. tmpfs and shared memory segments count "
"toward pagecache."
msgstr ""
"ファイルデータのキャッシュです。ディスクやネットワークからファイルが読み込ま"
"れると、その内容がページキャッシュ内に保存されます。ページキャッシュ内の内容"
"が最新のものであれば、ディスクやネットワークへのアクセスは不要となります。ま"
"た、 tmpfs や共有メモリセグメントについても、ページキャッシュとしてカウントし"
"ます。"

#. Tag: para
#: tuning_memory.xml:135
#, no-c-format
msgid ""
"When a file is written to, the new data is stored in pagecache before being "
"written back to a disk or the network (making it a write-back cache). When a "
"page has new data not written back yet, it is called <quote>dirty</quote> . "
"Pages not classified as dirty are <quote>clean</quote> . Clean pagecache "
"pages can be reclaimed if there is a memory shortage by simply freeing them. "
"Dirty pages must first be made clean before being reclaimed."
msgstr ""
"ファイルに対して書き込みが行われる場合、ディスクやネットワークに書き戻される "
"(つまりライトバックキャッシュを作成する) 前に、ページキャッシュにも保存が行わ"
"れます。まだディスクやネットワークに書き込まれていない新しいデータが存在する"
"場合、そのページは <quote>dirty</quote> であるとされます。逆に dirty ではない"
"ページは、 <quote>clean</quote> であるとされます。 clean やページキャッシュの"
"ページは、メモリが枯渇した場合、単純に解放するだけで回収することができます。 "
"dirty なページの場合は、回収できるようにするためにまず clean にしなければなり"
"ません。"

#. Tag: title
#: tuning_memory.xml:147
#, no-c-format
msgid "Buffercache"
msgstr "バッファキャッシュ"

#. Tag: para
#: tuning_memory.xml:148
#, no-c-format
msgid ""
"This is a type of pagecache for block devices (for example, /dev/sda). A "
"file system typically uses the buffercache when accessing its on-disk "
"metadata structures such as inode tables, allocation bitmaps, and so forth. "
"Buffercache can be reclaimed similarly to pagecache."
msgstr ""
"ブロックデバイス (例: /dev/sda) に対するページキャッシュの一種です。ファイル"
"システムでは、inode テーブルやアロケーションビットマップなど、ディスク内のメ"
"タデータにアクセスする際にバッファキャッシュを使用します。バッファキャッシュ"
"はページキャッシュと同様に回収することができます。"

#. Tag: title
#: tuning_memory.xml:157
#, no-c-format
msgid "Buffer heads"
msgstr "バッファヘッド"

#. Tag: para
#: tuning_memory.xml:158
#, no-c-format
msgid ""
"Buffer heads are small auxiliary structures that tend to be allocated upon "
"pagecache access. They can generally be reclaimed easily when the pagecache "
"or buffercache pages are clean."
msgstr ""
"バッファヘッドは小さな補助構造で、一般的にページキャッシュへのアクセス時に割"
"り当てられるものです。ページキャッシュやバッファキャッシュが clean であれば、"
"これらも一般に容易に回収することができます。"

#. Tag: title
#: tuning_memory.xml:166
#, no-c-format
msgid "Writeback"
msgstr "ライトバック"

#. Tag: para
#: tuning_memory.xml:167
#, no-c-format
msgid ""
"As applications write to files, the pagecache becomes dirty and the "
"buffercache may become dirty. When the amount of dirty memory reaches a "
"specified number of pages in bytes ( <emphasis>vm.dirty_background_bytes</"
"emphasis> ), or when the amount of dirty memory reaches a specific ratio to "
"total memory ( <emphasis>vm.dirty_background_ratio</emphasis> ), or when the "
"pages have been dirty for longer than a specified amount of time "
"( <emphasis>vm.dirty_expire_centisecs</emphasis> ), the kernel begins "
"writeback of pages starting with files that had the pages dirtied first. The "
"background bytes and ratios are mutually exclusive and setting one will "
"overwrite the other. Flusher threads perform writeback in the background and "
"allow applications to continue running. If the I/O cannot keep up with "
"applications dirtying pagecache, and dirty data reaches a critical setting "
"( <emphasis>vm.dirty_bytes</emphasis> or <emphasis>vm.dirty_ratio</"
"emphasis> ), then applications begin to be throttled to prevent dirty data "
"exceeding this threshold."
msgstr ""
"アプリケーションがファイルに書き込みを行う場合、ページキャッシュが dirty とな"
"るほか、バッファキャッシュについても必要に応じて dirty となります。 dirty な"
"メモリの量が指定したページ数 (バイト単位で <emphasis>vm."
"dirty_background_bytes</emphasis> で指定します) を越えるか、メモリの全体量と"
"の比率が指定した値 ( <emphasis>vm.dirty_background_ratio</emphasis> ) を越え"
"るか、もしくは指定した時間 ( <emphasis>vm.dirty_expire_centisecs</"
"emphasis> ) より長く dirty であり続けた場合、カーネルは最初に dirty になった"
"ページのファイルからページの書き戻しを始めます。なお、 bytes の設定と ratio "
"のパラメータは相互に排他な仕組みであり、一方を変更すると他方を上書きすること"
"になります。また、 flusher スレッドは裏で書き戻しを行う仕組みであるため、アプ"
"リケーションは通常通り動作し続けることができます。ただし、 I/O の速度よりもア"
"プリケーションがページを dirty にする速度のほうが早い場合、 dirty なデータが"
"致命的な水準 ( <emphasis>vm.dirty_bytes</emphasis> もしくは <emphasis>vm."
"dirty_ratio</emphasis> ) を越えると、アプリケーションの速度が抑制され、 "
"dirty なデータが過剰に生み出されないようにします。"

#. Tag: title
#: tuning_memory.xml:188
#, no-c-format
msgid "Readahead"
msgstr "先読み"

#. Tag: para
#: tuning_memory.xml:189
#, no-c-format
msgid ""
"The VM monitors file access patterns and may attempt to perform readahead. "
"Readahead reads pages into the pagecache from the file system that have not "
"been requested yet. It is done to allow fewer, larger I/O requests to be "
"submitted (more efficient). And for I/O to be pipelined (I/O performed at "
"the same time as the application is running)."
msgstr ""
"VM はファイルのアクセスパターンを監視し、必要に応じて先読みを行おうとします。"
"先読みはその名前の通り、まだ要求されていない範囲のものをファイルシステムから"
"ページキャッシュにページを読み込むものです。これは、より少ない回数でより大き"
"な I/O を送信できるようにする (これによって効率化を図る) ためのものです。ま"
"た、 I/O をパイプライン化する (アプリケーションの実行と同時に I/O を実施す"
"る) ためのものでもあります。"

#. Tag: title
#: tuning_memory.xml:200
#, no-c-format
msgid "VFS caches"
msgstr "VFS キャッシュ"

#. Tag: title
#: tuning_memory.xml:203
#, no-c-format
msgid "Inode cache"
msgstr "inode キャッシュ"

#. Tag: para
#: tuning_memory.xml:204
#, no-c-format
msgid ""
"This is an in-memory cache of the inode structures for each file system. "
"These contain attributes such as the file size, permissions and ownership, "
"and pointers to the file data."
msgstr ""
"これは各ファイルシステムに対する inode の構造をメモリ内にキャッシュしておくも"
"のです。この中にはファイルサイズやパーミッション、所有者情報やファイルデータ"
"に対するポインタなどを保持しています。"

#. Tag: title
#: tuning_memory.xml:211
#, no-c-format
msgid "Directory entry cache"
msgstr "ディレクトリエントリキャッシュ"

#. Tag: para
#: tuning_memory.xml:212
#, no-c-format
msgid ""
"This is an in-memory cache of the directory entries in the system. These "
"contain a name (the name of a file), the inode which it refers to, and "
"children entries. This cache is used when traversing the directory structure "
"and accessing a file by name."
msgstr ""
"これはシステム内でのディレクトリエントリのメモリ内キャッシュです。この中には"
"名前 (ファイル名) のほか、それが指し示す inode と、子エントリが含まれます。こ"
"のキャッシュは、ディレクトリ構造をたどる場合と、名前でファイルにアクセスする"
"場合に使用されるものです。"

#. Tag: title
#: tuning_memory.xml:222
#, no-c-format
msgid "Reducing memory usage"
msgstr "メモリ使用量の削減"

#. Tag: title
#: tuning_memory.xml:227
#, no-c-format
msgid "Reducing malloc (anonymous) usage"
msgstr "malloc (匿名) 利用の削減"

#. Tag: para
#: tuning_memory.xml:228
#, no-c-format
msgid ""
"Applications running on &productname; &productnumber; can allocate more "
"memory compared to older releases. This is because of <systemitem class="
"\"resource\">glibc</systemitem> changing its default behavior while "
"allocating user space memory. See <link xlink:href=\"http://www.gnu.org/s/"
"libc/manual/html_node/Malloc-Tunable-Parameters.html\"/> for explanation of "
"these parameters."
msgstr ""
"&productname; &productnumber; で動作しているアプリケーションは、以前のバー"
"ジョンに比べてより多くのメモリを割り当てることができます。これは <systemitem "
"class=\"resource\">glibc</systemitem> がユーザスペースメモリの割り当てに際し"
"て、既定の動作を変更したことによるものです。これらのパラメータについて、詳し"
"くは <link xlink:href=\"http://www.gnu.org/s/libc/manual/html_node/Malloc-"
"Tunable-Parameters.html\"/> (英語) をお読みください。"

#. Tag: para
#: tuning_memory.xml:236
#, no-c-format
msgid ""
"To restore behavior similar to older releases, M_MMAP_THRESHOLD should be "
"set to 128*1024. This can be done with mallopt() call from the application, "
"or via setting MALLOC_MMAP_THRESHOLD environment variable before running the "
"application."
msgstr ""
"以前のバージョンの動作に戻したい場合、 M_MMAP_THRESHOLD の値を 128*1024 に設"
"定する必要があります。これはアプリケーション側から mallopt() を利用することに"
"よって実現できるほか、アプリケーションの起動前に MALLOC_MMAP_THRESHOLD 環境変"
"数を設定しても実現することができます。"

#. Tag: title
#: tuning_memory.xml:245
#, no-c-format
msgid "Reducing kernel memory overheads"
msgstr "カーネルのメモリオーバーヘッドの削減"

#. Tag: para
#: tuning_memory.xml:246
#, no-c-format
msgid ""
"Kernel memory that is reclaimable (caches, described above) will be trimmed "
"automatically during memory shortages. Most other kernel memory cannot be "
"easily reduced but is a property of the workload given to the kernel."
msgstr ""
"回収可能なカーネルメモリ (上述のとおりキャッシュなど) については、メモリの枯"
"渇時に自動的に解放が行われます。その他のカーネルメモリについては容易に縮小す"
"ることができますが、カーネルに与えられた負荷の特性によって決まります。"

#. Tag: para
#: tuning_memory.xml:252
#, no-c-format
msgid ""
"Reducing the requirements of the user space workload will reduce the kernel "
"memory usage (fewer processes, fewer open files and sockets, etc.)"
msgstr ""
"ユーザスペースの負荷要件を減らす (プロセスを減らす、ファイルやソケットを減ら"
"すなど) ことで、カーネルのメモリ使用量も削減することができます。"

#. Tag: title
#: tuning_memory.xml:260
#, no-c-format
msgid "Memory controller (memory cgroups)"
msgstr "メモリコントローラ (メモリ cgroup)"

#. Tag: para
#: tuning_memory.xml:261
#, no-c-format
msgid ""
"If the memory cgroups feature is not needed, it can be switched off by "
"passing cgroup_disable=memory on the kernel command line, reducing memory "
"consumption of the kernel a bit. There is also a slight performance benefit "
"as there is a small amount of accounting overhead when memory cgroups are "
"available even if none are configured."
msgstr ""
"メモリ cgroup の機能が不要である場合は、カーネルのコマンドラインに "
"cgroup_disable=memory を追加することで、無効化を行うことができます。これによ"
"り、カーネルが使用するメモリを少しだけ削減することができます。また、メモリ "
"cgroup が利用できるにもかかわらず設定していない場合、少しだけメモリのオーバー"
"ヘッドが存在するため、少しだけ性能改善をはかることもできます。"

#. Tag: title
#: tuning_memory.xml:271
#, no-c-format
msgid "Virtual memory manager (VM) tunable parameters"
msgstr "仮想メモリマネージャ (VM) のチューニングパラメータ"

#. Tag: para
#: tuning_memory.xml:273
#, no-c-format
msgid ""
"When tuning the VM it should be understood that some changes will take time "
"to affect the workload and take full effect. If the workload changes "
"throughout the day, it may behave very differently at different times. A "
"change that increases throughput under some conditions may decrease it under "
"other conditions."
msgstr ""
"VM をチューニングする場合、パラメータが実際の処理に反映されるまでには、しばら"
"くの時間がかかるものがあることに注意してください。また、負荷が 1 日を通して変"
"化するような場合、時間帯によっては異なる振る舞いになることもあります。それだ"
"けでなく、特定の環境でスループットを改善できるパラメータが、別の環境ではス"
"ループットを悪化させてしまうようなこともあります。"

#. Tag: title
#: tuning_memory.xml:282
#, no-c-format
msgid "Reclaim ratios"
msgstr "回収比率に関するパラメータ"

#. Tag: term
#: tuning_memory.xml:285
#, no-c-format
msgid "<filename>/proc/sys/vm/swappiness</filename>"
msgstr "<filename>/proc/sys/vm/swappiness</filename>"

#. Tag: para
#: tuning_memory.xml:288
#, no-c-format
msgid ""
"This control is used to define how aggressively the kernel swaps out "
"anonymous memory relative to pagecache and other caches. Increasing the "
"value increases the amount of swapping. The default value is <literal>60</"
"literal> ."
msgstr ""
"この変数は、ページキャッシュやその他のキャッシュに比べて、カーネルがどれだけ"
"の比率で匿名メモリを積極的にスワップアウトさせるかを指定するものです。この値"
"を増やすことで、スワップ量が増えることになります。既定値は <literal>60</"
"literal> です。"

#. Tag: para
#: tuning_memory.xml:294
#, no-c-format
msgid ""
"Swap I/O tends to be much less efficient than other I/O. However, some "
"pagecache pages will be accessed much more frequently than less used "
"anonymous memory. The right balance should be found here."
msgstr ""
"スワップの I/O は一般に、その他の I/O よりも効率が落ちやすいものです。しかし"
"ながら、ページキャッシュのページによっては、あまり使用されない匿名メモリより"
"も、頻繁にアクセスされるものがあります。ここでは適切な比率を設定してくださ"
"い。"

#. Tag: para
#: tuning_memory.xml:299
#, no-c-format
msgid ""
"If swap activity is observed during slowdowns, it may be worth reducing this "
"parameter. If there is a lot of I/O activity and the amount of pagecache in "
"the system is rather small, or if there are large dormant applications "
"running, increasing this value might improve performance."
msgstr ""
"速度が低下した際にスワップの動作が観測できた場合、このパラメータを減らしてみ"
"ることをお勧めします。また、多くの I/O 動作が存在する環境で、システム内のペー"
"ジキャッシュの量が比較的少ない場合や、動作しているものの休眠中の巨大なアプリ"
"ケーションが存在するような環境では、この値を増やすことで性能を改善できるかも"
"しれません。"

#. Tag: para
#: tuning_memory.xml:306
#, no-c-format
msgid ""
"Note that the more data is swapped out, the longer the system will take to "
"swap data back in when it is needed."
msgstr ""
"ただし、データのスワップアウト量が増えれば増えるほど、必要とされた際にスワッ"
"プアウトされたデータを取り戻すのに時間がかかるようになります。"

#. Tag: term
#: tuning_memory.xml:313
#, no-c-format
msgid "<filename>/proc/sys/vm/vfs_cache_pressure</filename>"
msgstr "<filename>/proc/sys/vm/vfs_cache_pressure</filename>"

#. Tag: para
#: tuning_memory.xml:316
#, no-c-format
msgid ""
"This variable controls the tendency of the kernel to reclaim the memory "
"which is used for caching of VFS caches, versus pagecache and swap. "
"Increasing this value increases the rate at which VFS caches are reclaimed."
msgstr ""
"この変数は、ページキャッシュやスワップに比べて、カーネルが VFS キャッシュに使"
"用しているメモリの回収傾向を制御するためのものです。この値を増やすと、 VFS "
"キャッシュの回収比率を高めることができます。"

#. Tag: para
#: tuning_memory.xml:322
#, no-c-format
msgid ""
"It is difficult to know when this should be changed, other than by "
"experimentation. The <command>slabtop</command> command (part of the package "
"<systemitem class=\"resource\">procps</systemitem> ) shows top memory "
"objects used by the kernel. The vfs caches are the \"dentry\" and the "
"\"*_inode_cache\" objects. If these are consuming a large amount of memory "
"in relation to pagecache, it may be worth trying to increase pressure. Could "
"also help to reduce swapping. The default value is <literal>100</literal> ."
msgstr ""
"ただし、試してみる以外の方法では、適切な値を推測することは難しい値でもありま"
"す。 <command>slabtop</command> コマンド (<systemitem class=\"resource"
"\">procps</systemitem> パッケージ内に含まれています) を使用することで、カーネ"
"ル側で使用しているメモリオブジェクトを多い順に並べることができます。このコマ"
"ンド内で、 vfs キャッシュは \"dentry\" と \"*_inode_cache\" として表示されま"
"す。ページキャッシュに比べてこれらのメモリが巨大になっている場合、この圧力値"
"を増やしてみることをお勧めします。これにより、スワップ処理を減らすことにもな"
"ります。既定値は <literal>100</literal> です。"

#. Tag: term
#: tuning_memory.xml:335
#, no-c-format
msgid "<filename>/proc/sys/vm/min_free_kbytes</filename>"
msgstr "<filename>/proc/sys/vm/min_free_kbytes</filename>"

#. Tag: para
#: tuning_memory.xml:338
#, no-c-format
msgid ""
"This controls the amount of memory that is kept free for use by special "
"reserves including <quote>atomic</quote> allocations (those which cannot "
"wait for reclaim). This should not normally be lowered unless the system is "
"being very carefully tuned for memory usage (normally useful for embedded "
"rather than server applications). If <quote>page allocation failure</quote> "
"messages and stack traces are frequently seen in logs, min_free_kbytes could "
"be increased until the errors disappear. There is no need for concern, if "
"these messages are very infrequent. The default value depends on the amount "
"of RAM."
msgstr ""
"この変数は、 <quote>アトミックな</quote> (回収を待つことができない) 割り当て"
"など、特別に予約しておく必要のあるメモリ量を制御します。これは通常、メモリ使"
"用率について非常に注意深くチューニングしている場合を除いて、減らすべきではな"
"い値です (通常はサーバ用途ではなく、組み込み用途で設定すべき値です) 。もしも"
"ログ内に <quote>ページ割り当て失敗</quote> に関するメッセージとスタックトレー"
"スが頻繁に現れているような場合、 min_free_kbytes をエラーが出なくなる程度まで"
"増やしてみることをお勧めします。このようなメッセージが頻繁に現れたりしていな"
"ければ、特に気にする必要はありません。既定値は搭載されている RAM の量に従って"
"決められます。"

#. Tag: term
#: tuning_memory.xml:352
#, no-c-format
msgid "<filename>/proc/sys/vm/watermark_scale_factor</filename>"
msgstr "<filename>/proc/sys/vm/watermark_scale_factor</filename>"

#. Tag: para
#: tuning_memory.xml:354
#, no-c-format
msgid ""
"Broadly speaking, free memory has high, low and min watermarks. When the low "
"watermark is reached then <command>kswapd</command> wakes to reclaim memory "
"in the background. It stays awake until free memory reaches the high "
"watermark. Applications will stall and reclaim memory when the min watermark "
"is reached."
msgstr ""
"大まかに言うと、空きメモリには高／低／最小の水準が設定されています。低い水準"
"に達した場合、 <command>kswapd</command> が動作して裏でメモリの回収を行うよう"
"になります。この回収作業は、高いほうの水準に達するまで続けられます。最小の水準"
"に達した場合、アプリケーションの動作は一時的に停止させられます。"

#. Tag: para
#: tuning_memory.xml:361
#, no-c-format
msgid ""
"The <literal>watermark_scale_factor</literal> defines the amount of memory "
"left in a node/system before kswapd is woken up and how much memory needs to "
"be free before kswapd goes back to sleep. The unit is in fractions of "
"10,000. The default value of 10 means the distances between watermarks are "
"0.1% of the available memory in the node/system. The maximum value is 1000, "
"or 10% of memory."
msgstr ""
"<literal>watermark_scale_factor</literal> は、 kswapd が動き出す際のノードも"
"しくはシステム内のメモリ量を表す値と、そこから kswapd が休眠状態に戻るまでに"
"どれだけの量のメモリを解放するのかを表す値です。単位は 10,000 の対する比率"
"で、既定値の 10 は 水準間の長さがノード／システム内で利用できるメモリの 0.1% "
"分であることを示します。最大値は 1000 で、 10% 分を表します。"

#. Tag: para
#: tuning_memory.xml:369
#, no-c-format
msgid ""
"Workloads that frequently stall in direct reclaim, accounted by "
"<literal>allocstall</literal> in <filename>/proc/vmstat</filename> , may "
"benefit from altering this parameter. Similarly, if <command>kswapd</"
"command> is sleeping prematurely, as accounted for by "
"<literal>kswapd_low_wmark_hit_quickly</literal> , then it may indicate that "
"the number of pages kept free to avoid stalls is too low."
msgstr ""
"直接的な回収処理で処理速度が遅くなってしまっている場合、 <filename>/proc/"
"vmstat</filename> 内の <literal>allocstall</literal> の値が増えますが、この場"
"合はこのパラメータを変更することで、問題を回避できるかもしれません。同様に "
"<command>kswapd</command> が早く休眠状態になってしまう場合、 "
"<literal>kswapd_low_wmark_hit_quickly</literal> の値が増えますが、この場合は"
"アプリケーションの一時停止を回避するために空いているページ数が、少なすぎるこ"
"とを表しています。"

#. Tag: title
#: tuning_memory.xml:383
#, no-c-format
msgid "Writeback parameters"
msgstr "ライトバック (書き戻し) 関連のパラメータ"

#. Tag: para
#: tuning_memory.xml:384
#, no-c-format
msgid ""
"One important change in writeback behavior since &productname; 10 is that "
"modification to file-backed mmap() memory is accounted immediately as dirty "
"memory (and subject to writeback). Whereas previously it would only be "
"subject to writeback after it was unmapped, upon an msync() system call, or "
"under heavy memory pressure."
msgstr ""
"<!-- NOTE: wrong for openSUSE? &productname; 10 --> 以前の &productname; バー"
"ジョンからのライトバック関連の主な変更点として、ファイルに結びつけられた "
"mmap() メモリが、即時に dirty なメモリとして判断されるようになった (つまりラ"
"イトバックの対象となる) ことがあげられます。以前のバージョンでは、 munmap() "
"でマップが解除された場合や msync() システムコールが呼び出された場合、もしくは"
"メモリへの圧力が大きい場合にのみ書き戻しが行われていました。"

#. Tag: para
#: tuning_memory.xml:391
#, no-c-format
msgid ""
"Some applications do not expect mmap modifications to be subject to such "
"writeback behavior, and performance can be reduced. Increasing writeback "
"ratios and times can improve this type of slowdown."
msgstr ""
"アプリケーションによっては、このような書き戻し動作への変更を期待していないも"
"のもあり、場合によっては性能が低下してしまうことがあります。"

#. Tag: term
#: tuning_memory.xml:398
#, no-c-format
msgid "<filename>/proc/sys/vm/dirty_background_ratio</filename>"
msgstr "<filename>/proc/sys/vm/dirty_background_ratio</filename>"

#. Tag: para
#: tuning_memory.xml:401
#, no-c-format
msgid ""
"This is the percentage of the total amount of free and reclaimable memory. "
"When the amount of dirty pagecache exceeds this percentage, writeback "
"threads start writing back dirty memory. The default value is <literal>10</"
"literal> (%)."
msgstr ""
"この変数は空き領域や回収可能なメモリの全体量の割合を表すものです。この割合以"
"上に dirty なページキャッシュが存在していると、ライトバックスレッドが dirty "
"なメモリを書き込み始めるようになります。既定値は <literal>10</literal> (%) で"
"す。"

#. Tag: term
#: tuning_memory.xml:410
#, no-c-format
msgid "<filename>/proc/sys/vm/dirty_background_bytes</filename>"
msgstr "<filename>/proc/sys/vm/dirty_background_bytes</filename>"

#. Tag: para
#: tuning_memory.xml:413
#, no-c-format
msgid ""
"This contains the amount of dirty memory at which the background kernel "
"flusher threads will start writeback. <filename>dirty_background_bytes</"
"filename> is the counterpart of <filename>dirty_background_ratio</"
"filename> . If one of them is set, the other one will automatically be read "
"as <literal>0</literal> ."
msgstr ""
"この変数は、裏で動作するカーネルのライトバックスレッドが、その書き込みを始め"
"る割合を表すものです。 <filename>dirty_background_bytes</filename> は "
"<filename>dirty_background_ratio</filename> に対応するもので、一方を設定する"
"と他方が自動的に <literal>0</literal> に設定されます。"

#. Tag: term
#: tuning_memory.xml:423
#, no-c-format
msgid "<filename>/proc/sys/vm/dirty_ratio</filename>"
msgstr "<filename>/proc/sys/vm/dirty_ratio</filename>"

#. Tag: para
#: tuning_memory.xml:426
#, no-c-format
msgid ""
"Similar percentage value as for <filename>dirty_background_ratio</"
"filename> . When this is exceeded, applications that want to write to the "
"pagecache are blocked and wait for kernel background flusher threads to "
"reduce the amount of dirty memory. The default value is <literal>20</"
"literal> (%)."
msgstr ""
"<filename>dirty_background_ratio</filename> に似た意味を持つ割合値です。ここ"
"で指定した割合を超過すると、ページキャッシュに対して書き込みを行いたいアプリ"
"ケーションの動作が一時的に止められ、カーネルのライトバックスレッドが dirty な"
"メモリを書き込んで clean に戻すまで待機するようになります。既定値は "
"<literal>20</literal> (%) です。"

#. Tag: term
#: tuning_memory.xml:436
#, no-c-format
msgid "<filename>/proc/sys/vm/dirty_bytes</filename>"
msgstr "<filename>/proc/sys/vm/dirty_bytes</filename>"

#. Tag: para
#: tuning_memory.xml:439
#, no-c-format
msgid ""
"This file controls the same tunable as <filename>dirty_ratio</filename> "
"however the amount of dirty memory is in bytes as opposed to a percentage of "
"reclaimable memory. Since both <filename>dirty_ratio</filename> and "
"<filename>dirty_bytes</filename> control the same tunable, if one of them is "
"set, the other one will automatically be read as <literal>0</literal> . The "
"minimum value allowed for <filename>dirty_bytes</filename> is two pages (in "
"bytes); any value lower than this limit will be ignored and the old "
"configuration will be retained."
msgstr ""
"この変数は <filename>dirty_ratio</filename> と同じようなチューニングパラメー"
"タですが、ここでは割合ではなくバイト単位でサイズを指定します。 "
"<filename>dirty_ratio</filename> と <filename>dirty_bytes</filename> は同じ"
"チューニングパラメータであることから、一方を設定すると他方が自動的に "
"<literal>0</literal> になります。 <filename>dirty_bytes</filename> に設定可能"
"な最小値は 2 ページ分 (ただしバイト単位) で、それより小さい値を設定しようとし"
"ても、それは無視されて元の値に戻ってしまいます。"

#. Tag: term
#: tuning_memory.xml:453
#, no-c-format
msgid "<filename>/proc/sys/vm/dirty_expire_centisecs</filename>"
msgstr "<filename>/proc/sys/vm/dirty_expire_centisecs</filename>"

#. Tag: para
#: tuning_memory.xml:456
#, no-c-format
msgid ""
"Data which has been dirty in-memory for longer than this interval will be "
"written out next time a flusher thread wakes up. Expiration is measured "
"based on the modification time of a file's inode. Therefore, multiple "
"dirtied pages from the same file will all be written when the interval is "
"exceeded."
msgstr ""
"この変数は、ここで設定した値よりも長い時間 dirty であり続けたメモリが存在した"
"場合、次回のライトバックスレッドの起床時に書き込みが行われるようになるもので"
"す。ここでの期限設定はファイルの inode に設定された最終更新日時を基準にしま"
"す。そのため、同じファイルに対して発生した複数の dirty ページが存在した場合、"
"この期限を超過するとそれら全てが書き込まれるようになります。"

#. Tag: para
#: tuning_memory.xml:466
#, no-c-format
msgid ""
"<filename>dirty_background_ratio</filename> and <filename>dirty_ratio</"
"filename> together determine the pagecache writeback behavior. If these "
"values are increased, more dirty memory is kept in the system for a longer "
"time. With more dirty memory allowed in the system, the chance to improve "
"throughput by avoiding writeback I/O and to submitting more optimal I/O "
"patterns increases. However, more dirty memory can either harm latency when "
"memory needs to be reclaimed or at points of data integrity "
"( <quote>synchronization points</quote> ) when it needs to be written back "
"to disk."
msgstr ""
"<filename>dirty_background_ratio</filename> と <filename>dirty_ratio</"
"filename> は、いずれもページキャッシュのライトバック動作を設定するためのもの"
"です。これらの値を増やした場合、システム内に dirty なメモリがより多く、かつ長"
"く保持されることになります。システム内により多くの dirty なメモリを保持できる"
"環境であれば、ライトバックの I/O を減らして、より最適な I/O パターンでの書き"
"込みを増やすために、これらの値を活用できる場合があります。ただし dirty なメモ"
"リが多く存在していると、メモリを回収する必要がある場合や、ディスクに書き込む"
"必要が発生した場合に一貫性を確保するタイミング ( <quote>同期ポイント</"
"quote> ) で、遅延が発生する場合があります。"

#. Tag: title
#: tuning_memory.xml:480
#, no-c-format
msgid "Timing differences of I/O writes between &sle; 12 and &sle; 11"
msgstr "&sle; 12 と &sle; 11 における I/O 書き込みのタイミングの違いについて"

#. Tag: para
#: tuning_memory.xml:481
#, no-c-format
msgid ""
"The system is required to limit what percentage of the system's memory "
"contains file-backed data that needs writing to disk. This guarantees that "
"the system can always allocate the necessary data structures to complete I/"
"O. The maximum amount of memory that can be dirty and requires writing at "
"any time is controlled by <literal>vm.dirty_ratio</literal> ( <filename>/"
"proc/sys/vm/dirty_ratio</filename> ). The defaults are:"
msgstr ""
"システム側では、ディスクに書き込むべきファイルデータをどれだけ保持しておくの"
"かを、システムのメモリサイズに応じて制限する必要があります。これにより、 I/O "
"を完了させるのに必要なデータ構造を常に割り当てることができるようにしていま"
"す。 dirty になりうるメモリの最大量と、その書き込みの必要性は、 <literal>vm."
"dirty_ratio</literal> ( <filename>/proc/sys/vm/dirty_ratio</filename> ) で制"
"御することができます。既定値はそれぞれ下記のとおりです:"

#. Tag: screen
#: tuning_memory.xml:490
#, no-c-format
msgid ""
"SLE-11-SP3:     vm.dirty_ratio = 40\n"
"SLE-12:         vm.dirty_ratio = 20"
msgstr ""
"SLE-11-SP3:     vm.dirty_ratio = 40\n"
"SLE-12:         vm.dirty_ratio = 20"

#. Tag: para
#: tuning_memory.xml:492
#, no-c-format
msgid ""
"The primary advantage of using the lower ratio in &sle; 12 is that page "
"reclamation and allocation in low memory situations completes faster as "
"there is a higher probability that old clean pages will be quickly found and "
"discarded. The secondary advantage is that if all data on the system must be "
"synchronized, then the time to complete the operation on &sle; 12 will be "
"lower than &sle; 11 SP3 by default. Most workloads will not notice this "
"change as data is synchronized with <literal>fsync()</literal> by the "
"application or data is not dirtied quickly enough to hit the limits."
msgstr ""
"&sle; 12 で低い割合が設定されていることの利点は、メモリが枯渇した場合のページ"
"回収と割り当てをできる限り素早く処理し、古い dirty ページを素早く見つけて廃棄"
"することができる点にあります。もう 1 つの利点としては、システム内の全てのデー"
"タは同期しておかなければならず、既定では &sle; 11 SP3 よりも &sle; 12 のほう"
"がより早く終わることになる点です。ほとんどの処理では、アプリケーション側から "
"<literal>fsync()</literal> が呼ばれたり、その制限に引っかかるほど素早く "
"dirty になることがありませんので、このような変更が行われていることに気がつく"
"ことはありません。"

#. Tag: para
#: tuning_memory.xml:503
#, no-c-format
msgid ""
"There are exceptions and if your application is affected by this, it will "
"manifest as an unexpected stall during writes. To prove it is affected by "
"dirty data rate limiting then monitor <literal>/proc/"
"<replaceable>PID_OF_APPLICATION</replaceable>/stack</literal> and it will be "
"observed that the application spends significant time in "
"<literal>balance_dirty_pages_ratelimited</literal> . If this is observed and "
"it is a problem, then increase the value of <literal>vm.dirty_ratio</"
"literal> to 40 to restore the &sle; 11 SP3 behavior."
msgstr ""
"もしも例外的なアプリケーションをお使いの場合で、個の変更による影響を受けてし"
"まっているような場合、書き込み時に予期せずアプリケーションの動きが遅くなって"
"しまうことがあります。この場合、 dirty なデータの速度が制限されていることを確"
"認するため、 <literal>/proc/<replaceable>アプリケーションのプロセス_ID</"
"replaceable>/stack</literal> を監視しておき、アプリケーションが "
"<literal>balance_dirty_pages_ratelimited</literal> 内で時間を費やしていること"
"を確認してください。このような現象に当てはまる場合で、かつそれが問題となって"
"いる場合、 <literal>vm.dirty_ratio</literal> を 40 に戻すことで、従来の "
"&sle; 11 SP3 の動作に戻すことをお勧めします。"

#. Tag: para
#: tuning_memory.xml:514
#, no-c-format
msgid ""
"It is important to note that the overall I/O throughput is the same "
"regardless of the setting. The only difference is the timing of when the I/O "
"is queued."
msgstr ""
"ただし、全体としての I/O スループットはこの設定とは無関係であることに注意して"
"ください。違いは I/O がキュー内に存在するタイミングだけです。"

#. Tag: para
#: tuning_memory.xml:519
#, no-c-format
msgid ""
"This is an example of using <command>dd</command> to asynchronously write "
"30% of memory to disk which would happen to be affected by the change in "
"<literal>vm.dirty_ratio</literal> :"
msgstr ""
"下記の例は <command>dd</command> コマンドを利用して、メモリの 30% 分をディス"
"クに書き込むことで、 <literal>vm.dirty_ratio</literal> の変更による影響を受け"
"られるようにした場合の例です:"

#. Tag: screen
#: tuning_memory.xml:524
#, no-c-format
msgid ""
"&prompt.root;MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`\n"
"&prompt.root;sysctl vm.dirty_ratio=40\n"
"&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count="
"$((MEMTOTAL_MBYTES*30/100))\n"
"2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s\n"
"&prompt.root;sysctl vm.dirty_ratio=20\n"
"dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))\n"
"2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s"
msgstr ""
"&prompt.root;MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`\n"
"&prompt.root;sysctl vm.dirty_ratio=40\n"
"&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count="
"$((MEMTOTAL_MBYTES*30/100))\n"
"2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s\n"
"&prompt.root;sysctl vm.dirty_ratio=20\n"
"dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))\n"
"2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s"

#. Tag: para
#: tuning_memory.xml:531
#, no-c-format
msgid ""
"Note that the parameter affects the time it takes for the command to "
"complete and the apparent write speed of the device. With "
"<literal>dirty_ratio=40</literal> , more of the data is cached and written "
"to disk in the background by the kernel. It is very important to note that "
"the speed of I/O is identical in both cases. To demonstrate, this is the "
"result when <command>dd</command> synchronizes the data before exiting:"
msgstr ""
"なお、このパラメータは、コマンドの処理が完了するまでの時間と、デバイスへの見"
"かけ上の書き込み速度に影響を与えていることに注意してください。 "
"<literal>dirty_ratio=40</literal> では、多くのデータがキャッシュされ、カーネ"
"ルによって裏でディスクへの書き込みが行われています。また、両方の実行例でデバ"
"イスの I/O 速度は同じであることにも注意してください。もう一度、確認の目的で "
"<command>dd</command> コマンドを実行し、今回は終了前にデータ同期を行ってみま"
"す:"

#. Tag: screen
#: tuning_memory.xml:540
#, no-c-format
msgid ""
"&prompt.root;sysctl vm.dirty_ratio=40\n"
"&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count="
"$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync\n"
"2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s\n"
"&prompt.root;sysctl vm.dirty_ratio=20\n"
"&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count="
"$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync\n"
"2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s"
msgstr ""
"&prompt.root;sysctl vm.dirty_ratio=40\n"
"&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count="
"$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync\n"
"2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s\n"
"&prompt.root;sysctl vm.dirty_ratio=20\n"
"&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count="
"$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync\n"
"2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s"

#. Tag: para
#: tuning_memory.xml:546
#, no-c-format
msgid ""
"Note that <literal>dirty_ratio</literal> had almost no impact here and is "
"within the natural variability of a command. Hence, <literal>dirty_ratio</"
"literal> does not directly impact I/O performance but it may affect the "
"apparent performance of a workload that writes data asynchronously without "
"synchronizing."
msgstr ""
"上記のとおり、 <literal>dirty_ratio</literal> の設定変更による影響はほとんど"
"発生しておらず、誤差の範囲内であることがわかります。このことから、 "
"<literal>dirty_ratio</literal> の設定は直接的な I/O の性能に影響を与えること"
"はありませんが、同期せずに書き込みを行っている場合、見かけ上の性能には影響が"
"あることに注意してください。"

#. Tag: title
#: tuning_memory.xml:556
#, no-c-format
msgid "Readahead parameters"
msgstr "先読み関連のパラメータ"

#. Tag: term
#: tuning_memory.xml:559
#, no-c-format
msgid ""
"<filename>/sys/block/<replaceable>&lt;bdev&gt;</replaceable>/queue/"
"read_ahead_kb</filename>"
msgstr ""
"<filename>/sys/block/<replaceable>ブロックデバイス</replaceable>/queue/"
"read_ahead_kb</filename>"

#. Tag: para
#: tuning_memory.xml:562
#, no-c-format
msgid ""
"If one or more processes are sequentially reading a file, the kernel reads "
"some data in advance (ahead) to reduce the amount of time that processes "
"need to wait for data to be available. The actual amount of data being read "
"in advance is computed dynamically, based on how much \"sequential\" the I/O "
"seems to be. This parameter sets the maximum amount of data that the kernel "
"reads ahead for a single file. If you observe that large sequential reads "
"from a file are not fast enough, you can try increasing this value. "
"Increasing it too far may result in readahead thrashing where pagecache used "
"for readahead is reclaimed before it can be used, or slowdowns because of a "
"large amount of useless I/O. The default value is <literal>512</literal> "
"(KB)."
msgstr ""
"1 つもしくは複数のプロセスがファイルを順次読み込みしている場合、カーネルはそ"
"れらのデータを前もって読み込み (先読みし) 、プロセスがデータを待つ時間を減ら"
"すように処理を行います。先読みのデータ量は、どれだけ \"順に\" I/O を行ってい"
"るのかに従って、動的に計算されます。このパラメータは、カーネルが先読みを行う"
"際、 1 ファイルあたりの最大データ量を設定するためのものです。巨大なファイルの"
"順次読み込みが十分に早いものであるとは思えない場合、この値を増やしてみること"
"をお勧めします。ただし、大きくしすぎると、先読みスラッシングと呼ばれる現象が"
"発生し、先読みで使用したページキャッシュが、使用される前に回収されてしまうこ"
"とがあります。また、意味のない I/O が発生して速度が落ちてしまうこともありま"
"す。既定値は <literal>512</literal> [KB] です。"

#. Tag: title
#: tuning_memory.xml:582
#, no-c-format
msgid "Transparent HugePage parameters"
msgstr "Transparent HugePage 関連のパラメータ"

#. Tag: para
#: tuning_memory.xml:583
#, no-c-format
msgid ""
"Transparent HugePages (THP) provide a way to dynamically allocate huge pages "
"either on&#x2011;demand by the process or deferring the allocation until "
"later via the <command>khugepaged</command> kernel thread. This method is "
"distinct from the use of <literal>hugetlbfs</literal> to manually manage "
"their allocation and use. Workloads with contiguous memory access patterns "
"can benefit greatly from THP. A 1000-fold decrease in page faults can be "
"observed when running synthetic workloads with contiguous memory access "
"patterns."
msgstr ""
"Transparent HugePages (THP) は動的な huge page の割り当て方法で、プロセス側か"
"ら要求によって割り当てたり、後から <command>khugepaged</command> カーネルス"
"レッドを介して遅延割り当てを行ったりするためのものです。この方式は "
"<literal>hugetlbfs</literal> の使用とは区別されていて、こちらは割り当てと使用"
"を手作業で管理する方式です。連続したメモリのアクセスパターンが存在する負荷の"
"場合、 THP によって大きく性能を改善できる可能性があります。連続したメモリのア"
"クセスパターンで一括処理を行うと、ページフォルトを 1000 倍減少させることもで"
"きてしまいます。"

#. Tag: para
#: tuning_memory.xml:593
#, no-c-format
msgid ""
"There are cases when THP may be undesirable. Workloads with sparse memory "
"access patterns can perform poorly with THP due to excessive memory usage. "
"For example, 2 MB of memory may be used at fault time instead of 4 KB for "
"each fault and ultimately lead to premature page reclaim. <phrase os=\"sles;"
"sled\"> On releases older than &sle; 12 SP2, it was possible for an "
"application to stall for long periods of time trying to allocate a THP which "
"frequently led to a recommendation of disabling THP. Such recommendations "
"should be re-evaluated for &sle; 12 SP3 and later releases.</phrase>"
msgstr ""
"逆に THP が望ましくない場合もあります。たとえばメモリ内のあちこちをアクセスす"
"るようなパターンの場合、 メモリの使用量が増える結果になってしまうため、かえっ"
"て性能が落ちてしまいます。たとえばフォルトごとに 4 KB ではなく、フォルト発生"
"時点で 2 MB のメモリを使用してしまうことがあり、最終的にはページの回収を早め"
"る結果になってしまいます。<phrase os=\"sles;sled\"> &sle; 12 SP2 よりも古い"
"バージョンでは、 THP の割り当てに伴ってアプリケーション側が長い時間待機させら"
"れてしまうことがありましたので、これが THP を無効化する理由にもなっていまし"
"た。このような推奨は <phrase os=\"sles;sled\">&sle; 12 SP3</phrase> で改めら"
"れています。 </phrase>"

#. Tag: para
#: tuning_memory.xml:604
#, no-c-format
msgid ""
"The behavior of THP may be configured via the <option>transparent_hugepage=</"
"option> kernel parameter or via sysfs. For example, it may be disabled by "
"adding the kernel parameter <option>transparent_hugepage=never</option> , "
"rebuilding your grub2 configuration, and rebooting. Verify if THP is "
"disabled with:"
msgstr ""
"THP の動作は <option>transparent_hugepage=</option> のカーネルパラメータか、 "
"sysfs 経由で設定することができます。たとえばカーネルのパラメータに "
"<option>transparent_hugepage=never</option> を追加して grub2 の設定を再構築"
"し、システムを再起動します。すると、下記のように入力して実行することで、 THP "
"が無効化されていることを確認できます:"

#. Tag: screen
#: tuning_memory.xml:612
#, no-c-format
msgid ""
"&prompt.root;cat /sys/kernel/mm/transparent_hugepage/enabled\n"
"always madvise [never]"
msgstr ""
"&prompt.root;cat /sys/kernel/mm/transparent_hugepage/enabled\n"
"always madvise [never]"

#. Tag: para
#: tuning_memory.xml:614
#, no-c-format
msgid ""
"If disabled, the value <literal>never</literal> is shown in square brackets "
"like in the example above. A value of <literal>always</literal> will always "
"try and use THP at fault time but defer to <command>khugepaged</command> if "
"the allocation fails. A value of <literal>madvise</literal> will only "
"allocate THP for address spaces explicitly specified by an application."
msgstr ""
"無効化されている場合、上記の例のように <literal>never</literal> が [] で囲ま"
"れて表示されます。 <literal>always</literal> に設定すると THP を常に使用する"
"ようになりますが、割り当てが失敗した場合は <command>khugepaged</command> に従"
"います。 <literal>madvise</literal> に設定すると、アプリケーション側で明示的"
"に指定されたアドレス空間に THP を割り当てるだけになります。"

#. Tag: term
#: tuning_memory.xml:624
#, no-c-format
msgid "<filename>/sys/kernel/mm/transparent_hugepage/defrag</filename>"
msgstr "<filename>/sys/kernel/mm/transparent_hugepage/defrag</filename>"

#. Tag: para
#: tuning_memory.xml:627
#, no-c-format
msgid ""
"This parameter controls how much effort an application commits when "
"allocating a THP. A value of <literal>always</literal> is the default for "
"<phrase os=\"sles;sled\">&sle; 12 SP1 and earlier</phrase> <phrase os=\"osuse"
"\">&opensuse; 42.1 and earlier</phrase> releases that supported THP. If a "
"THP is not available, the application will try to defragment memory. It "
"potentially incurs large stalls in an application if the memory is "
"fragmented and a THP is not available."
msgstr ""
"このパラメータは、 THP を割り当てる際にアプリケーションがどれだけの努力を行う"
"のかを制御するものです。 <literal>always</literal> は <phrase os=\"sles;sled"
"\">&sle; 12 SP1 およびそれ以前</phrase> <phrase os=\"osuse\">&opensuse; 42.1 "
"およびそれ以前</phrase> の THP に対応するリリースでの既定値でした。もしも "
"THP が利用できない場合、アプリケーションはメモリのデフラグを実施しようとしま"
"す。つまり THP が利用できない場合、メモリがフラグメントされていると、アプリ"
"ケーションの動作が潜在的に一時的ながら停止する可能性があることになります。"

#. Tag: para
#: tuning_memory.xml:636
#, no-c-format
msgid ""
"A value of <literal>madvise</literal> means that THP allocation requests "
"will only defragment if the application explicitly requests it. This is the "
"default for <phrase os=\"sles;sled\">&sle; 12 SP2</phrase> <phrase os=\"osuse"
"\">&opensuse; 42.2</phrase> and later releases."
msgstr ""
"値に <literal>madvise</literal> を設定すると、 THP の割り当てリクエストでは、"
"アプリケーション側から明示的に要求された場合にのみ、デフラグを実施します。こ"
"ちらが <phrase os=\"sles;sled\">&sle; 12 SP2</phrase> <phrase os=\"osuse"
"\">&opensuse; 42.2</phrase> およびそれ以降のバージョンでの既定値となります。"

#. Tag: para
#: tuning_memory.xml:643
#, no-c-format
msgid ""
"<literal>defer</literal> is only available on <phrase os=\"sles;sled\">&sle; "
"12 SP2</phrase> <phrase os=\"osuse\">&opensuse; 42.2</phrase> and later "
"releases. If a THP is not available, the application will fall back to using "
"small pages if a THP is not available. It will wake the <command>kswapd</"
"command> and <command>kcompactd</command> kernel threads to defragment "
"memory in the background and a THP will be allocated later by "
"<command>khugepaged</command> ."
msgstr ""
"<literal>defer</literal> は <phrase os=\"sles;sled\">&sle; 12 SP2</phrase> "
"<phrase os=\"osuse\">&opensuse; 42.2</phrase> およびそれ以降で利用できるよう"
"になった値で、 THP が利用できない場合には小さなページを使用して処理するように"
"動作します。これにより <command>kswapd</command> と <command>kcompactd</"
"command> の各カーネルスレッドを起床させ、裏でデフラグを行うことで、後から "
"<command>khugepaged</command> が THP を割り当てる動作になります。"

#. Tag: para
#: tuning_memory.xml:653
#, no-c-format
msgid ""
"The final option <literal>never</literal> will use small pages if a THP is "
"unavailable but no other action will take place."
msgstr ""
"最後の値である <literal>never</literal> は、 THP が利用できない場合には単純に"
"小さなページを利用して処理を行うもので、それ以外の処理は行わない意味になりま"
"す。"

#. Tag: title
#: tuning_memory.xml:662
#, no-c-format
msgid "khugepaged parameters"
msgstr "khugepaged のパラメータ"

#. Tag: para
#: tuning_memory.xml:663
#, no-c-format
msgid ""
"khugepaged will be automatically started when <literal>transparent_hugepage</"
"literal> is set to <literal>always</literal> or <literal>madvise</literal> , "
"and it will be automatically shut down if it is set to <literal>never</"
"literal> . Normally this runs at low frequency but the behavior can be tuned."
msgstr ""
"khugepaged は <literal>transparent_hugepage</literal> の値が "
"<literal>always</literal> もしくは <literal>madvise</literal> である場合に自"
"動的に開始され、 <literal>never</literal> である場合には自動的に停止します。"
"通常は低頻度で動作するものではありますが、動作をチューニングすることもできま"
"す。"

#. Tag: term
#: tuning_memory.xml:672
#, no-c-format
msgid "<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/defrag</filename>"
msgstr "<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/defrag</filename>"

#. Tag: para
#: tuning_memory.xml:674
#, no-c-format
msgid ""
"A value of 0 will disable <command>khugepaged</command> even though THP may "
"still be used at fault time. This may be important for latency-sensitive "
"applications that benefit from THP but cannot tolerate a stall if "
"<command>khugepaged</command> tries to update an application memory usage."
msgstr ""
"値に 0 を設定すると、フォルトの時点で THP が使用されていても "
"<command>khugepaged</command> を無効化します。これは遅延に敏感なアプリケー"
"ションにとっては重要な設定で、 THP による利点を受けるものの、 "
"<command>khugepaged</command> がアプリケーションのメモリ使用を更新しようとす"
"る際に、一時的に動作が止まってしまう事象を防ぐことができるからです。"

#. Tag: term
#: tuning_memory.xml:685
#, no-c-format
msgid ""
"<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan</"
"filename>"
msgstr ""
"<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan</"
"filename>"

#. Tag: para
#: tuning_memory.xml:687
#, no-c-format
msgid ""
"This parameter controls how many pages are scanned by <command>khugepaged</"
"command> in a single pass. A scan identifies small pages that can be "
"reallocated as THP. Increasing this value will allocate THP in the "
"background faster at the cost of CPU usage."
msgstr ""
"この変数は、 <command>khugepaged</command> が 1 回の処理でスキャンするページ"
"数を制御するためのものです。スキャン処理では小さいページを検出して、 THP で再"
"割り当てができないかどうかを確認します。この値を増やすことで、 CPU の使用率が"
"上がるものの、裏でより高速に THP を割り当てることができるようになります。"

#. Tag: term
#: tuning_memory.xml:698
#, no-c-format
msgid ""
"<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/"
"scan_sleep_millisecs</filename>"
msgstr ""
"<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/"
"scan_sleep_millisecs</filename>"

#. Tag: para
#: tuning_memory.xml:700
#, no-c-format
msgid ""
"<command>khugepaged</command> sleeps for a short interval specified by this "
"parameter after each pass to limit how much CPU usage is used. Reducing this "
"value will allocate THP in the background faster at the cost of CPU usage. A "
"value of 0 will force continual scanning."
msgstr ""
"この変数は、 <command>khugepaged</command> が 1 回の処理を完了した後に待機す"
"る時間を設定するためのもので、 CPU の使用率が過剰に上がらないようにするための"
"ものです。この値を小さくすると、 CPU の使用率が上がる代わりに、裏でより高速"
"に THP を割り当てることができるようになります。"

#. Tag: term
#: tuning_memory.xml:710
#, no-c-format
msgid ""
"<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/"
"alloc_sleep_millisecs</filename>"
msgstr ""
"<filename>/sys/kernel/mm/transparent_hugepage/khugepaged/"
"alloc_sleep_millisecs</filename>"

#. Tag: para
#: tuning_memory.xml:713
#, no-c-format
msgid ""
"This parameter controls how long <command>khugepaged</command> will sleep in "
"the event it fails to allocate a THP in the background waiting for "
"<command>kswapd</command> and <command>kcompactd</command> to take action."
msgstr ""
"この変数は、 <command>khugepaged</command> が <command>kswapd</command> と "
"<command>kcompactd</command> が動作するのを待っている間、裏で THP が割り当て"
"に失敗した際に休眠する時間を指定するためのものです。"

#. Tag: para
#: tuning_memory.xml:723
#, no-c-format
msgid ""
"The remaining parameters for <command>khugepaged</command> are rarely useful "
"for performance tuning but are fully documented in <filename>/usr/src/linux/"
"Documentation/vm/transhuge.txt</filename>"
msgstr ""
"<command>khugepaged</command> に対するその他のパラメータは、性能チューニング"
"にあたってはほとんど用途のないものではありますが、 <filename>/usr/src/linux/"
"Documentation/vm/transhuge.txt</filename> ファイル (英語) で詳しく説明してい"
"ます。"

#. Tag: title
#: tuning_memory.xml:730
#, no-c-format
msgid "Further VM parameters"
msgstr "その他の VM パラメータ"

#. Tag: para
#: tuning_memory.xml:731
#, no-c-format
msgid ""
"For the complete list of the VM tunable parameters, see <filename>/usr/src/"
"linux/Documentation/sysctl/vm.txt</filename> (available after having "
"installed the <systemitem class=\"resource\">kernel-source</systemitem> "
"package)."
msgstr ""
"VM に関連するチューニングパラメータに関する完全な一覧については、 <filename>/"
"usr/src/linux/Documentation/sysctl/vm.txt</filename> ファイル (英語, "
"<systemitem class=\"resource\">kernel-source</systemitem> パッケージ内に含ま"
"れています) をお読みください。"

#. Tag: title
#: tuning_memory.xml:776
#, no-c-format
msgid "Monitoring VM behavior"
msgstr "VM の動作監視"

#. Tag: para
#: tuning_memory.xml:778
#, no-c-format
msgid "Some simple tools that can help monitor VM behavior:"
msgstr "VM の動作の監視には、下記のようなシンプルなツールを使用することができます:"

#. Tag: para
#: tuning_memory.xml:784
#, no-c-format
msgid ""
"vmstat: This tool gives a good overview of what the VM is doing. See <xref "
"linkend=\"sec-util-multi-vmstat\"/> for details."
msgstr ""
"vmstat: このツールは、 VM が現在何をしているのかをわかりやすく表示することが"
"できます。詳しくは <xref linkend=\"sec-util-multi-vmstat\"/> をお読みくださ"
"い。"

#. Tag: para
#: tuning_memory.xml:790
#, no-c-format
msgid ""
"<filename>/proc/meminfo</filename> : This file gives a detailed breakdown of "
"where memory is being used. See <xref linkend=\"sec-util-memory-meminfo\"/> "
"for details."
msgstr ""
"<filename>/proc/meminfo</filename> : このファイルは、メモリの使途を分解して示"
"しているファイルです。詳しくは <xref linkend=\"sec-util-memory-meminfo\"/> を"
"お読みください。"

#. Tag: para
#: tuning_memory.xml:797
#, no-c-format
msgid ""
"<command>slabtop</command> : This tool provides detailed information about "
"kernel slab memory usage. buffer_head, dentry, inode_cache, "
"ext3_inode_cache, etc. are the major caches. This command is available with "
"the package <systemitem class=\"resource\">procps</systemitem> ."
msgstr ""
"<command>slabtop</command> : このツールは、カーネルの slab メモリについて、そ"
"の使用状況を詳細に表示することができます。 buffer_head, dentry, inode_cache, "
"ext3_inode_cache などの値が主なキャッシュです。このコマンドは <systemitem "
"class=\"resource\">procps</systemitem> パッケージ内に含まれています。"

#. Tag: para
#: tuning_memory.xml:805
#, no-c-format
msgid ""
"<filename>/proc/vmstat</filename> : This file gives a detailed breakdown of "
"internal VM behavior. The information contained within is implementation "
"specific and may not always be available. Some information is duplicated in "
"<filename>/proc/meminfo</filename> and other information can be presented in "
"a friendly fashion by utilities. For maximum utility, this file needs to be "
"monitored over time to observe rates of change. The most important pieces of "
"information that are hard to derive from other sources are as follows:"
msgstr ""
"<filename>/proc/vmstat</filename> : このファイルには VM の内部動作を詳しく分"
"解して表示したものが含まれています。含まれている情報は実装に依存して決まるも"
"のであり、環境によって表示される内容が異なります。いくつかの項目は "
"<filename>/proc/meminfo</filename> でも表示されていますが、それ以外の項目は"
"ユーティリティを使用してわかりやすく表示するのがよいでしょう。また、情報を最"
"大限に活用するため、このファイルは変化率を観察するために長時間監視しておくこ"
"とをお勧めします。他の情報源からは導き出すのが難しい主な情報は下記のとおりで"
"す:"

#. Tag: term
#: tuning_memory.xml:817
#, no-c-format
msgid "<literal>pgscan_kswapd_*, pgsteal_kswapd_*</literal>"
msgstr "<literal>pgscan_kswapd_*, pgsteal_kswapd_*</literal>"

#. Tag: para
#: tuning_memory.xml:819
#, no-c-format
msgid ""
"These report respectively the number of pages scanned and reclaimed by "
"<command>kswapd</command> since the system started. The ratio between these "
"values can be interpreted as the reclaim efficiency with a low efficiency "
"implying that the system is struggling to reclaim memory and may be "
"thrashing. Light activity here is generally not something to be concerned "
"with."
msgstr ""
"これらのレポートには、システムが起動してからスキャンしたページ数の合計と、 "
"<command>kswapd</command> が回収したページ数の合計が書かれています。これらの"
"値の比率は回収効率として解釈することができ、この値が低い場合は、システムがメ"
"モリを回収するのに苦労していて、おそらく使用する前に回収されてしまっているこ"
"とを表しています。処理が軽い場合は、あまり気にする必要はありません。"

#. Tag: term
#: tuning_memory.xml:830
#, no-c-format
msgid "<literal>pgscan_direct_*, pgsteal_direct_*</literal>"
msgstr "<literal>pgscan_direct_*, pgsteal_direct_*</literal>"

#. Tag: para
#: tuning_memory.xml:832
#, no-c-format
msgid ""
"These report respectively the number of pages scanned and reclaimed by an "
"application directly. This is correlated with increases in the "
"<literal>allocstall</literal> counter. This is more serious than "
"<command>kswapd</command> activity as these events indicate that processes "
"are stalling. Heavy activity here combined with <command>kswapd</command> "
"and high rates of <literal>pgpgin</literal> , <literal>pgpout</literal> and/"
"or high rates of <literal>pswapin</literal> or <literal>pswpout</literal> "
"are signs that a system is thrashing heavily."
msgstr ""
"これらのレポートには、アプリケーションが直接スキャンしたページ数の合計と、回"
"収したページ数の合計が書かれています。これは <literal>allocstall</literal> の"
"カウンタ値と相関しています。これらのイベントが多く発生している場合、プロセス"
"の動作が一時的に止まっていることを示すことから、 <command>kswapd</command> で"
"の処理より深刻な問題となります。 <command>kswapd</command> での処理が重く、 "
"<literal>pgpgin</literal> , <literal>pgpout</literal> の値が高いか、もしくは "
"<literal>pswapin</literal> や <literal>pswpout</literal> の値が高い場合は、シ"
"ステムが過剰にスラッシングしている (実際に使用される前に回収されてしまってい"
"る) ことを表しています。"

#. Tag: para
#: tuning_memory.xml:843
#, no-c-format
msgid "More detailed information can be obtained using tracepoints."
msgstr "より詳しい情報を得たい場合は、トレースポイントを使用してください。"

#. Tag: term
#: tuning_memory.xml:849
#, no-c-format
msgid "<literal>thp_fault_alloc, thp_fault_fallback</literal>"
msgstr "<literal>thp_fault_alloc, thp_fault_fallback</literal>"

#. Tag: para
#: tuning_memory.xml:851
#, no-c-format
msgid ""
"These counters correspond to how many THPs were allocated directly by an "
"application and how many times a THP was not available and small pages were "
"used. Generally a high fallback rate is harmless unless the application is "
"very sensitive to TLB pressure."
msgstr ""
"これらのカウンタは THP がアプリケーションから直接割り当てられた回数と、 THP "
"が利用できずに小さいページを使用した回数を表しています。 thp_fault_fallback "
"の値が大きい場合でも、アプリケーションが TLB の圧力に非常に敏感でない限り、有"
"害ではありません。"

#. Tag: term
#: tuning_memory.xml:860
#, no-c-format
msgid "<literal>thp_collapse_alloc, thp_collapse_alloc_failed</literal>"
msgstr "<literal>thp_collapse_alloc, thp_collapse_alloc_failed</literal>"

#. Tag: para
#: tuning_memory.xml:862
#, no-c-format
msgid ""
"These counters correspond to how many THPs were allocated by "
"<command>khugepaged</command> and how many times a THP was not available and "
"small pages were used. A high fallback rate implies that the system is "
"fragmented and THPs are not being used even when the memory usage by "
"applications would allow them. It is only a problem for applications that "
"are sensitive to TLB pressure."
msgstr ""
"これらのカウンタは、 <command>khugepaged</command> が割り当てた THP の回数"
"と、 THP が利用できずに小さなページを使用した回数を示しています。 <!-- NOTE: "
"not fallback, but failed? --> failed の割合が高い場合、システムがフラグメント"
"状態にあり、アプリケーション側が許可したメモリ使用量であるにもかかわらず、 "
"THP が使用されていないことを表しています。アプリケーションが TLB の圧力に敏感"
"である場合にのみ、問題となる項目です。"

#. Tag: term
#: tuning_memory.xml:873
#, no-c-format
msgid ""
"<literal>compact_*_scanned, compact_stall, compact_fail, compact_success</"
"literal>"
msgstr ""
"<literal>compact_*_scanned, compact_stall, compact_fail, compact_success</"
"literal>"

#. Tag: para
#: tuning_memory.xml:875
#, no-c-format
msgid ""
"These counters may increase when THP is enabled and the system is "
"fragmented. <literal>compact_stall</literal> is incremented when an "
"application stalls allocating THP. The remaining counters account for pages "
"scanned, the number of defragmentation events that succeeded or failed."
msgstr ""
"これらのカウンタは THP が有効化され、システムがフラグメントしている場合に増え"
"るものです。 <literal>compact_stall</literal> は THP の割り当てに際してアプリ"
"ケーションの動作が一時的に停止した場合に増加します。それ以外のカウンタは、ス"
"キャンしたページの数と、成功もしくは失敗したデフラグイベントの数を表していま"
"す。"

