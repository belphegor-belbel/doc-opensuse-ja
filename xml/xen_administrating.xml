<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-xen-admin">
 <title>管理作業</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para/>
 <sect1 xml:id="sec-xen-config-bootloader">
  <title>ブートローダプログラム</title>

  <para>ブートローダには仮想化ソフトウエアの起動と実行に関する制御が含まれています。 &yast; を使用するか、もしくは直接設定ファイルを編集することで、ブートローダプログラムの設定を変更することができます。</para>

  <para>&yast; のブートローダプログラムは <menuchoice> <guimenu>&yast;</guimenu> <guimenu>システム</guimenu> <guimenu>ブートローダ</guimenu> </menuchoice> にあります。 <guimenu>ブートローダのオプション</guimenu> タブを選択して、 &xen; カーネルを含むものを <guimenu>既定のブートセクション</guimenu> で選択してください。</para>

  <figure>
   <title>ブートローダの設定</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>設定が終わったら <guimenu>OK</guimenu> を押します。これでホストの次回の起動時に &xen; が読み込まれ、 &xen; 仮想化環境を使用できるようになります。</para>

  <para>ブートローダプログラムを使用することで、様々な機能を調整することができます。具体的には下記のようなものがあります:</para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>カーネルのコマンドラインパラメータの指定</para>
   </listitem>
   <listitem>
    <para>カーネルイメージと初期 RAM ディスクの指定</para>
   </listitem>
   <listitem>
    <para>特定のハイパーバイザの選択</para>
   </listitem>
   <listitem>
    <para>ハイパーバイザに対する追加パラメータの指定。詳しくは <link xlink:href="http://xenbits.xen.org/docs/unstable/misc/xen-command-line.html"/> (英語) をお読みください。</para>
   </listitem>
  </itemizedlist>

  <para>仮想化環境のカスタマイズに際しては、 <filename>/etc/default/grub</filename> ファイルを編集してください。このファイルに対して、 <literal>GRUB_CMDLINE_XEN="<replaceable>&lt;パラメータ&gt;"</replaceable></literal> のような行を追加します。なお、編集後は忘れずに <command>grub2-mkconfig -o /boot/grub2/grub.cfg</command> を実行してください。</para>
 </sect1>
 <sect1 xml:id="sec-xen-config-sparse">
  <title>スパースイメージファイルとディスク領域</title>

  <para>ホスト側の物理ディスクの容量がいっぱいになり空き容量が無くなると、スパース (疎な) イメージファイルをベースとした仮想ディスクを使用している仮想マシンは、ディスクへの書き込みができなくなります。これにより、 I/O エラーが発生することになります。</para>

  <para>このような状況に陥ってしまった場合、まずは物理ディスク側にある不要なファイルを削除するなどして容量を空けてください。その後、仮想マシン側のファイルシステムを再マウントして、ファイルシステムを書き込み可能な状態に戻してください。</para>

  <para>スパースイメージファイルが実際に使用している容量を調べるには、 <command>du -h <replaceable>&lt;イメージファイル名&gt;</replaceable></command> を実行します。</para>

  <para>スパースイメージファイルの容量を増やすには、まずファイルサイズを増やしてから、ファイルシステム側のサイズを増やしてください。</para>

  <warning>
   <title>サイズ変更を実施する前にバックアップを採取しておく必要性について</title>
   <para>パーティションのサイズを変更する場合もスパースファイルのサイズを変更する場合も、処理の失敗に備えて必ずバックアップを採取しておいてください。バックアップ無しに作業を行なってはなりません。</para>
  </warning>

  <para>イメージファイルのサイズ変更はオンラインで実施することができます。つまり、 &vmguest; が動作している間でも実行することができます。スパースイメージファイルのサイズを拡張するには、下記のコマンドを入力して実行します:</para>

<screen>&prompt.sudo;dd if=/dev/zero of=<replaceable>&lt;イメージファイル&gt;</replaceable> count=0 bs=1M seek=<replaceable>&lt;サイズ_(メガバイト単位)&gt;</replaceable></screen>

  <para>たとえば <filename>/var/lib/xen/images/sles/disk0</filename> というファイルを 16GB まで拡張させたい場合は、下記を実行します:</para>

<screen>&prompt.sudo;dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 count=0 bs=1M seek=16000</screen>

  <note>
   <title>非スパースイメージの拡張</title>
   <para>非スパースファイルをベースにしたイメージファイルの場合であっても、イメージファイルのサイズを拡張することができます。ただし、この場合は現時点でのイメージサイズを正確に知っておかなければなりません。現時点でのイメージサイズを seek パラメータに設定して、拡張するサイズを count サイズで指定します:</para>
<screen>&prompt.sudo;dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 seek=8000 bs=1M count=2000</screen>
  </note>

  <para>なお、 seek の値を間違えてはなりません。誤った値を指定してしまうと、データの一部を消してしまうことになります。</para>

  <para>サイズ変更の作業時に &vmguest; が動作中であった場合、 &vmguest; 側にイメージを提供しているループバックデバイス側でもサイズ変更を行なう必要があります。まずは下記のコマンドを実行して、現在使用しているループバックデバイスを検出します:</para>

<screen>&prompt.sudo;losetup -j /var/lib/xen/images/sles/disk0</screen>

  <para>あとはループバックデバイスのサイズを変更します。たとえばループバックデバイスが <filename>/dev/loop0</filename> である場合は、下記のようなコマンドになります:</para>

<screen>&prompt.sudo;losetup -c /dev/loop0</screen>

  <para>あとはゲストシステム内でブロックデバイスのサイズを確認します。具体的には <command>fdisk -l /dev/xvdb</command> のようなコマンドを入力して実行し、確認を行ないます。なお、デバイス名はお使いの環境に合わせて変更してください。</para>

  <para>スパースファイル内にあるファイルシステム側の変更については、使用しているファイルシステムの種類によって異なりますので、対応するツールをお使いのうえ変更を行なってください。 <phrase os="sles">詳しくは <xref linkend="book-storage"/> をお読みください。</phrase></para>
 </sect1>
 <sect1 xml:id="sec-xen-manage-migrate">
  <title>&xen; &vmguest; システムの移行</title>

  <para>&xen; では、ほとんどサービスの停止を伴うことなく &vmguest; のシステムを一方の &vmhost; から他方の &vmhost; に移行することができます。これはたとえば、負荷の重い &vmguest; をより強力なハードウエアの搭載された &vmhost; に移行したり、負荷に余裕のある &vmhost; に移行したりする際に使用します。また、何らかの理由で &vmhost; を停止させる必要が発生した場合、その中で動作している &vmguest; を他の &vmhost; に移行して、サービスの停止を回避するためにも使用します。ここには 2 種類の理由しか記述していませんが、実際にはさまざまな理由で移行が必要となることがあります。</para>

  <para>移行を開始する前に、まずは &vmhost; に関するいくつかの事前要件を考慮する必要があります:</para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>移行に関わる全ての &vmhost; のシステムで、同じような CPU を使用する必要があります。動作周波数などは異なっていてもかまいませんが、同一のファミリの CPU を使用しておく必要があります。 CPU に関する詳細は、 <command>cat /proc/cpuinfo</command> を実行すると取得することができます。</para>
   </listitem>
   <listitem>
    <para>特定のゲストシステムが使用している全てのリソースは、移行に関わる全ての &vmhost; から使用できなければなりません。具体的には、使用している全てのブロックデバイスは、両方の &vmhost; システム内に存在していなければなりません。</para>
   </listitem>
   <listitem>
    <para>移行処理に関わる &vmhost; が異なるサブネット内に存在している場合、ゲストに対して DHCP リレーを提供する必要があります。なお、ゲスト側で固定のネットワーク設定を使用している場合、ネットワークの設定果て座興で修正する必要があります。</para>
   </listitem>
   <listitem>
    <para><literal>&pciback;</literal> などの特殊機能を使用していると、問題が発生する場合があります。異なる &vmhost; 間で &vmguest; を移行する可能性がある場合は、これらの機能を使用しないでください。</para>
   </listitem>
   <listitem>
    <para>移行を高速に行なうには、ネットワークも高速化してください。可能であればギガビットイーサネットで高速なスイッチを使用してください。また、 VLAN を使用することで、衝突を防ぐことができます。</para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-xen-manage-migrate-block">
   <title>移行のためのブロックデバイスの準備</title>
   <para>&vmguest; 側で必要となるブロックデバイスは、移行に関わる全ての &vmhost; システムからアクセスできなければなりません。これは移行を行なう &vmguest; システムのルートファイルシステムを、共有型ストレージ内に配置することで実現することができます。一般的には下記のようなものがあります:</para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para><literal>iSCSI</literal>: 複数のシステムに対して、同時に同一のブロックデバイスにアクセスする機能を提供することができます。 <phrase os="sles">iSCSI に関する詳細は、 <xref linkend="cha-iscsi"/> をお読みください。</phrase></para>
    </listitem>
    <listitem>
     <para><literal>NFS</literal>: 複数のシステムから容易にアクセスすることのできるルートファイルシステムとして、幅広く使用されています。詳しくは <xref linkend="cha-nfs"/> をお読みください。</para>
    </listitem>
    <listitem>
     <para><literal>DRBD</literal>: 2 台の &vmhost; システム間でのみ使用することのできるシステムです。この仕組みを使用すると、ネットワークを介してデータが複製されることになりますので、データの保全性をさらに高めることができます。 <phrase os="sles;sled">詳しくは、 <link xlink:href="&dsc-ha;/"/> で提供されている <citetitle>SUSE Linux Enterprise High Availability Extension &productnumber;</citetitle> の文書をお読みください。 </phrase></para>
    </listitem>
    <listitem>
     <para><literal>SCSI</literal>: ハードウエア側で共有が許可されていれば、同じディスクに対して複数のシステムがアクセスすることができます。</para>
    </listitem>
    <listitem>
     <para><literal>NPIV</literal>: ファイバチャネルディスクを使用するための特殊なモードです。ただし、この場合は移行に関わる &vmhost; が同じファイバチャネルのスイッチに接続されていなければなりません。 NPIV に関する詳細は、 <xref linkend="sec-xen-config-disk"/> をお読みください。一般的に、これは 4 ギガビット以上のファイバチャネル接続環境でのみ動作します。</para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-xen-manage-migrate-xl">
   <title>&vmguest; システムの移行</title>
   <para>&vmguest; システムの実際の移行作業は、下記のコマンドで行ないます:</para>
<screen>&prompt.sudo;xl migrate <replaceable>&lt;ドメイン名&gt;</replaceable> <replaceable>&lt;ホスト名&gt;</replaceable></screen>
   <para>移行の速度はメモリのディスクへの書き込み速度と、新しい &vmhost; への送信速度、そして新しい &vmhost; での読み込み速度によって決まります。つまり、メモリの少ない &vmguest; のほうが、メモリの多い &vmguest; よりも素早く移行できることになります。</para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-xen-monitor">
  <title>&xen; の監視</title>

  <para>多数の仮想化ゲストを定常的に管理している場合、動作している多数の &vmguest; の正常性を監視する作業が欠かせません。 &xen; では、システムに関する情報を収集するためのさまざまなシステムツールを提供しています。</para>

  <tip>
   <title>&vmhost; の監視</title>
   <para>&vmhost; の基本的な監視 (I/O および CPU) については、 &vmm; 側で提供しています。詳しくは <xref linkend="cha-libvirt-admin-monitor-virt-manager"/> をお読みください。</para>
  </tip>

  <sect2 xml:id="sec-xen-monitor-xentop">
   <title><command>xentop</command> を利用した &xen; の管理</title>
   <para>&xen; 仮想化環境で情報を収集するために使用する端末アプリケーションとして、 <command>xentop</command> が用意されています。ただし、このツールを使用するにはサイズの大きな (文字数の多い) 端末が必要となります。サイズの小さな端末を使用してしまうと、表示が改行されて読みにくくなってしまいます。</para>
   <para><command>xentop</command> には監視の際の設定を変更するためのコマンドキーが用意されています。主なものは下記のとおりです:</para>
   <variablelist>
    <varlistentry>
     <term>D</term>
     <listitem>
      <para>画面の更新間隔を指定します。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>N</term>
     <listitem>
      <para>ネットワークの統計情報も表示するようにします。なお、標準的な設定のみが表示されます。また、ルーティング型ネットワークのような特殊な設定にも対応していません。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>B</term>
     <listitem>
      <para>接続されているブロックデバイスと、その累積しよう回数を表示します。</para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para><command>xentop</command> に関する詳細は、 <command>man 1 xentop</command> で表示されるマニュアルページをお読みください。</para>

   <tip>
    <title><command>virt-top</command></title>
    <para>libvirt にはハイパーバイザに依存しない <command>virt-top</command> というツールが提供されています。こちらも &vmguest; の監視にはお勧めです。詳しくは <xref linkend="cha-libvirt-admin-monitor-virt-top"/> をお読みください。</para>
   </tip>

  </sect2>

  <sect2 xml:id="sec-xen-monitor-tools">
   <title>追加のツール</title>
   <para>稼働中の <phrase os="sles;sled">&sle;</phrase> <phrase os="osuse">&opensuse;</phrase> システムを監視したりデバッグしたりするためのシステムツールは数多く存在しています。それらのうちの多くは <xref linkend="cha-util"/> で説明を行なっています。ここでは特に、仮想化環境を監視するためのツールについて説明しています:</para>
   <variablelist>
    <varlistentry>
     <term>ip</term>
     <listitem>
      <para>コマンドラインユーティリティである <command>ip</command> コマンドは、任意のネットワークインターフェイスの監視を行なうことができるコマンドです。このコマンドは、ルーティング型のネットワークやマスカレード型のネットワークを構成しているような場合、特に有用です。たとえば <literal>&xenguest;.0</literal> という名前のネットワークインターフェイスを監視したい場合は、下記のコマンドを実行します:</para>
<screen>&prompt.user;watch ip -s link show &xenguest;.0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bridge</term>
     <listitem>
      <para>標準的な設定では、全ての &xen; &vmguest; は仮想ネットワークブリッジに接続されます。 <command>bridge</command> コマンドを実行すると、 &vmguest; システム内の仮想ネットワークアダプタが、どのブリッジに接続されているのかを知ることができます。この場合は、 <command>bridge link</command> と入力して実行します。出力は下記のようになります:</para>
<screen>
2: eth0 state DOWN : &lt;NO-CARRIER, ...,UP&gt; mtu 1500 master br0
8: vnet0 state UNKNOWN : &lt;BROADCAST, ...,LOWER_UP&gt; mtu 1500 master virbr0 \
  state forwarding priority 32 cost 100
</screen>
      <para>上記の出力では、 2 種類の仮想ブリッジがシステム内に定義されています。一方は物理イーサネットデバイスである <literal>eth0</literal> に、もう一方は VLAN インターフェイスである <literal>vnet0</literal> に接続されています。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iptables-save</term>
     <listitem>
      <para>マスカレード型のネットワークを使用している場合や、イーサネットインターフェイスに対してファイアウオールの設定を行なっている場合、現在のファイアウオールルールを確認しておく必要がある場合があります。</para>
      <para>この場合、 <command>iptables</command> コマンドを実行することで、さまざまなファイアウオール設定を一括で表示することができます。チェイン内の全てのルールを表示したい場合、もしくは全ての設定を出力したい場合は、 <command>iptables-save</command> もしくは <command>iptables -S</command> を実行してください。</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-xen-admin-vhostmd">
  <title>&vmguest; システムに対するホスト情報の提供</title>

  <para>標準的な &xen; 環境では、 &vmguest; システムに対しては、 &vmhost; システムの一部の情報しか伝達されません。ゲスト側でさらに詳しい &vmhost; の情報を提供する必要がある場合は、 <systemitem>vhostmd</systemitem> を利用して、選択したゲストに情報提供を行なってください。お使いのシステムで <systemitem>vhostmd</systemitem> を動作させるには、下記の手順を行ないます:</para>

  <procedure>
   <step>
    <para>まずは &vmhost; に vhostmd パッケージをインストールします。</para>
   </step>
   <step>
    <para>設定内に <literal>metric</literal> セクションを追加または削除するには、 <filename>/etc/vhostmd/vhostmd.conf</filename> ファイルを編集します。ただし、既定値のままでもかまいません。</para>
   </step>
   <step>
    <para>下記のコマンドを実行して、 <filename>vhostmd.conf</filename> 設定ファイルの書式が正しいことを確認します:</para>
<screen>&prompt.user;cd /etc/vhostmd
&prompt.user;xmllint --postvalid --noout vhostmd.conf
    </screen>
   </step>
   <step>
    <para>あとは <command>sudo systemctl start vhostmd</command> を実行して、 vhostmd デーモンを開始します。</para>
    <para>vhostmd をシステムの起動時に開始するように設定したい場合は、下記を実行します:</para>
<screen>&prompt.sudo;systemctl enable vhostmd</screen>
   </step>
   <step>
    <para>&xenguest; という名前の &vmguest; に対して、 <filename>/dev/shm/vhostmd0</filename> というイメージファイルを接続するため、下記のコマンドを実行します:</para>
<screen>&prompt.user;xl block-attach opensuse /dev/shm/vhostmd0,,xvdb,ro</screen>
   </step>
   <step>
    <para>&vmguest; 側のシステムにログインします。</para>
   </step>
   <step>
    <para>クライアント側のパッケージ <systemitem>vm-dump-metrics</systemitem> をインストールします。</para>
   </step>
   <step>
    <para><command>vm-dump-metrics</command> コマンドを実行します。結果をファイルに保存したい場合は、 <systemitem>-d <replaceable>&lt;ファイル名&gt;</replaceable></systemitem> オプションを追加してください。</para>
   </step>
  </procedure>

  <para><systemitem>vm-dump-metrics</systemitem> の出力は XML 形式で行なわれます。また、出力される内容は <filename>/etc/vhostmd/metric.dtd</filename> の DTD 定義に従って行なわれます。</para>

  <para>さらに詳しい情報については、 <command>man 8 vhostmd</command> で表示されるマニュアルページのほか、 &vmhost; システム側にある <filename>/usr/share/doc/vhostmd/README</filename> (英語) ファイルをお読みください。ゲスト側では、 <command>man 1 vm-dump-metrics</command> のマニュアルページもお読みいただくことができます。</para>
 </sect1>
</chapter>
