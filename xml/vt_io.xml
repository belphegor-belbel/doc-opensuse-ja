<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1 [
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="sec-vt-io">
 <title>I/O の仮想化</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
  </dm:docmanager>
 </info>

 <para>&vmguest; はホストシステム内の CPU やメモリリソースを共有するだけでなく、 I/O サブシステムを共有することもできます。ソフトウエアによる I/O 仮想化技術はベアメタル (物理的なサーバ) に比べると性能が落ちますが、ハードウエアによる技術を使用すると、ほぼ <quote>ネイティブ</quote> に近い性能を発揮できるようになっています。 &productname; では、下記のような I/O 仮想化技術に対応しています:</para>

 <variablelist>
  <varlistentry xml:id="vt-io-fullv">
   <term>完全仮想化</term>
   <listitem>
    <para>完全仮想化 (Fully Virtualized (FV)) ドライバでは、幅広く使用されている実在デバイスを擬似する仕組みであるため、 &vmguest; 内でも既存のドライバを使用することができます。ゲストは <emphasis>ハードウエア仮想マシン</emphasis> (Hardware Virtual Machine; HVM) も呼ぶこともあります。 &vmhost; 内の物理デバイスは擬似されているデバイスとは異なるため、ハイパーバイザはゲスト側からの全ての I/O 操作を物理デバイスに渡す際、それらを正しく処理しなければならないことを意味します。そのため、全ての I/O 操作はソフトウエア層にまで引き上げて処理を行うことになりますので、 I/O 性能がかなり落ちてしまうだけでなく、 CPU 側にも負荷がかかることになります。</para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt-io-parav">
   <term>準仮想化</term>
   <listitem>
    <para>準仮想化 (Paravirtualization (PV)) ではハイパーバイザと &vmguest; の間で直接的な通信を行います。オーバーヘッドの少ない仕組みであることから、性能は完全仮想化に比べてずっと良好になります。しかしながら、準仮想化ではゲスト側のオペレーティングシステムを修正し、準仮想化 API に対応させるようにするか、準仮想化用のドライバを使用する必要があります。 <phrase os="sles;sled">準仮想化に対応するゲスト側のオペレーティングシステムの一覧について、詳しくは <xref linkend="sec-kvm-requires-guests-virt-drivers"/> をお読みください。</phrase></para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt-io-pvhvm">
   <term>PVHVM</term>
   <listitem>
    <para>この種類の仮想化では、 HVM (<xref linkend="vt-io-fullv"/> をお読みください) でありながら、準仮想化 (PV) ドライバと PV による割り込みやタイマー処理を使用します。</para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt-io-vfio">
   <term>VFIO</term>
   <listitem>
    <para>VFIO は <emphasis>Virtual Function I/O</emphasis> の略で、 Linux 向けの新しいユーザレベルドライバのフレームワークです。従来の &kvm; &pciback; デバイス割り当てを置き換えるものです。 VFIO ドライバはユーザスペースに対して、メモリの機密を保持 ( <xref linkend="gloss-vt-acronym-iommu"/> ) しながら、保護環境下で直接のデバイスアクセス機能を提供するものです。 VFIO を利用することで、 &vmguest; は &vmhost; 内のハードウエアデバイスに、性能面のクリティカルパスの擬似による性能劣化を引き起こすことなく、直接アクセスできるようになります。ただし、この方式はデバイスの共有には使用できません。各デバイスは単一の &vmguest; にのみ割り当てることができます。また、 VFIO は &vmhost; の CPU やチップセット、 BIOS/EFI でそれぞれ対応している必要があります。</para>
    <para>従来型の &kvm; PCI デバイス割り当てと比較して、 VFIO には下記のような利点があります:</para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>リソースへのアクセスは Secure Boot との互換性があります。</para>
     </listitem>
     <listitem>
      <para>デバイスは孤立した存在になり、メモリアクセスも保護されます。</para>
     </listitem>
     <listitem>
      <para>より柔軟なデバイス所有者モデルを採用した、ユーザスペースのデバイスドライバが提供されています。</para>
     </listitem>
     <listitem>
      <para>&kvm; 技術とは独立しているほか、 x86 アーキテクチャのみに固有のものでもありません。</para>
     </listitem>
    </itemizedlist>
    <para><phrase os="sles;sled">&sls; 12 SP2</phrase> <phrase os="osuse">openSUSE 42.2</phrase> およびそれ以降のバージョンでは、 USB と PCI のパススルーによるデバイスの割り当ては廃止予定とされ、 VFIO モデルで置き換えられています。</para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt-io-sriov">
   <term>SR-IOV</term>
   <listitem>
    <para>最新の I/O 仮想化技術で、 Single Root I/O Virtualization の略です。 SR-IOV では前述の技術の利点を組み合わせて作られています。性能を強化しているだけでなく、複数の &vmguest; でデバイスを共有する機能を備えています。 SR-IOV では、リソースを複製して複数のデバイスとしてアクセスすることのできる、特殊な I/O デバイスが必要となります。この場合、それぞれの <quote>擬似</quote> デバイスは 1 台のゲストからアクセスすることになります。しかしながら、たとえばネットワークカードなどの場合、同時に使用することのできるキュー数は制限されているため、準仮想化に比べると &vmguest; の性能が落ちる可能性があることになります。 &vmhost; 側としては、 SR-IOV が I/O デバイス側と CPU 、チップセットと BIOS/EFI 、そしてハイパーバイザでそれぞれ対応していなければなりません。詳しい設定手順については、 <xref linkend="sec-libvirt-config-pci"/> をお読みください。 <!-- fs 2014-02-21: no list available ATM &productname; supports the SRV-IOV capable network cards listed below --></para>
   </listitem>
  </varlistentry>
 </variablelist>

 <important xml:id="ann-vt-io-require">
  <title>VFIO および SR-IOV の要件について</title>
  <para>VFIO や SR-IOV の機能を使用できるようにするには、 &vmhost; 側が下記の要件を満たす必要があります:</para>
  <itemizedlist>
   <listitem>
    <para>BIOS や EFI で IOMMU が有効化されていること。</para>
   </listitem>
   <listitem>
    <para>Intel CPU の場合、カーネルのコマンドライン内のパラメータに <literal>intel_iommu=on</literal> を追加する必要があります。詳しくは <xref linkend="sec-grub2-yast2-config-advanced-kernel-parameters"/> をお読みください。</para>
   </listitem>
   <listitem>
    <para>VFIO インフラストラクチャを利用できるようにする必要があります。こちらはカーネルモジュールである <systemitem class="resource">vfio_pci</systemitem> を読み込むことで、利用できるようになります。詳しくは <xref linkend="sec-boot-systemd-advanced-kernel-modules"/> をお読みください。</para>
   </listitem>
  </itemizedlist>
 </important>
</sect1>
