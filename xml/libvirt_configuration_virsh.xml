<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="libvirt_configuration_virsh.xml" version="5.0" xml:id="cha-libvirt-config-virsh">
 <title>&virsh; を利用した仮想マシンの設定</title>
 <info>
  <abstract>
   <para>&vmguest; を設定するにあたっては、 &vmm; だけでなく &virsh; も使用することができます。 &virsh; は仮想マシン (VM) を管理するためのコマンドラインツールで、これを利用することで、 VM の状態の制御や設定の編集、他のホストへの移行などを行うことができます。下記の章では、 &virsh; を利用した管理の方法について説明しています。</para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-libvirt-config-editing-virsh">
  <title>VM の設定変更</title>

  <para>仮想マシンの設定ファイルは <filename>/etc/libvirt/qemu/</filename> 内に XML 形式で保存されていて、下記のような内容になっているはずです:</para>

  <example>
   <title>XML 設定ファイルの例</title>
<screen>
&lt;domain type='kvm'&gt;
  &lt;name&gt;sles15&lt;/name&gt;
  &lt;uuid&gt;ab953e2f-9d16-4955-bb43-1178230ee625&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;2097152&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;2097152&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;
  &lt;/os&gt;
  &lt;features&gt;...&lt;/features&gt;
  &lt;cpu mode='custom' match='exact' check='partial'&gt;
    &lt;model fallback='allow'&gt;Skylake-Client-IBRS&lt;/model&gt;
  &lt;/cpu&gt;
  &lt;clock&gt;...&lt;/clock&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;destroy&lt;/on_crash&gt;
  &lt;pm&gt;
    &lt;suspend-to-mem enabled='no'/&gt;
    &lt;suspend-to-disk enabled='no'/&gt;
  &lt;/pm&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;...&lt;/disk&gt;
  &lt;/devices&gt;
  ...
&lt;/domain&gt;
</screen>
  </example>

  <para>&vmguest; の設定を変更する場合は、まずシャットオフ状態になっているかどうかを確認します:</para>

<screen>&prompt.sudo;<command>virsh list --inactive</command></screen>

  <para>上記のコマンドの実行結果に編集対象の &vmguest; が現れている場合は、そのまま設定を変更してかまいません:</para>

<screen>&prompt.sudo;<command>virsh edit <replaceable>VM_名</replaceable></command>
    </screen>

  <para>なお設定の保存時には、 &virsh; が RelaxNG スキーマを利用して設定内容のチェックを行います。</para>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-machinetype-virsh">
  <title>マシンの種類の変更</title>

  <para><command>virt-install</command> ツールを利用してインストールを行った場合、 &vmguest; のマシンの種類は、既定で <emphasis>pc-q35</emphasis> になります。マシンの種類も &vmguest; の設定ファイル内に含まれていて、 <tag>type</tag> というタグ要素内に書かれています:</para>

<screen>&lt;type arch='x86_64' machine='pc-q35-2.3'&gt;hvm&lt;/type&gt;</screen>

  <para>下記の手順では、例としてマシンの種類を <literal>q35</literal> に変更します。 <literal>q35</literal> という値は Intel* 社のチップセットを表す文字列で、 <xref linkend="gloss-vt-acronym-pcie"/> が含まれているほか、最大で 12 個までの USB ポートに対応し、 <xref linkend="gloss-vt-acronym-sata"/> や <xref linkend="gloss-vt-acronym-iommu"/> にも対応しています。 <!-- IRQ routing has also been improved. --></para>

  <procedure>
   <title>マシンの種類の変更</title>
   <step>
    <para>まずは &vmguest; が停止していることを確認します:</para>
<screen>&prompt.sudo;<command>virsh list --inactive</command>
Id    名前                           状態
----------------------------------------------------
-     sles15                         シャットオフ</screen>
   </step>
   <step>
    <para>この &vmguest; に対する設定を編集します:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
   </step>
   <step>
    <para>タグ内の <tag class="attribute">machine</tag> という属性の値を、 <tag class="attvalue">pc-q35-2.0</tag> に変更します:</para>
<screen>&lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;</screen>
   </step>
   <step>
    <para>&vmguest; を起動し直します:</para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
   <step>
    <para>マシンの種類が変更されていることを確認します。 &vmguest; を起動してログインし、下記のコマンドを実行します:</para>
<screen>&prompt.sudo;<command>dmidecode | grep Product</command>
Product Name: Standard PC (Q35 + ICH9, 2009)</screen>
   </step>
  </procedure>

  <tip>
   <title>マシンの種類を更新した際の推奨事項について</title>
   <para>ホストシステム側の QEMU のバージョンをアップグレードした場合 (たとえば &vmhost; のディストリビューションのバージョンをアップグレードした場合など) 、 &vmguest; 側のマシンの種類についても、利用可能な最新版にアップグレードするようにしてください。どのような種類を指定できるのかを知りたい場合は、 &vmhost; 側で <command>qemu-system-x86_64 -M help</command> を実行してください。</para>
   <para>また、既定で使用されるマシン種類 (<literal>pc-i440fx</literal>) なども、定期的に更新が行われます。お使いの &vmguest; が今も <literal>pc-i440fx-1.<replaceable>X</replaceable></literal> のマシン種類で動作している場合は、 <literal>pc-i440fx-2.<replaceable>X</replaceable></literal> にアップグレードすることを強くお勧めします。これにより、最新の更新内容を受けることができることになり、不具合の修正や将来の互換性維持に役立つことになります。</para>
  </tip>
 </sect1>
 <sect1 xml:id="sec-libvirt-hypervisor-features-virsh">
  <title>ハイパーバイザ機能の設定</title>

  <para><command>libvirt</command> はほとんどの状況下において適切なハイパーバイザ設定を行うよう自動設定されていますが、必要に応じて特定の機能を有効化したり無効化したりすることができます。たとえば Xen の既定では PCI パススルーが有効化されていませんが、 <literal>passthrough</literal> を設定することで有効化することができます。ハイパーバイザの機能は &virsh; を利用して、 &vmguest; の設定ファイル内に <tag>&lt;features&gt;</tag> 要素を指定して設定します。たとえば &xen; での PCI パススルー機能を有効化するには、下記のように設定します:</para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;features&gt;
    &lt;xen&gt;
      &lt;passthrough/&gt;
    &lt;/xen&gt;
 &lt;/features&gt;
</screen>

  <para>あとは設定を保存して &vmguest; を再起動してください。</para>

  <para>詳しくは <link xlink:href="https://libvirt.org/formatdomain.html#elementsFeatures"/> (英語) にある libvirt の <citetitle>Domain XML format</citetitle> 内の <citetitle>Hypervisor features</citetitle> の章をお読みください。</para>
 </sect1>
 <sect1 xml:id="libvirt-cpu-virsh">
  <title>CPU の設定</title>

  <para>&virsh; を利用することで、 &vmguest; に対して提供される様々な仮想 CPU の設定を変更することができます。 &vmguest; に割り当てる CPU 数の現在値や最大値のほか、 CPU のモデルや機能セットなども設定することができます。下記の章では、 &vmguest; で一般的な CPU 設定を変更する方法を説明しています。</para>

  <sect2 xml:id="sec-libvirt-cpu-num-virsh">
   <title>CPU 数の設定</title>
   <para>&vmguest; の CPU の割当数は、 <filename>/etc/libvirt/qemu/</filename> にある XML 設定ファイル内の <tag class="attribute">vcpu</tag> タグ内に書かれています:</para>
<screen>&lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;</screen>

   <para>上記の例では、 &vmguest; 側には CPU を 1 つだけ割り当てていることになります。下記の手順では、 &vmguest; 側への CPU の割当数の変更方法を説明しています:</para>

   <procedure>
    <step>
     <para>まずは &vmguest; が停止していることを確認します:</para>
<screen>&prompt.sudo;<command>virsh list --inactive</command>
Id    名前                           状態
----------------------------------------------------
-     sles15                         シャットオフ</screen>
    </step>
    <step>
     <para>既存の &vmguest; に対する設定を編集します:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>CPU の割当数を変更して保存します:</para>
<screen>&lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;</screen>
    </step>
    <step>
     <para>&vmguest; を起動し直します:</para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
    </step>
    <step>
     <para>VM に対して割り当てられている CPU 数が変更されていることを確認します:</para>
<screen>
&prompt.sudo;<command>virsh vcpuinfo sled15</command>
VCPU:           0
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy

VCPU:           1
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy
</screen>
    </step>
   </procedure>
   <para>&vmguest; が動作中の場合であっても、 CPU 数を変更することができます。ただし、 &vmguest; の起動時に設定した最大数までしかホットプラグ (活性接続) することができませんし、逆に減らす場合の最小数は 1 であることにも注意してください。下記の例では、現在の CPU 数である 2 を、あらかじめ設定しておいた最大値 4 に変更する手順を説明しています。</para>
   <procedure>
    <step>
     <para>まずは現時点での CPU 数を確認します:</para>
<screen>&prompt.sudo;<command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           2
</screen>
    </step>
    <step>
     <para>CPU 数の現在値 (使用可能な CPU 数) を 4 に変更します:</para>
<screen>&prompt.sudo;<command>virsh setvcpus sles15 --count 4 --live</command></screen>
    </step>
    <step>
     <para>最後に vcpu 数が 4 になっていることを確認します:</para>
<screen>&prompt.sudo;<command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           4
</screen>
    </step>
   </procedure>
   <important>
     <title>255 個以上の CPU を設定する場合について</title>
     <para>&kvm; を利用する場合、 255 個以上の CPU を &vmguest; に設定することができます。ただし、 &vmguest; の起動時に追加の設定を行っておく必要があります。具体的には <literal>ioapic</literal> の機能を調整し、 IOMMU デバイスを &vmguest; に追加する必要があります。下記は 288 個の CPU を設定した場合の例です。</para>
<screen>
&lt;domain&gt;
 &lt;vcpu placement='static'&gt;288&lt;/vcpu&gt;
 &lt;features&gt;
  &lt;ioapic driver='qemu'/&gt;
 &lt;/features&gt;
 &lt;devices&gt;
  &lt;iommu model='intel'&gt;
   &lt;driver intremap='on' eim='on'/&gt;
  &lt;/iommu&gt;
 &lt;/devices&gt;
&lt;/domain&gt;
</screen>
   </important>
  </sect2>
  <sect2 xml:id="sec-libvirt-cpu-model-virsh">
   <title>CPU モデルの設定</title>
   <para>&vmguest; に対して提示される CPU モデルの設定は、その中で動作する処理に影響します。既定の CPU モデルは <literal>host-model</literal> という設定値で、ホスト側の CPU モデルをそのまま &vmguest; に提示する設定になります。</para>
<screen>&lt;cpu mode='host-model'/&gt;</screen>
   <para>CPU モデルとして <literal>host-model</literal> を指定して &vmguest; を起動すると、 &libvirt; はホストの CPU モデルをそのままコピーして &vmguest; 側に対して提供するようになります。 &vmguest; 側の定義にコピーされた CPU モデルと機能については、 <command>virsh capabilities</command> コマンドの出力で確認することができます。</para>
   <para>それ以外にも、 <literal>host-passthrough</literal> という値を設定することもできます。</para>
<screen>&lt;cpu mode='host-passthrough'/&gt;</screen>
   <para>CPU モデルに <literal>host-passthrough</literal> を指定して &vmguest; を起動すると、 &vmhost; の CPU はホスト側の CPU と全く同一になります。これは、 単純化された <literal>host-model</literal> CPU では提供されていない特別な機能を必要とする &vmguest; を動作させる際には便利な設定です。このほか、 &vmguest; を 4TB 以上のメモリで動作させるような場合にも、 <literal>host-passthrough</literal> モデルを設定する必要があります。ただし、この <literal>host-passthrough</literal> モデルの場合は、移行の柔軟性が低くなることに注意してください。具体的には、全く同一のハードウエア構成の &vmhost; 間でのみ移行が可能となります。</para>
   <para>このほか、 <literal>host-passthrough</literal> の CPU モデルを指定した場合で、特定の不要な機能のみを無効化したいような場合は、下記のように設定することもできます。下記の例では、ホスト側の CPU と全く同一のモデルでありながら、 <literal>vmx</literal> 機能のみを無効化しています。</para>
<screen>
&lt;cpu mode='host-passthrough'&gt;
  &lt;feature policy='disable' name='vmx'/&gt;
  &lt;/cpu&gt;
</screen>
   <para>また、 <literal>custom</literal> CPU モデルを指定することで、 CPU モデルを標準化し、クラスタ内にある異なる CPU 構成のホスト間の移行を可能にすることができます。たとえばクラスタ内に Nehalem, IvyBridge, SandyBridge の各 CPU モデルが混在しているような場合、 &vmguest; で <literal>custom</literal> CPU モデルを指定することで、 &vmguest; の CPU モデルを Nehalem に統一することができます。</para>
<screen>
&lt;cpu mode='custom' match='exact'&gt;
  &lt;model fallback='allow'&gt;Nehalem&lt;/model&gt;
  &lt;feature policy='require' name='vme'/&gt;
  &lt;feature policy='require' name='ds'/&gt;
  &lt;feature policy='require' name='acpi'/&gt;
  &lt;feature policy='require' name='ss'/&gt;
  &lt;feature policy='require' name='ht'/&gt;
  &lt;feature policy='require' name='tm'/&gt;
  &lt;feature policy='require' name='pbe'/&gt;
  &lt;feature policy='require' name='dtes64'/&gt;
  &lt;feature policy='require' name='monitor'/&gt;
  &lt;feature policy='require' name='ds_cpl'/&gt;
  &lt;feature policy='require' name='vmx'/&gt;
  &lt;feature policy='require' name='est'/&gt;
  &lt;feature policy='require' name='tm2'/&gt;
  &lt;feature policy='require' name='xtpr'/&gt;
  &lt;feature policy='require' name='pdcm'/&gt;
  &lt;feature policy='require' name='dca'/&gt;
  &lt;feature policy='require' name='rdtscp'/&gt;
  &lt;feature policy='require' name='invtsc'/&gt;
  &lt;/cpu&gt;
</screen>
   <para>&libvirt; の CPU モデルとトポロジのオプションについて、詳しくは <link xlink:href="https://libvirt.org/formatdomain.html#cpu-model-and-topology"/> (英語) にある <citetitle>CPU model and topology</citetitle> のドキュメンテーションをお読みください。</para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-boot-menu-virsh">
  <title>起動オプションの変更</title>

  <para>&vmguest; の起動メニューの設定は <tag>os</tag> タグ内に含まれ、下記のような内容になっています:</para>

<screen>&lt;os&gt;
  &lt;type&gt;hvm&lt;/type&gt;
  &lt;loader&gt;readonly='yes' secure='no' type='rom'/&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;nvram template='/usr/share/OVMF/OVMF_VARS.fd'/&gt;/var/lib/libvirt/nvram/guest_VARS.fd&lt;/nvram&gt;
  &lt;boot dev='hd'/&gt;
  &lt;boot dev='cdrom'/&gt;
  &lt;bootmenu enable='yes' timeout='3000'/&gt;
  &lt;smbios mode='sysinfo'/&gt;
  &lt;bios useserial='yes' rebootTimeout='0'/&gt;
  &lt;/os&gt;</screen>

  <para>上記の例では、 <tag class="attvalue">hd</tag> と <tag class="attvalue">cdrom</tag> という 2 つのデバイスが有効化されています。設定の順序は実際の起動順序にも影響し、上記の例では <tag class="attvalue">cdrom</tag> よりも前に <tag class="attvalue">hd</tag> の起動が試されることになります。</para>

  <sect2 xml:id="sec-libvirt-config-bootorder-virsh">
   <title>起動順序の変更</title>
   <para>&vmguest; の起動順序は、 XML 設定ファイル内での出現順序で表されます。つまり、デバイスのタグを入れ替えることで起動順序を変更できることになります。</para>
   <procedure>
    <step>
     <para>&vmguest; の XML 設定ファイルを開きます。</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>デバイスの順序を入れ替えます。</para>
<screen>
...
&lt;boot dev='cdrom'/&gt;
&lt;boot dev='hd'/&gt;
...
      </screen>
    </step>
    <step>
     <para>&vmguest; の BIOS 設定内の起動メニューを確認して、起動順序が変更されていることを確認します。</para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-config-directkernel-virsh">
   <title>直接カーネル起動の使用</title>
   <para>直接カーネル起動を使用することで、ホスト内に保存されているカーネルと initrd を利用して起動を行うことができます。この場合は、 <tag>kernel</tag> と <tag>initrd</tag> のタグを追加してファイルを指定します:</para>
<screen>&lt;os&gt;
    ...
  &lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
  &lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
    ...
&lt;os&gt;</screen>
   <para>直接カーネル起動を有効化するには、下記の手順を実施します:</para>
   <procedure>
    <step>
     <para>&vmguest; の XML 設定を開きます:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para><tag>os</tag> タグ内に <tag>kernel</tag> タグを追加し、ホスト側でのカーネルファイルのパスを指定します:</para>
<screen>...
&lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
...</screen>
    </step>
    <step>
     <para>同様に <tag>initrd</tag> タグを追加し、ホスト内での initrd ファイルのパスを指定します:</para>
<screen>...
&lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
...</screen>
    </step>
    <step>
     <para>あとは VM を起動すると、新しいカーネルでの起動が行われます:</para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-memory-virsh">
  <title>メモリ割り当ての設定</title>

  <para>&vmguest; に対するメモリ割当量の設定変更は、 &virsh; でも行うことができます。メモリ割当量は <tag>memory</tag> 要素内に書かれていて、この中に &vmguest; の起動時の最大メモリ割当量を設定します。また <tag>currentMemory</tag> という任意指定の要素を指定することで、 &vmguest; に割り当てる実際のメモリ量を指定することもできます。なお、 <tag>currentMemory</tag> の値を <tag>memory</tag> よりも少なくしておくことで、 &vmguest; の動作中にメモリ量を増やす (<emphasis>バルーン</emphasis> と呼びます) ことができます。また、 <tag>currentMemory</tag> の指定を省略すると、 <tag>memory</tag> と同じ値を指定したものと見なされます。</para>
  <para>なお、 &vmguest; の設定を編集することで、メモリ設定を変更することができますが、変更した内容は次回の起動時まで反映されないことに注意してください。下記の手順では、 &vmguest; に対して起動時に 4GB のメモリを割り当て、あとから 8GB まで増やす場合の例を示しています:</para>

  <procedure>
   <step>
    <para>&vmguest; の XML 設定を開きます:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
   </step>
   <step>
    <para><tag>memory</tag> タグを検索して、メモリの割当量を 8GB に変更します:</para>
<screen>...
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
...</screen>
   </step>
   <step>
    <para><tag>currentMemory</tag> 要素が存在していない場合は <tag>memory</tag> 要素内に <tag>currentMemory</tag> 要素を追加します。既に存在している場合は、値のみを変更します:</para>
<screen>
[...]
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
&lt;currentMemory unit='KiB'&gt;4194304&lt;/currentMemory&gt;
[...]</screen>
   </step>
  </procedure>
  <para>&vmguest; の動作中にメモリ割当量を変更したい場合は、 <command>setmem</command> サブコマンドを使用します。下記の手順では、メモリの割り当てを 8GB まで増やしています:</para>

   <procedure>
   <step>
    <para>現時点での &vmguest; のメモリ設定を表示します:</para>
<screen>&prompt.sudo;<command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    4194608 KiB
</screen>
   </step>
   <step>
    <para>8GB までメモリ量を増やします:</para>
<screen>&prompt.sudo;<command>virsh setmem sles15 8388608</command></screen>
   </step>
   <step>
    <para>変更が反映されたことを確認します:</para>
<screen>&prompt.sudo;<command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    8388608 KiB
</screen>
   </step>
  </procedure>
  <important>
   <title>巨大なメモリを使用する &vmguest; の場合について</title>
   <para>4TB 以上のメモリを必要とする &vmguest; を動作させる場合、現時点では <literal>host-passthrough</literal> CPU モデルを使用する必要があります。</para>
  </important>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-pci-virsh">
  <title>PCI デバイスの追加</title>

  <para>&virsh; を利用して &vmguest; に対して PCI デバイスを追加するには、下記の手順を実施します:</para>

  <procedure>
   <step>
    <para>まずは &vmguest; に割り当てるホスト側の PCI デバイスを識別します。下記の例では、 DEC 社のネットワークカードをゲストに割り当てようとしています:</para>
<screen>&prompt.sudo;<command>lspci -nn</command>
[...]
<emphasis role="bold">03:07.0</emphasis> Ethernet controller [0200]: Digital Equipment Corporation DECchip \
21140 [FasterNet] [1011:0009] (rev 22)
[...]</screen>
    <para>デバイス ID (上記の例では <literal>03:07.0</literal>) をメモしておきます。</para>
   </step>
   <step>
    <para><command>virsh nodedev-dumpxml <replaceable>ID</replaceable></command> を実行して、デバイスに関する詳細情報を取得します。ここで <replaceable>ID</replaceable> にはデバイス ID (この例では <literal>03:07.0</literal> ) を指定しますが、コロン (:) とピリオド (.) をアンダースコア (_) に置き換え、かつ <quote>pci_0000_</quote> という前置きを置いた値 (この例では <literal>pci_0000_03_07_0</literal> になります) を指定して実行します:</para>
<screen>&prompt.sudo;<command>virsh nodedev-dumpxml pci_0000_03_07_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_03_07_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:14.4/0000:03:07.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_14_4&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;tulip&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;3&lt;/bus&gt;
    &lt;slot&gt;7&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x0009'&gt;DECchip 21140 [FasterNet]&lt;/product&gt;
    &lt;vendor id='0x1011'&gt;Digital Equipment Corporation&lt;/vendor&gt;
    &lt;numa node='0'/&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
    <para>出力された値の中から、 <tag>domain</tag>, <tag>bus</tag>, <!-- NOTE: slot? --> <tag>slot</tag>, <tag>function</tag> の値 (上記太字部分) をメモしておきます。</para>
   </step>
   <step>
    <para>&vmguest; に対して割り当てを行う前に、 &vmhost; 側からの切り離しを行います:</para>
<screen>&prompt.sudo;<command>virsh nodedev-detach pci_0000_03_07_0</command>
  Device pci_0000_03_07_0 detached</screen>
    <tip>
     <title>多機能型 PCI デバイスについて</title>
     <para>FLR (Function Level Reset; 機能レベルリセット) や PM (Power Management; 電源管理) リセットに対応していない多機能型の PCI デバイスを使用している場合、 &vmguest; 側に割り当てるには、 &vmhost; 側で全ての機能を切り離す必要があります。また、セキュリティ上の理由から、デバイス全体をリセットする必要があります。 <systemitem>libvirt</systemitem> では、 &vmhost; 側もしくは他の &vmguest; 側で機能の一部が使用されている場合、その割り当てを拒否するようになっています。</para>
    </tip>
   </step>
   <step>
    <para>domain, bus, slot, function の各値を 16 進数に変換します。上記の例では、 domain = 0, bus = 3, slot = 7, function = 0 ですので、下記のようなコマンドを実行して変換を行います。なお、指定の順序を間違えないようにしてください:</para>
<screen>&prompt.user;<command>printf "&lt;address domain='0x%x' bus='0x%x' slot='0x%x' function='0x%x'/&gt;
" 0 3 7 0</command></screen>
    <para>上記を実行すると、下記のように出力されるはずです:</para>
<screen>&lt;address domain='0x0' bus='0x3' slot='0x7' function='0x0'/&gt;</screen>
   </step>
   <step>
    <para>あとは対象の &vmguest; の設定ファイルを <command>virsh edit</command> で編集して、上記の手順の出力結果を <literal>&lt;devices&gt;</literal> タグ内に貼り付けます:</para>
<screen>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
  &lt;source&gt;
    &lt;address domain='0x0' bus='0x03' slot='0x07' function='0x0'/&gt;
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
    <tip xml:id="tip-libvirt-config-pci-virsh-managed">
     <title><literal>managed</literal> と <literal>unmanaged</literal> の違いについて</title>
     <para><systemitem>libvirt</systemitem> では、 PCI デバイスの処理方法を 2 種類 (<literal>managed</literal> と <literal>unmanaged</literal>) 用意しています。 <literal>managed</literal> の場合、 <systemitem>libvirt</systemitem> は必要に応じて既存のドライバからデバイスを切り離す処理からデバイスのリセット、仮想マシンを起動する前の <systemitem>vfio-pci</systemitem> への接続など、全ての詳細処理を制御するようになります。また、仮想マシンが終了した場合や仮想マシンからデバイスを切り離した場合、 <systemitem>libvirt</systemitem> は <systemitem>vfio-pci</systemitem> への接続を解除して元のドライバに再接続する処理までを行うようになります。逆に <literal>unmanaged</literal> の場合、 <systemitem>libvirt</systemitem> はそれらの処理を行わず、仮想マシンにハードウエアを割り当てる際と、仮想マシンからハードウエアを切り離す際には、それらの処理をユーザ側で行わなければなりません。</para>
    </tip>
    <para>上記の例では <literal>managed='yes'</literal> を指定しているため、 <literal>managed</literal> を選択していることになります。 <literal>unmanaged</literal> に切り替えたい場合は、これを <literal>managed='no'</literal> に変更してください。なお、この場合は <command>virsh nodedev-detach</command> や <command>virsh nodedev-reattach</command> のコマンドを利用して、対応するドライバに対する処理を行う必要があります。具体的には、 &vmguest; を起動する前に <command>virsh nodedev-detach pci_0000_03_07_0</command> を実行してホスト側から切り離し、終了後には <command>virsh nodedev-reattach pci_0000_03_07_0</command> を実行して、ホスト側で認識できるように再設定する必要があります。</para>
   </step>
   <step>
    <para>ホスト側で &selnx; が動作している場合は、 &vmguest; をシャットダウンして &selnx; を無効化します。</para>
<screen>&prompt.sudo;<command>setsebool -P virt_use_sysfs 1</command></screen>
   </step>
   <step>
    <para>&vmguest; を起動すると、 &vmguest; から割り当てられた PCI デバイスにアクセスできるようになります:</para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
  </procedure>

  <important>
   <title>&slsa;11 SP4 の &kvm; ゲストについて</title>
   <para>新しい &qemu; マシンタイプ (pc-i440fx-2.0 もしくはそれ以降) を設定した &slsa;11 SP4 &kvm; ゲストの場合、ゲスト内では既定で <systemitem class="resource">acpiphp</systemitem> モジュールが読み込まれません。このモジュールはディスクやネットワークデバイスのホットプラグ (活性接続) を行うために読み込んでおかなければならないモジュールですので、必要であれば <command>modprobe acpiphp</command> コマンドを実行して読み込んでください。なお、 <filename>/etc/modprobe.conf.local</filename> ファイル内に <literal>install acpiphp /bin/true</literal> の行を追加すると、システムの起動時に自動読み込みを行うことができます。</para>
  </important>

  <important>
   <title>&qemu; Q35 マシンタイプを使用する &kvm; ゲストについて</title>
   <para>&qemu; Q35 マシンタイプを使用する &kvm; マシンの場合、 1 つの <literal>pcie-root</literal> コントローラと 7 つの <literal>pcie-root-port</literal> コントローラからなる PCI トポロジを構成します。 <literal>pcie-root</literal> コントローラはホットプラグ (活性接続) には対応しませんが、 <literal>pcie-root-port</literal> コントローラはそれぞれ 1 つの PCIe デバイスのホットプラグに対応します。 PCI コントローラ自身はホットプラグに対応していませんので、 7 つ以上の PCIe デバイスのホットプラグが必要となる場合、あらかじめ <literal>pcie-root-port</literal> コントローラを追加しておくようにしてください。また <literal>pcie-to-pci-bridge</literal> コントローラを追加することで、古い PCI デバイスのホットプラグを実現することもできます。 &qemu; のマシンタイプ別の PCI トポロジの詳細について、詳しくは <link xlink:href="https://libvirt.org/pci-hotplug.html"/> (英語) をお読みください。</para>
  </important>

  <sect2 xml:id="tip-libvirt-config-zpci">
   <title>&zseries; 向けの &pciback;</title>
   <para>&zseries; をサポートするため、 &qemu; では追加の属性を設定するための PCI 表記を拡張しています。具体的には <option>uid</option> と <option>fid</option> という 2 種類の属性が &libvirt; 仕様内の <literal>&lt;zpci/&gt;</literal> に追加されています。 <option>uid</option> はユーザ定義の識別値を、 <option>fid</option> の PCI 機能の識別値をそれぞれ設定します。これらの属性はいずれも任意指定で、何も指定しない場合は矛盾の無い値を自動的に生成します。</para>
   <para>ドメイン設定内に zPCI 属性を含めたい場合は、下記の例を参考にしてください:</para>
<screen>
&lt;controller type='pci' index='0' model='pci-root'/&gt;
&lt;controller type='pci' index='1' model='pci-bridge'&gt;
  &lt;model name='pci-bridge'/&gt;
  &lt;target chassisNr='1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0001' fid='0x00000000'/&gt;
  &lt;/address&gt;
&lt;/controller&gt;
&lt;interface type='bridge'&gt;
  &lt;source bridge='virbr0'/&gt;
  &lt;model type='virtio'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0007' fid='0x00000003'/&gt;
  &lt;/address&gt;
&lt;/interface&gt;
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-usb-virsh">
  <title>USB デバイスの追加</title>

  <para>USB デバイスを &vmguest; に割り当てるには、 &virsh; を使用して下記のように実施します:</para>

  <procedure>
   <step>
    <para>&vmguest; に接続されている USB デバイスを識別します:</para>
<screen>&prompt.sudo;<command>lsusb</command>
[...]
Bus 001 Device 003: ID <emphasis role="bold">0557:2221</emphasis> ATEN International Co., Ltd Winbond Hermon
[...]</screen>
    <para>出力された ID をメモしておきます。上記の例では、製造元 ID が <literal>0557</literal> 、製品 ID が <literal>2221</literal> となります。</para>
   </step>
   <step>
    <para>仮想マシンに対して <command>virsh edit</command> を実行し、下記の内容を <literal>&lt;devices&gt;</literal> タグ内に追加します。このとき、 vendor と product の箇所にそれぞれメモした値を指定します:</para>
<screen>&lt;hostdev mode='subsystem' type='usb'&gt;
  &lt;source startupPolicy='optional'&gt;
   <emphasis role="bold">&lt;vendor id='0557'/&gt;
   &lt;product id='2221'/&gt;</emphasis>
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
    <tip>
     <title>製造元 (vendor) と製品 (product) の指定ではなく、デバイスのアドレス (address) を設定する方法について</title>
     <para><tag class="emptytag">vendor</tag> および <tag class="emptytag">product</tag> の ID を指定する方法のほかにも、 <xref linkend="sec-libvirt-config-pci-virsh"/> で PCI デバイスを割り当てる際の説明と同様に、 <tag class="emptytag">address</tag> タグを利用して割り当てる方法もあります。</para>
    </tip>
   </step>
   <step>
    <para>ホスト内で &selnx; が動作している場合は、 &vmguest; をシャットダウンして &selnx; を無効化します。</para>
<screen>&prompt.sudo;<command>setsebool -P virt_use_sysfs 1</command></screen>
   </step>
   <step>
    <para>&vmguest; を起動すると、 &vmguest; から割り当てられた PCI デバイスにアクセスできるようになります:</para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-io">
  <title>SR-IOV デバイスの追加</title>

  <para>Single Root I/O Virtualization ( <xref linkend="vt-io-sriov"/> ) に対応した <xref linkend="gloss-vt-acronym-pcie"/> デバイスは、リソースを複製することができるため、複数のデバイスとして振る舞うことができます。複製されたリソースは <quote>擬似デバイス</quote> として、 &vmguest; への割り当てを行うことができます。</para>

  <para><xref linkend="vt-io-sriov"/> は Peripheral Component Interconnect Special Interest Group (PCI-SIG) が作成した工業仕様で、物理機能 (Physical Functions (PF)) と仮想機能 (Virtual Functions (VF)) を提供しています。 PF はデバイスを管理したり設定したりするための完全な <xref linkend="gloss-vt-acronym-pcie"/> 機能で、データの移動も行うことができます。それに対して VF 側には管理部分が提供されておらず、データの移動と設定機能の一部のみが提供されています。 VF は全ての <xref linkend="gloss-vt-acronym-pcie"/> 機能を持っているわけではないので、ホスト側のオペレーティングシステムもしくは <xref linkend="gloss-vt-hypervisor"/> が <xref linkend="vt-io-sriov"/> に対応し、 VF へのアクセスと初期化を行わなければなりません。論理上の VF の最大数は、 1 デバイスあたり 256 個まで (たとえば 2 ポートのイーサネットカードであれば 512 個) になります。実際には各 VF がリソースを消費してしまうことから、この最大値はもっとずっと小さくなります。</para>

  <sect2 xml:id="sec-libvirt-config-io-requirements">
   <title>要件</title>
   <para><xref linkend="vt-io-sriov"/> を使用するには、下記の要件を全て満たさなければなりません:</para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para><xref linkend="vt-io-sriov"/> に対応したネットワークカードを用意すること (&productname; ではネットワークカードのみに対応しています) 。</para>
    </listitem>
    <listitem>
     <para>AMD64/Intel 64 でハードウエア仮想化 (AMD-V もしくは Intel VT-x) に対応していること。 <phrase os="sles;sled">詳しくは <xref linkend="sec-kvm-requires-hardware"/> をお読みください。</phrase></para>
    </listitem>
    <listitem>
     <para>デバイスの割り当て (AMD-Vi もしくは Intel <xref linkend="gloss-vt-acronym-vtd"/>) に対応したチップセットであること。</para>
    </listitem>
    <listitem>
     <para>&libvirt; 0.9.10 もしくはそれ以降が存在すること。</para>
    </listitem>
    <listitem>
     <para>ホストシステム内で <xref linkend="vt-io-sriov"/> ドライバが読み込まれ、設定されていること。</para>
    </listitem>
    <listitem>
     <para>ホスト側の設定が <xref linkend="ann-vt-io-require"/> に示されている要件を満たしていること。</para>
    </listitem>
    <listitem>
     <para>&vmguest; に割り当てる予定の VF の PCI アドレスの一覧を用意していること。</para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>デバイスが SR-IOV に対応しているかどうかの確認方法</title>
    <para>デバイスが <xref linkend="vt-io-sriov"/> に対応しているかどうかは、 <command>lspci <!-- NOTE: "-v" is required? -->-v</command> を実行して表示される情報から判断することができます。 <xref linkend="vt-io-sriov"/> に対応するデバイスである場合、下記のような表示が現れるはずです:</para>
<screen>Capabilities: [160 v1] Single Root I/O Virtualization (<xref linkend="vt-io-sriov"/>)</screen>
   </tip>
   <note>
    <title>&vmguest; の作成時における SR-IOV デバイスの追加について</title>
    <para>初期設定時に &vmguest; に SR-IOV デバイスを追加する場合は、あらかじめ <xref linkend="sec-libvirt-config-io-config"/> で説明している手順で設定を済ませておく必要があります。</para>
   </note>
  </sect2>

  <sect2 xml:id="sec-libvirt-config-io-config">
   <title>SR-IOV ホストドライバの読み込みと設定</title>
   <para>VF にアクセスして準備を行うには、 SR-IOV 対応のドライバをホスト側のシステムに読み込んでおく必要があります。</para>
   <procedure>
    <step>
     <para>ドライバを読み込む前に、まずは <command>lspci</command> を実行して、カードが正しく検出されていることを確認します。下記の例では、 <command>lspci</command> がデュアルポートの Intel 82576NS ネットワークカードを検出しています:</para>
<screen>&prompt.sudo;<command>/sbin/lspci | grep 82576</command>
01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)</screen>
     <para>カードが検出されていない場合、 BIOS/EFI の設定でハードウエア仮想化の設定が有効化されていないことが考えられます。ハードウエア仮想化機能が有効化されているか同化を調べるには、ホスト側の BIOS 設定をご確認ください。</para>
    </step>
    <step>
     <para>次に <command>lsmod</command> を実行して、 <xref linkend="vt-io-sriov"/> ドライバが読み込まれているかどうかを確認します。下記の例では igb ドライバ (Intel 82576NS ネットワークカード向けのドライバです) が読み込まれているかどうかの確認になります。下記のように表示されれば、ドライバが既に読み込まれていることになります。何も出力を返さない場合は、ドライバが読み込まれていないことになります。</para>
<screen>&prompt.sudo;<command>/sbin/lsmod | egrep "^igb "</command>
igb                   185649  0</screen>
    </step>
    <step>
     <para>ドライバが既に読み込まれている場合は、この手順を飛ばしてください。 <xref linkend="vt-io-sriov"/> ドライバが読み込まれていない場合は、あらかじめ <xref linkend="vt-io-sriov"/> 非対応のドライバの読み込みを解除する必要があります。読み込みを解除するには、 <command>rmmod</command> コマンドをお使いください。下記の例では、 Intel 82576NS ネットワークカード向けの <xref linkend="vt-io-sriov"/> 非対応ドライバの読み込みを解除しています:</para>
<screen>&prompt.sudo;<command>/sbin/rmmod igbvf</command></screen>
    </step>
    <step>
     <para>あとは <command>modprobe</command> を利用して、 <xref linkend="vt-io-sriov"/> 対応のドライバを読み込みます。このとき、 VF パラメータ ( <literal>max_vfs</literal> ) を必ず指定してください:</para>
<screen>&prompt.sudo;<command>/sbin/modprobe igb max_vfs=8</command></screen>
    </step>
   </procedure>
   <remark>Unsure if the following procedure is really needed.</remark>
   <para>代わりの方法として、 SYSFS を介してドライバを読み込む方法もあります:</para>
   <procedure>
    <step>
     <para>イーサネットデバイスの一覧を表示して、物理 NIC の PCI ID を確認します:</para>
<screen>&prompt.sudo;<command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
    </step>
    <step>
     <para>VF を有効化するには、 <literal>sriov_numvfs</literal> パラメータに対して必要な VF 数を書き込みます:</para>
<screen>&prompt.sudo;<command>echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs</command></screen>
    </step>
    <step>
     <para>VF NIC が読み込まれたことを確認します:</para>
<screen>&prompt.sudo;<command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:08.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
    </step>
    <step>
     <para>設定可能な VF の最大数を知りたい場合は、下記のようなコマンドを入力して実行します:</para>
<screen>&prompt.sudo;<command>lspci -vvv -s 06:00.1 | grep 'Initial VFs'</command>
                       Initial VFs: 32, Total VFs: 32, Number of VFs: 0,
Function Dependency Link: 01</screen>
    </step>
    <step>
     <para><filename>/etc/systemd/system/before.service</filename> ファイルを作成して、システムの起動時に SYSFS 経由で VF を自動設定するように設定します:</para>
<screen>[Unit]
Before=
[Service]
Type=oneshot
RemainAfterExit=true
ExecStart=/bin/bash -c "echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs"
# 注意: 実行ファイルはシェル経由ではなく、直接実行されます。詳しい書式については、
# systemd.service と systemd.unit のマニュアルページをお読みください
[Install]
# このサービスを開始するターゲットの指定
WantedBy=multi-user.target
#WantedBy=graphical.target</screen>
    </step>
    <step>
     <para>また VM を起動する前に、もう 1 つのサービスファイル ( <filename>after-local.service</filename> ) を作成し、 NIC の切り離しを行うスクリプト <filename>/etc/init.d/after.local</filename> を実行するように設定します。これを行わないと、 VM の起動が失敗するようになってしまいます:</para>
<screen>[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target</screen>
    </step>
    <step>
     <para>上記の内容を <filename>/etc/systemd/system</filename> にコピーします。</para>
<screen>#! /bin/sh
# ...
virsh nodedev-detach pci_0000_06_08_0</screen>
     <para>上記の内容を <filename>/etc/init.d/after.local</filename> に保存します。</para>
    </step>
    <step>
     <para>マシンを再起動したあと、手順の冒頭で実行した <command>lspci</command> コマンドを実行しなおし、 SR-IOV ドライバが読み込まれていることを確認します。 SR-IOV ドライバが正しく読み込まれていれば、下記のように VF 向けの追加の行が現れているはずです:</para>
<screen>01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-config-io-attach">
   <title>&vmguest; に対する VF ネットワークデバイスの追加</title>
   <para><xref linkend="vt-io-sriov"/> 対応のハードウエアの設定を &vmhost; 内で正しく実施したあとは、 &vmguest; に対して VF を追加していきます。これを行うには、まずいくつかのデータを収集しておく必要があります。</para>
   <procedure>
    <title>既存の &vmguest; に対する VF ネットワークデバイスの追加</title>
    <para>下記に示す手順は構成例となります。お使いの環境に合わせて各種のデータを変更して実行してください。</para>
    <step>
     <para><command>virsh nodedev-list</command> コマンドを実行して、割り当てる VF と対応する PF の PCI アドレスを取得します。 <command>lspci</command> の出力は <xref linkend="sec-libvirt-config-io-config"/> のようになります (たとえば <literal>01:00.0</literal> や <literal>04:00.1</literal> など) 。このアドレス情報のコロン (:) やドット (.) をアンダースコア (_) に変換し、冒頭に "pci_0000_" を付けたものが virsh で使用するアドレスとなります。たとえば <command>lspci</command> コマンドで "04:00.0" と出力された場合、 virsh のアドレスは "pci_0000_04_00_0" になります。下記の例では、 Intel 82576NS デュアルポートイーサネットカードでの PCI ID を取得しています:</para>
<screen>&prompt.sudo;<command>virsh nodedev-list | grep 0000_04_</command>
<emphasis role="bold">pci_0000_04_00_0</emphasis>
<emphasis role="bold">pci_0000_04_00_1</emphasis>
pci_0000_04_10_0
pci_0000_04_10_1
pci_0000_04_10_2
pci_0000_04_10_3
pci_0000_04_10_4
pci_0000_04_10_5
pci_0000_04_10_6
pci_0000_04_10_7
pci_0000_04_11_0
pci_0000_04_11_1
pci_0000_04_11_2
pci_0000_04_11_3
pci_0000_04_11_4
pci_0000_04_11_5</screen>
     <para>最初の 2 つの項目が <emphasis role="bold">PF</emphasis> を、残りの項目が VF を表しています。</para>
    </step>
    <step>
     <para>あとは <command>virsh nodedev-dumpxml</command> を実行して、追加したい VF の PCI ID を取得します:</para>
<screen>&prompt.sudo;<command>virsh nodedev-dumpxml pci_0000_04_10_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_04_10_0&lt;/name&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;16&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x10ca'&gt;82576 Virtual Function&lt;/product&gt;
    &lt;vendor id='0x8086'&gt;Intel Corporation&lt;/vendor&gt;
    &lt;capability type='phys_function'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/capability&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
     <para>次の手順では、下記のデータが必要となります:</para>
     <itemizedlist>
      <listitem>
       <para><literal>&lt;domain&gt;0&lt;/domain&gt;</literal></para>
      </listitem>
      <listitem>
       <para><literal>&lt;bus&gt;4&lt;/bus&gt;</literal></para>
      </listitem>
      <listitem>
       <para><literal>&lt;slot&gt;16&lt;/slot&gt;</literal></para>
      </listitem>
      <listitem>
       <para><literal>&lt;function&gt;0&lt;/function&gt;</literal></para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>あとは一時的な XML ファイル (たとえば <filename>/tmp/vf-interface.xml</filename>) を作成して、既存の &vmguest; に VF ネットワークデバイスを追加するのに必要なデータを記述していきます。このファイルを最も短く作成すると、下記のようになります:</para>
<screen>&lt;interface type='hostdev'&gt;<co xml:id="sriov-iface"/>
 &lt;source&gt;
  &lt;address type='pci' domain='0' bus='11' slot='16' function='0'2/&gt;<co xml:id="sriov-data"/>
 &lt;/source&gt;
&lt;/interface&gt;</screen>
     <calloutlist>
      <callout arearefs="sriov-iface">
       <para>VF は固定の MAC アドレスを取得しません。ホストが再起動されるたびに MAC アドレスが変更される形になります。この場合、 <tag class="attribute">hostdev</tag> で <quote>従来の方法を利用して</quote> ネットワークデバイスを追加すると、ホスト側の再起動が発生するたびに MAC アドレスが変わってしまうため、 &vmguest; 側のネットワークデバイスを再設定する必要が生じてしまいます。このような問題を回避するため、 &libvirt; では <tag class="attvalue">hostdev</tag> という値が提供されるようになり、これによってデバイスの割り当て <emphasis>前</emphasis> にネットワーク固有のデータを設定できるようになっています。</para>
      </callout>
      <callout arearefs="sriov-data">
       <para>以前の手順で取得したデータを指定します。</para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>デバイスが既にホスト側に割り当てられてしまっている場合、 &vmguest; に対して割り当てることができなくなります。ゲストに対して割り当てたい場合は、下記のようにしてホスト側から切り離しを行ってください:</para>
<screen>&prompt.sudo;<command>virsh nodedev-detach pci_0000_04_10_0</command></screen>
    </step>
    <step>
     <para>あとは既存の &vmguest; に VF インターフェイスを追加します:</para>
<screen>&prompt.sudo;<command>virsh attach-device <replaceable>仮想マシン名</replaceable> /tmp/vf-interface.xml --<replaceable>オプション</replaceable></command></screen>
     <para><replaceable>仮想マシン名</replaceable> の箇所には &vmguest; の名前のほか、 ID や UUID を指定することもできます。また、 --<replaceable>オプション</replaceable> の部分には下記を指定することができます:</para>
     <variablelist>
      <varlistentry>
       <term><option>--persistent</option></term>
       <listitem>
        <para>仮想マシンの XML 設定ファイルに対してデバイスを追加します。これに加えて、仮想マシンが動作中である場合、ホットプラグで接続することもできます。</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--config</option></term>
       <listitem>
        <para>仮想マシンの XML 設定ファイルに対してデバイスを追加します。仮想マシンが動作中の場合、ホットプラグは行われず、次回の再起動以降に現れるようになります。</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--live</option></term>
       <listitem>
        <para>仮想マシンの動作中の状態にのみ適用します。仮想マシンが動作中ではない場合、この操作は失敗します。 XML ファイル内には保存されませんので、 &vmguest; の再起動を行うとデバイスが消えてしまいます。</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--current</option></term>
       <listitem>
        <para>仮想マシンの現在の状態に反映させます。仮想マシンが動作中ではない場合、デバイスを XML 設定ファイル内に追加し、次回の起動時に現れるようになります。仮想マシンが動作中である場合、デバイスはホットプラグで追加されますが、 XML 設定ファイルには追加されないようになります。</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>VF インターフェイスを切り離すには、 <command>virsh detach-device</command> コマンドを使用します。オプション類は上記と同じです。</para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="libvirt-config-io-pool">
   <title>プールからの動的な VF の割り当て</title>
   <para><xref linkend="sec-libvirt-config-io-attach"/> の手順に従って &vmguest; の設定内に VF の PCI アドレスを指定してしまうと、他のホストへの移行が難しくなってしまいます。移行先のホストで移行元と同じ PCI バスに同じハードウエアが搭載されていれば問題はありませんが、そうでない場合は &vmguest; の設定を変更しなければならなくなってしまいます。</para>
   <para>このような場合は、 <xref linkend="vt-io-sriov"/> の全ての VF を含むデバイスプールを設定し、 &libvirt; 側から使用できるようにする方法があります。 &vmguest; では起動時にこのプールを参照し、空いているいずれかのデバイスを動的に使用することができます。 &vmguest; を停止すると VF はプール内に戻されますので、他のゲストから使用できるようになります。</para>
   <sect3 xml:id="libvirt-config-io-pool-host">
    <title>&vmhost; 内の VF プールを利用したネットワークの定義</title>
    <para>下記の例では、ホスト側で <literal>eth0</literal> のネットワークインターフェイスが割り当てられている PF に対して、それに結びつく全ての <xref linkend="vt-io-sriov"/> デバイスである VF のプールを作成しています:</para>
<screen>&lt;network&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
    &lt;forward mode='hostdev' managed='yes'&gt;
      &lt;pf dev='eth0'/&gt;
    &lt;/forward&gt;
  &lt;/network&gt;</screen>
    <para>このネットワークインターフェイスをホスト側で使用する場合は、上記のコードをファイルに保存 (例: <filename>/tmp/passthrough.xml</filename>) したあと、下記のコマンドを実行してください。なお、上記の <literal>eth0</literal> の箇所はお使いの環境内の <xref linkend="vt-io-sriov"/> デバイスの PF に置き換えてください:</para>
<screen>&prompt.sudo;<command>virsh net-define /tmp/passthrough.xml</command>
&prompt.sudo;<command>virsh net-autostart passthrough</command>
&prompt.sudo;<command>virsh net-start passthrough</command></screen>
   </sect3>
   <sect3 xml:id="libvirt-config-io-pool-guest">
    <title>プールから VF を使用するための &vmguest; 側の設定</title>
    <para>下記の &vmguest; のデバイスインターフェイス定義は、 <xref linkend="libvirt-config-io-pool-host"/> で作成したプールから <xref linkend="vt-io-sriov"/> デバイスの VF を使用する設定です。 &libvirt; では、最初のゲストの起動時に、 PF に結びつけられた VF の一覧を自動的に取得します。</para>
<screen>&lt;interface type='network'&gt;
  &lt;source network='passthrough'&gt;
&lt;/interface&gt;</screen>
    <para>VF のプールからネットワークインターフェイスを使用するように設定した &vmguest; を起動したあとは、 VF が正しく使用されていることを確認します。これを行うには、ホスト側で <command>virsh net-dumpxml passthrough</command> を実行します:</para>
<screen>&lt;network connections='1'&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
  &lt;uuid&gt;a6a26429-d483-d4ed-3465-4436ac786437&lt;/uuid&gt;
  &lt;forward mode='hostdev' managed='yes'&gt;
    &lt;pf dev='eth0'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x5'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x7'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x5'/&gt;
  &lt;/forward&gt;
  &lt;/network&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="libvirt-config-listing-host-devs">
  <title>接続されているデバイスの一覧表示</title>

  <para>&virsh; には &vmguest; に接続されている全ての &vmhost; のデバイスを一覧表示する機能はありませんが、指定した &vmguest; に接続されているデバイスを一覧表示することはできます。具体的には下記のようなコマンドを入力して実行します:</para>

<screen>virsh dumpxml <replaceable>VM_ゲスト名</replaceable> | xpath -e /domain/devices/hostdev</screen>

  <para>たとえば下記のようになります:</para>

<screen>
&prompt.sudo;virsh dumpxml sles12 | -e xpath /domain/devices/hostdev
Found 2 nodes:
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes"&gt;
  &lt;driver name="xen" /&gt;
  &lt;source&gt;
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x1" /&gt;
  &lt;/source&gt;
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0a" function="0x0" /&gt;
  &lt;/hostdev&gt;
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes"&gt;
  &lt;driver name="xen" /&gt;
  &lt;source&gt;
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x2" /&gt;
  &lt;/source&gt;
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0b" function="0x0" /&gt;
&lt;/hostdev&gt;
</screen>

  <tip xml:id="libvirt-config-listing-host-devs-sriov">
   <title><literal>&lt;interface type='hostdev'&gt;</literal> で接続されている SR-IOV デバイスの一覧表示について</title>
   <para><literal>&lt;interface type='hostdev'&gt;</literal> を利用して &vmhost; のデバイスに接続している SR-IOV デバイスの場合は、上記とは異なる XPath クエリを指定する必要があります:</para>
<screen>virsh dumpxml <replaceable>VM_ゲスト名</replaceable> | xpath -e /domain/devices/interface/@type</screen>
  </tip>
 </sect1>
 <sect1 xml:id="libvirt-config-storage-virsh">
  <title>ストレージデバイスの設定</title>

  <para>ストレージデバイスは <tag>disk</tag> 内に記述します。通常、 <tag>disk</tag> タグには複数の属性を指定します。そのうち、下記に示す 2 つの属性が最も重要です:</para>

  <itemizedlist>
   <listitem>
    <para><tag class="attribute">type</tag> 属性: 仮想ディスクデバイスのソースを指定するための属性です。 <tag class="attvalue">file</tag> , <tag class="attvalue">block</tag> , <tag class="attvalue">dir</tag> , <tag class="attvalue">network</tag> , <tag class="attvalue">volume</tag> のいずれかを指定します。</para>
   </listitem>
   <listitem>
    <para><tag class="attribute">device</tag> 属性: ディスクを &vmguest; 側の OS に示す際の方法を指定する属性です。 <tag class="attvalue">floppy</tag> (フロッピィディスク), <tag class="attvalue">disk</tag> (ハードディスク), <tag class="attvalue">cdrom</tag> (CD-ROM) などがあります。</para>
   </listitem>
  </itemizedlist>

  <para>子要素として最も重要なものは下記のとおりです:</para>

  <itemizedlist>
   <listitem>
    <para><tag>driver</tag> にはドライバとバスに関する情報を指定します。ここには &vmguest; 側での処理方法などを指定します。</para>
   </listitem>
   <listitem>
    <para><tag>target</tag> には &vmguest; 内でのデバイス名の設定が含まれます。ここには任意指定の bus 属性 (接続されているストレージバスに関する情報) を含むことができます。</para>
   </listitem>
  </itemizedlist>

  <para>&vmguest; にストレージデバイスを追加するには、下記の手順を実施します:</para>

  <procedure>
   <step>
    <para>既存の &vmguest; に対する設定を編集します:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
   </step>
   <step>
    <para><tag>disk</tag> 要素内に <tag>disk</tag> 要素を追加し、その中に <tag class="attvalue">type</tag> と <tag class="attvalue">device</tag> の属性を追加します。</para>
<screen>&lt;disk type='file' device='disk'&gt;</screen>
   </step>
   <step>
    <para><tag>driver</tag> タグでは下記のような既定値を指定します:</para>
<screen>&lt;driver name='qemu' type='qcow2'/&gt;</screen>
   </step>
   <step>
    <para>新しい仮想ディスクデバイスのソースとなるディスクイメージを作成します:</para>
<screen>&prompt.sudo;<command>qemu-img create -f qcow2 /var/lib/libvirt/images/sles15.qcow2 32G</command></screen>
   </step>
   <step>
    <para>あとは <tag>source</tag> タグでソースを指定します:</para>
<screen>&lt;source file='/var/lib/libvirt/images/sles15.qcow2'/&gt;</screen>
   </step>
   <step>
    <para>&vmguest; 内でのデバイス名を表す <tag>target</tag> タグを追加します。このとき、接続先を表す <tag>bus</tag> 属性も設定しておきます:</para>
<screen>&lt;target dev='vda' bus='virtio'/&gt;</screen>
   </step>
   <step>
    <para>あとは仮想マシンを再起動します:</para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
  </procedure>

  <para>これで、 &vmguest; の OS 内から新しいストレージデバイスにアクセスできるようになります。</para>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-controllers-virsh">
  <title>コントローラデバイスの設定</title>

  <para><command>libvirt</command> では一般に、 &vmguest; が使用する仮想デバイスの種類に応じて、自動的にコントローラを管理することができます。たとえば &vmguest; 内に PCI デバイスや SCSI デバイスが存在する場合、 PCI や SCSI のコントローラは自動的に作成され管理されます。このほか、 <command>libvirt</command> ではハイパーバイザ固有のコントローラ、たとえば KVM の &vmguest; であれば <literal>virtio-serial</literal> 、 Xen の &vmguest; であれば <literal>xenbus</literal> を作成することができます。通常は既定のコントローラとそれに付随する設定で問題なく動作しますが、必要に応じてコントローラやその設定を調整することもできます。たとえば virtio-serial でより多くのポートが必要となっている場合や、 xenbus コントローラに対してより多くのメモリを割り当てたり、割り込みを多く割り当てたりするような場合がそれに該当します。</para>

  <para>xenbus コントローラは、全ての Xen 準仮想化デバイスに対するコントローラとして動作する点でユニークなものであると言えます。また、 &vmguest; に数多くのディスクやネットワークデバイスが接続されるような場合には、コントローラに対してもメモリを多く割り当てる必要があるかもしれません。 Xen の <literal>max_grant_frames</literal> 属性は許可するフレームの数、もしくは共有メモリのブロック数を指定しますが、これはそれぞれの &vmguest; に対する <literal>xenbus</literal> コントローラに対して割り当てられます。</para>

  <para>既定値は 32 で、ほとんどの環境において十分な値ではありますが、 I/O デバイスの多い &vmguest; や I/O 負荷の高いシステムでは、フレームの枯渇によって性能が落ちることがあります。このような場合、 <command>xen-diag</command> コマンドを利用して dom0 およびお使いの &vmguest; の <literal>max_grant_frames</literal> の現在値および最大値を確認してください。なお、ゲストは動作中でなければなりません:</para>

<screen>&prompt.sudo;virsh list
 Id   Name             State
--------------------------------
 0    Domain-0         running
 3    sle15sp1         running

 &prompt.sudo;xen-diag gnttab_query_size 0
domid=0: nr_frames=1, max_nr_frames=256

&prompt.sudo;xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=32
</screen>

  <para>上記では、 <literal>sle15sp1</literal> のゲストは 32 フレームのうちの 3 フレームしか使用していません。性能面で何らかの問題がある場合や、フレーム数が不足している旨のログが記録されているような場合は、 &virsh; でフレーム数を増やしてください。具体的には、ゲスト側の設定ファイル内にある <literal>&lt;controller type='xenbus'</literal> という行を探して、 <literal>maxGrantFrames</literal> という制御要素を追加します:</para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='40'/&gt;
</screen>

  <para>設定を保存してゲストを再起動してください。これで設定が反映されます:</para>

<screen>&prompt.sudo;xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=40
</screen>

  <para>maxGrantFrames と同様に、 xenbus コントローラには <option>maxEventChannels</option> という設定値も用意されています。それぞれのチャンネルは準仮想化された割り込みのような動作をするもので、許可するフレーム数と同時に、準仮想化ドライバ向けのデータ転送構造を構築するためのものです。仮想 CPU 数の多い &vmguest; や多数の準仮想化デバイスが接続されている &vmguest; の場合は、既定値である 1023 よりも大きい値を設定する必要があるかもしれません。 maxEventChannels は maxGrantFrames と同じように変更することができます:</para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='128' maxEventChannels='2047'/&gt;
</screen>

  <para>詳しくは <link xlink:href="https://libvirt.org/formatdomain.html#elementsControllers"/> (英語) にある libvirt <citetitle>Domain XML format</citetitle> マニュアルの中にある、 <citetitle>Controllers</citetitle> 章をお読みください。</para>
 </sect1>
 <sect1 xml:id="libvirt-video-virsh">
  <title>ビデオデバイスの設定</title>

  <para>仮想マシンマネージャを利用した場合、ビデオデバイスのモデルのみを設定することができます。 VRAM の割り当てや 2D/3D のアクセラレーションの設定については、 XML の設定を編集することでしか設定することができません。</para>

  <sect2 xml:id="libvirt-video-vram-virsh">
   <title>VRAM の割当量の変更</title>
   <procedure>
    <step>
     <para>既存の &vmguest; に対する設定を編集します:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>VRAM に割り当てるサイズを変更するには、下記のように設定します:</para>
<screen>&lt;video&gt;
&lt;model type='vga' vram='65535' heads='1'&gt;
...
&lt;/model&gt;
&lt;/video&gt;</screen>
    </step>
    <step>
     <para>あとは仮想マシンマネージャを開いて、仮想マシン側に割り当てられた VRAM のサイズを確認してください。</para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="libvirt-video-accel-virsh">
   <title>2D/3D アクセラレーションの設定変更</title>
   <procedure>
    <step>
     <para>既存の &vmguest; に対する設定を編集します:</para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>2D/3D アクセラレーション機能を有効化／無効化するには、それぞれ <literal>accel3d</literal> もしくは <literal>accel2d</literal> の設定を変更します:</para>
<screen>
&lt;video&gt;
 &lt;model&gt;
  &lt;acceleration accel3d='yes' accel2d='no'&gt;
 &lt;/model&gt;
&lt;/video&gt;</screen>
    </step>
   </procedure>
   <tip>
    <title>2D/3D アクセラレーションの有効化について</title>
    <para>2D/3D アクセラレーション機能を使用するには、 <literal>virtio</literal> もしくは <literal>vbox</literal> ビデオデバイスを使用する必要があります。それ以外のビデオデバイスである場合は、機能を有効化することができません。</para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="virsh-network-devices">
  <title>ネットワークデバイスの設定</title>

  <para>本章では、 &virsh; を利用して仮想ネットワークデバイスを設定するための方法について説明しています。</para>

  <para>&libvirt; のネットワークインターフェイス仕様について、詳しくは <link xlink:href="https://libvirt.org/formatdomain.html#elementsDriverBackendOptions"/> をお読みください。</para>

  <sect2 xml:id="virsh-multiqueue">
   <title>マルチキュー型の virtio-net によるネットワーク性能の強化</title>
   <para>マルチキュー型の virtio-net 機能を使用することで、 &vmguest; の仮想 CPU を複数個同時に使用することができるようになります。これにより、ネットワークの性能を改善することができるようになります。一般的な情報については <xref linkend="kvm-qemu-multiqueue"/> をお読みください。</para>
   <para>特定の &vmguest; に対して virtio-net のマルチキュー設定を行うには、 <xref linkend="sec-libvirt-config-editing-virsh"/> で示している手順で XML ファイルを編集します。具体的には、下記の箇所を修正します:</para>
<screen>
&lt;interface type='network'&gt;
 [...]
 &lt;model type='virtio'/&gt;
 &lt;driver name='vhost' queues='<replaceable>キュー数</replaceable>'/&gt;
&lt;/interface&gt;
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-direct">
  <title>&vmhost; のネットワークインターフェイスを共有するための macvtap の使用</title>

  <para>macvtap は &vmguest; の仮想インターフェイスをホストのネットワークインターフェイスに直接結びつけるための方法です。 macvtap ベースのインターフェイスは &vmhost; のネットワークインターフェイスを拡張するための仕組みで、同じイーサネットセグメント内で別の MAC アドレスを使用します。通常は &vmguest; と &vmhost; をそれぞれ同じスイッチに接続する形になります。</para>

  <note>
   <title>Linux ブリッジでは macvtap を使用することができない問題について</title>
   <para>macvtap は対象のインターフェイスが Linux ブリッジに接続されている場合、使用することができません。 macvtap インターフェイスを作成する前に、ブリッジからインターフェイスを削除しておく必要があります。</para>
  </note>

  <note>
   <title>macvtap を利用した &vmguest; と &vmhost; の通信</title>
   <para>macvtap を使用すると、 &vmguest; 同士のほか、ネットワーク内の他のホストとも通信を行うことができるようになります。ただし、 &vmguest; が動作している &vmhost; との間は、通信を行うことができません。これは macvtap の意図的な動作によるもので、 &vmhost; の物理イーサネットが macvtap ブリッジに割り当てられているためです。 &vmguest; からブリッジへのトラフィックはそのまま物理インターフェイスに送信され、 &vmhost; の IP スタックに戻ることができないようになっています。これと同様に、 &vmhost; の IP スタックからのトラフィックも物理インターフェイスにそのまま送信され、 &vmguest; の macvtap に戻ることができなくなっています。</para>
  </note>

  <para>macvtap ベースの仮想ネットワークインターフェイスは libvirt でサポートされていて、インターフェイスの種類 (type) を <literal>direct</literal> にすることによって実現することができます。たとえば下記のようになります:</para>

<screen>&lt;interface type='direct'&gt;
   &lt;mac address='aa:bb:cc:dd:ee:ff'/&gt;
   &lt;source dev='eth0' mode='bridge'/&gt;
   &lt;model type='virtio'/&gt;
   &lt;/interface&gt;</screen>

  <para>macvtap の操作モードは、 <literal>mode</literal> 属性を設定することで制御することができます。下記の一覧には、設定可能な値とそれらの説明を示しています:</para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para><literal>vepa</literal> : 全ての &vmguest; パケットを外部ブリッジに送信します。同じ &vmhost; 内の &vmguest; を宛先とするパケットは、 VEPA 対応ブリッジによって &vmhost; 側に戻される動作になります (現在のブリッジは一般に VEPA 対応ではありません) 。</para>
   </listitem>
   <listitem>
    <para><literal>bridge</literal> : 同じ &vmhost; を宛先とするパケットは、ターゲットの macvtap デバイスに直接配信されるようになります。送信元と送信先のデバイスは、直接配送に対応する <literal>bridge</literal> モードである必要があります。いずれかのモードが <literal>vepa</literal> である場合は、 VEPA 対応ブリッジが必要になります。</para>
   </listitem>
   <listitem>
    <para><literal>private</literal> : 全てのパケットを外部ブリッジに送信し、外部のルータやゲートウエイから送信されたパケットは同じ &vmhost; 内のターゲットの &vmguest; にのみ配信されるようになります。この処理は送信元と送信先のいずれかがプライベートモードである場合の動作になります。</para>
   </listitem>
   <listitem>
    <para><literal>passthrough</literal> : ネットワークインターフェイスに対してさらなる力を与えるための特殊なモードです。全てのパケットはインターフェイスに転送され、 virtio &vmguest; では MAC アドレスの変更やプロミスキャスモードに対応することで、インターフェイスのブリッジや VLAN インターフェイスの作成に対応するようになります。ただし、 <literal>passthrough</literal> モードではネットワークインターフェイスを共有することはできません。 &vmguest; にインターフェイスを割り当てると、 &vmhost; 側からは切り離されます。このような理由から、 SR-IOV 仮想機能は &vmguest; の <literal>passthrough</literal> モードと比較されます。</para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-disable-virtio-mellon">
  <title>メモリバルーンデバイスの無効化</title>

  <para>メモリバルーン機能は KVM では既定のオプションになっています。 &vmguest; 側には明示的にデバイスが割り当てられるようになっていますので、 &vmguest; の XML 設定ファイルには、メモリバルーンのタグを追加する必要はありません。何らかの理由で &vmguest; のメモリバルーン機能を無効化したい場合は、下記のようにして <literal>model='none'</literal> を指定する必要があります:</para>

<screen>&lt;devices&gt;
   &lt;memballoon model='none'/&gt;
&lt;/device&gt;</screen>
 </sect1>
 <sect1 xml:id="virsh-video-dual-head">
  <title>マルチモニタ (デュアルヘッド) の設定</title>

  <para>&libvirt; では、 &vmguest; に対して複数のモニタを接続してそれぞれにビデオ出力を行うことのできる、デュアルヘッド設定に対応しています。</para>

  <important>
   <title>&xen; がサポート対象外である点について</title>
   <para>&xen; ハイパーバイザは、デュアルヘッド設定に対応していません。</para>
  </important>

  <procedure>
   <title>デュアルヘッドの設定</title>
   <step>
    <para>仮想マシンを動作させた状態で、 &vmguest; 内に <package>xf86-video-qxl</package> パッケージがインストールされていることを確認します:</para>
<screen>&prompt.user;rpm -q xf86-video-qxl</screen>
   </step>
   <step>
    <para>&vmguest; をシャットダウンして、 <xref linkend="sec-libvirt-config-editing-virsh"/> で示している手順で XML ファイルを変更します。</para>
   </step>
   <step>
    <para>このとき、仮想グラフィックカードの型式 (モデル) を 'qxl' にしてください:</para>
<screen>
&lt;video&gt;
 &lt;model type='qxl' ... /&gt;
</screen>
   </step>
   <step>
    <para>グラフィックカードの設定内にある <option>heads</option> パラメータを増やして、デュアルヘッドの設定を行います。たとえば既定値の <literal>1</literal> から <literal>2</literal> に増やします:</para>
<screen>
&lt;video&gt;
 &lt;model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='2' primary='yes'/&gt;
 &lt;alias name='video0'/&gt;
 &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/&gt;
&lt;/video&gt;
</screen>
   </step>
   <step>
    <para>VNC ではなく、 Spice ディスプレイを使用するように仮想マシンを設定します:</para>
<screen>
&lt;graphics type='spice' port='5916' autoport='yes' listen='0.0.0.0'&gt;
 &lt;listen type='address' address='0.0.0.0'/&gt;
&lt;/graphics&gt;
</screen>
   </step>
   <step>
    <para>仮想マシンを起動して、 <command>virt-viewer</command> 経由でディスプレイに接続します。たとえば下記のように入力して実行します:</para>
<screen>&prompt.user;virt-viewer --connect qemu+ssh://<replaceable>ユーザ名@ホスト名</replaceable>/system</screen>
   </step>
   <step>
    <para>VM の一覧の中から設定を変更した仮想マシンを選択して、 <guimenu>接続</guimenu> を押します。</para>
   </step>
   <step>
    <para>&vmguest; 内でグラフィカルサブシステム (Xorg) が読み込まれたら、 <menuchoice><guimenu>表示</guimenu><guimenu>ディスプレイ</guimenu><guimenu>ディスプレイ 2</guimenu></menuchoice> を選択すると、 2 つめのモニタの出力を表示することができるようになります。</para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="virsh-kvm-zseries-crypto">
  <title>&zseries; における &kvm; ゲストへの暗号化アダプタのパススルー</title>

  <sect2 xml:id="virsh-kvm-zseries-crypto-intro">
   <title>概要</title>
   <para>&zseries; には、乱数生成やデジタル署名の検証、暗号化などの便利な機能を提供する暗号化ハードウエアが含まれています。 &kvm; では、これらの暗号化ハードウエアをゲストに占有させることで、パススルーデバイスとして使用することができます。このパススルー機能を設定した場合、ゲストとデバイスとの間の通信をハイパーバイザから監視することはできなくなります。</para>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-cover">
   <title>カバーされる範囲</title>
   <para>ここでは、 &zseries; ハードウエアで &kvm; ゲストに対する暗号化アダプタの占有方法を説明しています。手順は下記のような流れになっています:</para>
   <itemizedlist>
    <listitem>
     <para>まずはホスト側で暗号化アダプタとドメインを既定のドライバからマスクします。</para>
    </listitem>
    <listitem>
     <para><literal>vfio-ap</literal> ドライバを読み込みます。</para>
    </listitem>
    <listitem>
     <para><literal>vfio-ap</literal> ドライバに対して、暗号化アダプタとドメインを割り当てます。</para>
    </listitem>
    <listitem>
     <para>あとは暗号化アダプタを使用するようゲスト側を設定します。</para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-reqs">
   <title>要件</title>
   <itemizedlist>
    <listitem>
     <para>&qemu; / &libvirt; の仮想化環境を正しくインストールしておき、問題なく動作できるようにしておく必要があります。</para>
    </listitem>
    <listitem>
     <para>ホスト側のオペレーティングシステムで、 <literal>vfio_ap</literal> と <literal>vfio_mdev</literal> の各モジュールを用意しておきます。</para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-proc">
   <title>&kvm; ホスト側での暗号化ドライバの占有設定</title>
   <procedure>
    <step>
     <para>まずはホスト側で <literal>vfio_ap</literal> と <literal>vfio_mdev</literal> のカーネルモジュールが読み込まれていることを確認します:</para>
<screen>&prompt.user;lsmod | grep vfio_</screen>
     <para>どちらか 1 つでも読み込まれていない場合は、下記のようにして手作業で読み込みを行います:</para>
<screen>&prompt.sudo;modprobe vfio_mdev</screen>
    </step>
    <step>
     <para>ホスト内で新しい MDEV デバイスを作成し、それが追加されていることを確認します:</para>
<screen>
uuid=$(uuidgen)
$ echo ${uuid} | sudo tee /sys/devices/vfio_ap/matrix/mdev_supported_types/vfio_ap-passthrough/create
dmesg | tail
[...]
[272197.818811] iommu: Adding device 24f952b3-03d1-4df2-9967-0d5f7d63d5f2 to group 0
[272197.818815] vfio_mdev 24f952b3-03d1-4df2-9967-0d5f7d63d5f2: MDEV: group_id = 0
</screen>
    </step>
    <step>
     <para>次に &kvm; ゲストに占有させるホスト側の論理パーティション内のデバイスを識別します:</para>
<screen>&prompt.user;ls -l /sys/bus/ap/devices/
[...]
lrwxrwxrwx 1 root root 0 Nov 23 03:29 00.0016 -&gt; ../../../devices/ap/card00/00.0016/
lrwxrwxrwx 1 root root 0 Nov 23 03:29 card00 -&gt; ../../../devices/ap/card00/
</screen>
     <para>上記の例では、カードとキューがそれぞれ <literal>0</literal> と <literal>16</literal> になっていることがわかります。 Hardware Management Console (HMC) の設定とあわせるため、 16 進数表記 <literal>16</literal> を 10 進数表記 <literal>22</literal> に変換してください。</para>
    </step>
    <step>
     <para>対称のアダプタを <literal>zcrypt</literal> から使用されないようにマスクします:</para>
<screen>
&prompt.user;lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 5
00.0016 CEX5C CCA-Coproc online 5
</screen>
     <para>アダプタをマスクします:</para>
<screen>
&prompt.user;cat /sys/bus/ap/apmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/apmask
0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
     <para>ドメインをマスクします:</para>
<screen>
&prompt.user;cat /sys/bus/ap/aqmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/aqmask
0xfffffdffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
    </step>
    <step>
     <para>アダプタ 0, ドメイン 16 (10 進数で 22) を <literal>vfio-ap</literal> に割り当てます:</para>
<screen>
&prompt.sudo;echo +0x0 &gt; /sys/devices/vfio_ap/matrix/${uuid}/assign_adapter
&prompt.user;echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_domain
&prompt.user;echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_control_domain
</screen>
    </step>
    <step>
     <para>設定した matrix を確認します:</para>
<screen>
&prompt.user;cat /sys/devices/vfio_ap/matrix/${uuid}/matrix
00.0016
</screen>
    </step>
    <step>
     <para>あとは新しい VM を作成 (詳しくは <xref linkend="cha-kvm-inst"/> をお読みください) して準備が完了するまで待つか、既存の VM に設定を行います。いずれの場合にしても、 VM はシャットダウンしておく必要があります。</para>
    </step>
    <step>
     <para>MDEV デバイスを使用するよう設定を変更します:</para>
<screen>
&prompt.sudo;virsh edit <replaceable>VM_NAME</replaceable>
[...]
&lt;hostdev mode='subsystem' type='mdev' model='vfio-ap'&gt;
 &lt;source&gt;
  &lt;address uuid='24f952b3-03d1-4df2-9967-0d5f7d63d5f2'/&gt;
 &lt;/source&gt;
&lt;/hostdev&gt;
[...]
</screen>
    </step>
    <step>
     <para>あとは仮想マシンを再起動します:</para>
<screen>&prompt.sudo;virsh reboot <replaceable>VM_名</replaceable></screen>
    </step>
    <step>
     <para>ゲストにログインして、アダプタが存在していることを確認します:</para>
<screen>
&prompt.user;lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 1
00.0016 CEX5C CCA-Coproc online 1
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-moreinfo">
   <title>さらに詳しい情報</title>
   <itemizedlist>
    <listitem>
     <para>仮想化コンポーネントのインストールについては <xref linkend="cha-vt-installation"/> をお読みください。</para>
    </listitem>
    <listitem>
     <para><literal>vfio_ap</literal> の構造については <link xlink:href="https://www.kernel.org/doc/Documentation/s390/vfio-ap.txt"/> (英語) をお読みください。</para>
    </listitem>
    <listitem>
     <para>概要および詳細な手順については、 <link xlink:href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1787405"/> (英語) をお読みください。</para>
    </listitem>
    <listitem>
     <para>VFIO 仲介型デバイス (VFIO Mediated devices (MDEVs)) の構造については <link xlink:href="https://www.kernel.org/doc/html/latest/driver-api/vfio-mediated-device.html"/> をお読みください。</para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
</chapter>
