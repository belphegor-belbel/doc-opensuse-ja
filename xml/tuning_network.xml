<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "generic-entities.ent">
%entities;
]>
<chapter xmlns:its="http://www.w3.org/2005/11/its" xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-tuning-network">
<!-- ibm 33:
      offload depends on the adapter's features.
      ibm 34:
     Bonding: Documentation/networking/bonding.txt.
     link aggregation and load balancing
     Check, whether this is already describe somewhere else!
     Obviously in ha and xen guides...
 -->
<!-- tuning NFS performance:
      http://blogs.techrepublic.com.com/opensource/?p=64&tag=rbxccnbtr1
 -->
<!-- apache is similar -->
 <title>ネットワークのチューニング</title>
 <info>
    <meta name="description" its:translate="yes">Learn to tune Linux networking by configuring kernel socket buffers, TCP autotuning, Netfilter and RPS settings</meta>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
        </dm:bugtracker>
	<dm:translation>yes</dm:translation>
      </dm:docmanager>
    <revhistory xml:id="rh-cha-tuning-network">
   <revision>
     <date>2024-06-25</date>
     <revdescription>
       <para/>
     </revdescription>
   </revision>
 </revhistory>
</info>
    <para>ネットワークサブシステムは複雑な構造であるため、チューニングはシステムの用途に大きく依存するほか、ソフトウエアクライアントやハードウエアコンポーネント (スイッチ、ルータ、ゲートウエイ) などの外部要素にも依存します。 Linux カーネルでは、オーバーヘッドを低くしたり高いスループットを提供したりする代わりに、信頼性と遅延の少なさを主眼に置いてします。また、その他の設定はセキュリティを低下させますが、性能を改善することができるようになっています。</para>
 <sect1 xml:id="sec-tuning-network-buffers">
  <title>カーネルのソケットバッファの設定</title>

  <para>昨今のネットワーク通信の多くは TCP/IP プロトコルをベースにして行われていて、実際の処理はソケットインターフェイスを使用するのが一般的です。 TCP/IP に関する詳細については、 <xref linkend="cha-network"/> をお読みください。 Linux カーネルでは、ソケットインターフェイスを介して、バッファ内に受信したデータや送信すべきデータを蓄積し、必要な送受信を行います。これらのカーネルのソケットバッファについては、チューニングによる調整を行うことができます。</para>


  <important>
   <title>TCP の自動チューニングについて</title>
   <para>カーネルバージョン 2.6.17 もしくはそれ以降のバージョンでは、最大バッファサイズ 4 MB で自動チューニングを行うようになっています。この仕組みにより、手作業でチューニングを行ったとしても、通常はネットワーク性能を改善できないことを意味しています。また下記の変数についても、通常は変更せずにそのままにしておくことが最適です。変更を行う場合は、チューニング作業による影響をよく確認しておくことをお勧めします。</para>
   <para>また、古いバージョンのカーネルから更新した場合は、手作業で行っていた TCP のチューニングを削除し、自動チューニングに任せることをお勧めします。</para>
  </important>

  <para><filename>/proc</filename> ファイルシステム内にある特殊なファイルを使用することで、カーネルのソケットバッファのサイズや動作を変更することができます。 <filename>/proc</filename> ファイルシステムに関する一般的な情報については、 <xref linkend="sec-util-proc"/> をお読みください。このうち、ネットワーク関連のファイルは下記の中に含まれています:</para>

<screen>/proc/sys/net/core
/proc/sys/net/ipv4
/proc/sys/net/ipv6</screen>

  <para>一般的な <systemitem>net</systemitem> 関係の変数は、カーネルのドキュメンテーション ( <filename>linux/Documentation/sysctl/net.txt</filename> ) 内に説明があります。また、 <systemitem>ipv4</systemitem> 以下の変数は、 <filename>linux/Documentation/networking/ip-sysctl.txt</filename> と <filename>linux/Documentation/networking/ipvs-sysctl.txt</filename> 内に説明があります。</para>

  <para><filename>/proc</filename> ファイルシステム内では、たとえば全てのプロトコルに対する最大ソケット受信バッファサイズや最大ソケット送信バッファサイズを設定することができます。この場合、これらを TCP プロトコルにのみ適用することができる (<filename>ipv4</filename> 内) ほか、全てのプロトコルに対して上書きで設定することのできるもの (<filename>core</filename> 内) もあります。</para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename></term>
    <listitem>
     <para><filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename> を <literal>1</literal> に設定すると、自動チューニング機能が有効化され、バッファサイズが動的に調整されるようになります。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_rmem</filename></term>
    <listitem>
     <para>3 種類の値を設定する変数で、 1 接続あたりのメモリ内受信バッファの最小値／初期値／最大値をそれぞれ設定します。ここでは TCP のウインドウサイズだけでなく、実際のメモリ使用量を調整することにもなります。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_wmem</filename></term>
    <listitem>
     <para><filename>tcp_rmem</filename> と同じような変数ですが、こちらは 1 接続あたりのメモリ内の送信バッファを設定します。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/core/rmem_max</filename></term>
    <listitem>
     <para>アプリケーション側から要求することのできる、最大の受信バッファサイズを制限するための変数です。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/core/wmem_max</filename></term>
    <listitem>
     <para>アプリケーション側から要求することのできる、最大の送信バッファサイズを制限するための変数です。</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para><filename>/proc</filename> を利用することで、不要な TCP 機能を無効化することができます (既定では全ての TCP 機能が有効化されています) 。たとえば下記のようなファイルがあります:</para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_timestamps</filename></term>
    <listitem>
     <para>TCP のタイムスタンプ機能は、 RFC1323 で規定されているものです。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_window_scaling</filename></term>
    <listitem>
     <para>TCP のウインドウスケーリングについても、 RFC1323 で規定されています。</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_sack</filename></term>
    <listitem>
     <para>選択的確認応答 (SACK) の設定を行います。</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para><command>sysctl</command> コマンドを使用することで、 <filename>/proc</filename> ファイルシステム内の変数を読み込んだり書き込んだりすることができます。 <command>sysctl</command> コマンドは <filename>/etc/sysctl.conf</filename> ファイルから設定を読み込む仕組みであり、これによってシステムを再起動しても設定を再適用することができるため、 <command>cat</command> (読み込み) や <command>echo</command> (書き込み) を利用して <filename>/proc</filename> ファイルシステムにアクセスするよりは、 <command>sysctl</command> コマンドを使用することをお勧めします。 <command>sysctl</command> コマンドでの変数の読み込みや書き込みは簡単で、たとえば下記のように入力して実行することで、 TCP 関連の変数を全て表示することができます:</para>

<screen>&prompt.sudo;sysctl -a | grep tcp</screen>

<!--
  cf. id="sec-tuning-taskscheduler-cfs-tuning" -->

  <note>
   <title>ネットワーク変数のチューニングによる副次的な影響について</title>
   <para>ネットワーク変数のチューニングによって、 CPU やメモリなどの他のシステムリソースに影響がある場合があります。 <!-- (p.124)--> <!-- Also see "Tuning TCP behavior", ibm p. 130 --></para>
  </note>
 </sect1>
 <sect1 xml:id="sec-tuning-network-analyzing">
  <title>ネットワーク内でのボトルネックの発見とネットワークトラフィックの分析</title>

  <para>ネットワークのチューニングを行う前に、あらかじめネットワーク内にボトルネックが存在していないかを確認し、ネットワークのトラフィックパターンについても調べておくことが重要です。これらを実施するために、いくつかのツールが提供されています。</para>

  <para>ネットワークトラフィックを分析するには、 <command>netstat</command> , <command>tcpdump</command> , <command>wireshark</command> などのツールをお使いいただくことができます。 Wireshark はネットワークトラフィックアナライザです。</para>
 </sect1>
<!-- ibm 33:
      offload depends on the adapter's features.
      ibm 34:
     Bonding: Documentation/networking/bonding.txt.
     link aggregation and load balancing
     Check, whether this is already describe somewhere else!
     Obviously in ha and xen guides...
 -->
 <sect1 xml:id="sec-tuning-network-netfilter">
  <title>netfilter</title>

  <para>Linux におけるファイアウオール機能やマスカレード機能は、 netfilter と呼ばれるカーネルモジュールが提供する機能です。このモジュールは、ルールベースのフレームワークら介して、高度に設定することができます。あるパケットが指定したルールに該当した場合、 netfilter ではパケットを受け付けるか拒否するかを選択することができるほか、特殊なアクション ( <quote>ターゲット</quote> と呼びます) を指定して、アドレス変換などの処理を行うこともできます。</para>

  <para>netfilter には多数の設定項目が存在しています。そのため、ルールを多く設定すればするほど、パケットの処理にも時間がかかることになります。また、高度な接続追跡機能を使用すると、より CPU 負荷がかかることになりますので、ネットワーク全体の処理の低下にも繋がります。</para>

<!-- : security vs. performance -->

  <para>カーネル側のキューがいっぱいになると、新しく届いたパケットが廃棄されるようになります。これにより、既存の接続が正しく動作しなくなってしまいます。 '故障時開' (fail-open) の機能を使用すると、ネットワークトラフィックが多すぎる場合、一時的にパケットの調査を無効化して接続を維持することができるようになります。詳しくは <link xlink:href="https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/"/> (英語) をお読みください。</para>

  <para>詳しくは netfilter/iptables プロジェクトの Web ページ <link xlink:href="https://www.netfilter.org"/> (英語) をご覧ください。</para>
 </sect1>
 <sect1 xml:id="sec-tuning-network-rps">
  <title>Receive Packet Steering (RPS) によるネットワーク性能の改善</title>

  <para>新しいネットワークインターフェイスを使用している場合、多数のパケットを取り扱うことになることから、ホスト側が性能面のボトルネックとなる場合があります。これらのパケットを問題なく扱うことができるようにするため、システム側では複数の CPU コアに分散させて処理を行わなければなりません。</para>

  <para>新しいネットワークインターフェイスの場合、ハードウエア側に実装された複数の送受信キューを使用することで、複数の CPU コアに処理を分散させることができます。ハードウエア側にそのような仕組みが用意されておらず、単一のキューしか用意されていない場合、ドライバは単一の直列化されたストリームを介して、全ての到着パケットを処理しなければならなくなります。この問題に対応するには、オペレーティングシステムがストリームを <quote>並列化</quote> して、複数の CPU に処理を分散させなければなりません。このような分散処理の仕組みが Receive Packet Steering (RPS) です。 RPS は仮想環境でも使用することができます。</para>

  <para>RPS は各データストリームに対して、 IP アドレスとポート番号をベースにしたユニークなハッシュを作成します。このハッシュを使用することで、同じデータストリームを同じ CPU で処理できるようにし、性能を向上させることができるようになっています。</para>

  <para>RPS はネットワークデバイスの受信キューおよびインターフェイスごとに設定することができます。設定のファイル名は、下記のようになっています:</para>

<screen>/sys/class/net/<replaceable>デバイス名</replaceable>/queues/<replaceable>受信キュー名</replaceable>/rps_cpus</screen>

  <para><replaceable>デバイス名</replaceable> にはネットワークデバイスのデバイス名が入ります。たとえば <literal>eth0</literal> , <literal>eth1</literal> のような名前になります。また、 <replaceable>受信キュー名</replaceable> には、受信キューの名前が入ります。たとえば <literal>rx-0</literal> , <literal>rx-1</literal> のような名前になります。</para>

  <para>ネットワークインターフェイスが単一の受信キューしか提供していない場合は、 <literal>rx-0</literal> のみが存在することになります。複数の受信キューに対応している場合は、 rx- <replaceable>N</replaceable> のディレクトリが複数存在していることになります。</para>

  <para>下記の設定ファイルには、 <!-- NOTE: "comma-delimited"? --> CPU をビットマップ形式で指定します。既定では全てのビットが <literal>0</literal> になっています。この設定では、 RPS が無効化されているため、割り込みを処理した CPU がパケットキューの処理をも行うことになります。</para>

  <para>RPS を有効化し、特定の CPU がインターフェイスの受信キューを処理するように設定するには、その CPU のビットを <literal>1</literal> に設定します。たとえば CPU 0 から 3 までを eth0 の最初の受信キューの処理に使用したい場合は、ビット 0 から 3 までを 1 にした値、つまり 2 進数で <literal>00001111</literal> を設定します。なお、実際の設定作業は、 16 進数で指定します。この場合は <literal>F</literal> を設定することになります。このことから、設定作業は下記のようになります:</para>

<screen>&prompt.sudo;echo "f" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</screen>

  <para>CPU 8 から 15 までを有効化したい場合は、下記のように設定します:</para>

<screen>1111 1111 0000 0000 (2 進数)
15     15    0    0 (10 進数)
F       F    0    0 (16 進数)</screen>

  <para>生成できた 16 進数値は <literal>ff00</literal> になりますので、下記のコマンドで設定します:</para>

<screen>&prompt.sudo;echo "ff00" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</screen>

  <para>NUMA マシンの場合、 RPS を設定してインターフェイスの受信キュー割り込みの処理と同じ NUMA ノードの CPU で処理を行うようにすると、最適な性能を発揮できるようになります。</para>

  <para>非 NUMA マシンの場合、全ての CPU を使用することができます。割り込みの割合が大きい場合、ネットワークインターフェスを処理している CPU を除外することで、性能を発揮できるようになります。ネットワークインターフェイスを処理している CPU は、 <filename>/proc/interrupts</filename> ファイルから判断することができます。たとえば下記のようになります:</para>

<screen>&prompt.sudo;cat /proc/interrupts
            CPU0       CPU1       CPU2       CPU3
...
  51:  113915241          0          0          0      Phys-fasteoi   eth0
...</screen>

  <para>この場合、 <literal>eth0</literal> の割り込みを処理しているのは <literal>CPU 0</literal> になります。これは、 <literal>CPU0</literal> のみが 0 より大きくなっているためです。</para>

  <para>&x86; および &x86-64; プラットフォームの場合、 <command>irqbalance</command> を使用することで、ハードウエア割り込みを CPU 間に分散させることができます。詳しくは <command>man 1 irqbalance</command> をお読みください。</para>
 </sect1>
</chapter>
