<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-tuning-io">
 <title>I/O 性能のチューニング</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para>I/O スケジューリングの制御とは、ストレージに対して送信される入出力操作をどのような順序にするのかを決めるものです。 &productname; では <literal>エレベータ</literal> と呼ばれる様々な I/O アルゴリズムに対応しています。これにより、様々な使用環境に対応することができるようになっています。エレベータはシーク操作を減らすためのものであるほか、 I/O 要求に対する優先順位付けとしても使用されます。</para>
 <para>最適な I/O エレベータを選択するにあたって、その根拠となるのは負荷だけではなく、ハードウエアの仕様も含みます。たとえば単一の ATA ディスクと SSD, RAID アレイやネットワークストレージシステムでは、それぞれ異なるチューニング戦略が必要となります。</para>
 <sect1 xml:id="cha-tuning-io-switch">
  <title>I/O スケジューリングの切り替え</title>

  <para>&productname; では起動時に既定の I/O スケジューラを選択しますが、 I/O スケジューラはデバイス単位で即時に変更することができます。これにより、システムパーティションを含んでいるデバイスと、データベースを含んでいるデバイスで別々のアルゴリズムを設定したりすることができるようになっています。</para>

  <para>既定の I/O スケジューラは、デバイス側からの情報内に回転型のディスクである旨が書かれているかどうかによって決まります。回転型のディスクではない場合、 <systemitem class="resource">DEADLINE</systemitem> I/O スケジューラが選択されます。回転型のディスクである場合は、 <systemitem class="resource">CFQ</systemitem> (Completely Fair Queuing; 完全公平型キューイング) が選択されます。この既定値を変更したい場合は、下記の起動パラメータを設定してください:</para>

<screen>elevator=<replaceable>スケジューラ</replaceable></screen>

  <para>ここで、 <replaceable>スケジューラ</replaceable> には <option>cfq</option> , <option>noop</option> , <option>deadline</option> のいずれかを指定します。詳しくは <xref linkend="cha-tuning-io-schedulers"/> をお読みください。</para>

  <para>動作中のシステム内で、特定のデバイスに対するエレベータを変更したい場合は、下記のコマンドを実行します:</para>

<screen>&prompt.sudo;echo <replaceable>スケジューラ</replaceable> &gt; /sys/block/<replaceable>デバイス名</replaceable>/queue/scheduler</screen>

  <para>ここで、 <replaceable>スケジューラ</replaceable> には <option>cfq</option> , <option>noop</option> , <option>deadline</option> のいずれかを指定します。また <replaceable>デバイス名</replaceable> には、ブロックデバイスのデバイス名 (例: <systemitem>sda</systemitem>) を指定します。ただし、この手順で変更を行なった場合、システムを再起動すると既定値に戻ってしまうことに注意してください。特定のデバイスに対して I/O スケジューラを恒久的に変更したい場合は、起動時に実行されるスクリプトの中で上記を実行するように設定するか、もしくは <filename>/lib/udev/rules.d/</filename> 内に適切な udev ルールを記述してください。この方法によるチューニングについて、詳しくは <filename>/lib/udev/rules.d/60-ssd-scheduler.rules</filename> をお読みください。</para>

  <note os="sles" arch="zseries">
   <title>&zseries; での既定のスケジューラについて</title>
   <para>&zseries; 環境では、既定の I/O スケジューラはデバイスドライバ側で設定されます。</para>
  </note>
 </sect1>
 <sect1 xml:id="cha-tuning-io-schedulers">
  <title>利用可能な I/O エレベータ</title>

  <para>下記の表には、 &productname; で利用可能なエレベータの一覧を示しています。それぞれのエレベータにはさらに細かくチューニングを行なうための <quote>チューナブル</quote> パラメータが存在していますが、これらは下記のようなコマンドを実行することで、設定することができます:</para>

<screen>&prompt.sudo;echo <replaceable>値</replaceable> &gt; /sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/<replaceable>チューナブル名</replaceable></screen>

  <para>ここで、 <replaceable>チューナブル名</replaceable> には下記に示すチューナブルの名前を、 <replaceable>デバイス名</replaceable> にはブロックデバイス名をそれぞれ指定します。</para>

  <para>また、既定で使用されるエレベータを確認したい場合は、下記のようなコマンドを入力して実行します。現在選択されているエレベータが [] で括られて出力されます:</para>

<screen>&wsI;:~ # cat /sys/block/sda/queue/scheduler
noop deadline [cfq]</screen>

  <para>このファイルには <literal>none</literal> という文字列が含まれることがありますが、これはそのデバイスに対して、 I/O スケジューリングを行なわない意味になります。これは通常、対象のデバイスがマルチキュー型のキューイング機構を持っている場合に現われるものです (詳しくは <xref linkend="cha-tuning-io-scsimq"/> をお読みください) 。</para>

  <sect2 xml:id="sec-tuning-io-schedulers-cfq">
   <title><systemitem class="resource">CFQ</systemitem> (Completely Fair Queuing; 完全公平型キューイング)</title>
   <para><systemitem class="resource">CFQ</systemitem> は公平性を重視したスケジューラで、 &productname; では既定で使用されるスケジューラでもあります。このアルゴリズムでは、 I/O をディスクに送信することのできる各スレッドに対して、同じだけのタイムスライスを割り当てます。この仕組みにより、各スレッドに公平な I/O スループットを割り当てることになります。なおスケジューリングの決定においては、 I/O 優先順位も考慮して割り当てが行なわれることになります (詳しくは <xref linkend="cha-tuning-resources-disk-ionice"/> をお読みください) 。また、 <systemitem class="resource">CFQ</systemitem> スケジューラには下記のチューナブルパラメータが用意されています:</para>
   <variablelist>
    <varlistentry>
     <term><filename> /sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/slice_idle_us </filename></term>
     <listitem>
      <para>タスクに対して割り当てられたタイムスライス内で、それ以上送信すべき I/O がなくなった場合、 I/O スケジューラはしばらく待機してから次のスレッドに移行します。 <filename>slice_idle_us</filename> では、そのような I/O スケジューラの待機時間を、マイクロ秒単位で指定することができます。 <filename>slice_idle</filename> でも同じ設定を行なうことができますが、こちらはミリ秒単位での指定になります。 I/O を待機する時間を長くすることで、 I/O のローカリティ (局所性) を改善することになります。これに加えて、依存関係にある複数の I/O を行なっているプロセスに対して、タイムスライスを多く割り当てる結果になることから、この処理も改善することができます。依存関係にある I/O とは、一方の I/O が他方の I/O を生み出すような結果になる場合を指します。たとえば、読み込むべきデータブロックを知るためにインデックス (索引) ブロックを読み出すような処理があった場合、それらは依存関係にある I/O であると言えます。</para>
      <para>なお、ローカリティが性能面で大きな意味を持たない (たとえば SSD や多数のディスクが搭載された SAN など) メディアである場合は、 <filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/slice_idle_us</filename> を <literal>0</literal> に設定することで、スループットをかなり改善することができます。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename> /sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/quantum </filename></term>
     <listitem>
      <para>このオプションは、デバイス側で一括で処理されるリクエストの最大数を指定します。既定値は <literal>4</literal> です。複数のディスクから構成されるストレージの場合、この設定では不必要なリクエスト同時処理数を設定することになってしまいます。そのため、そのような環境であれば、値を増やすことで性能を改善することができます。ただし、この設定を増やしてしまうことで、ストレージ内にバッファリングされたリクエストが増えることになってしまいますので、特定の I/O 操作が遅延する結果になることもあります。この値を変更する場合は、 <filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/slice_async_rq</filename> (既定値: <literal>2</literal> ) についても、変更を検討してください。こちらの値は、一定のタイムスライス内に送信することのできる、非同期リクエスト (通常は書き込みリクエスト) の最大数を指定するためのものです。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/low_latency</filename></term>
     <listitem>
      <para>有効化した場合 (&productname; では既定で有効化されています) 、スケジューラは <literal>target_latency</literal> というチューニングパラメータで指定された値になるよう、動的にタイムスライスを調整するようになります。タイムスライスは、この <literal>target_latency</literal> に適合するように再計算され、限られた時間内でプロセスが公平にアクセスを取得できるようにします。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/target_latency</filename></term>
     <listitem>
      <para><systemitem class="resource">CFQ</systemitem> に対する待機時間の目標値を指定します。 <systemitem class="resource">CFQ</systemitem> では、この値を利用して、各タスクに割り当てるタイムスライスを調整します。</para>
     </listitem>
    </varlistentry>
     <varlistentry>
       <term><filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/group_idle_us</filename></term>
       <listitem>
         <para>依存関係にある I/O を行なう際、 blkio cgroup の処理時間が枯渇することを防ぐため、 CFQ は異なる blkio cgroup に対する I/O スケジューリングを行なう前に、 I/O が完了しても少しの時間だけ待機する処理を行ないます。 <literal>slice_idle_us</literal> が設定されている場合、このパラメータを設定しても大きな影響は編ません。しかしながら高速なメディアの場合、 <literal>slice_idle_us</literal> によるオーバーヘッドは無視できない大きさになってしまいます。 <literal>slice_idle_us</literal> を無効化して <literal>group_idle_us</literal> を指定することで、依存関係にある I/O をより低いオーバーヘッドで blkio cgroup に処理時間を与えられるようになります。なお、 <filename>group_idle</filename> というチューナブルもありますが、こちらはミリ秒の単位で指定します。</para>
       </listitem>
     </varlistentry>
   </variablelist>
   <example>
    <title><systemitem class="resource">CFQ</systemitem> を利用した個別スレッドに対するスループットの改善</title>
    <para>&productname; &productnumber; では、 <literal>low_latency</literal> チューニングパラメータが既定で有効化されていて、限られた時間内でプロセスが公平にアクセスを取得できるようになっています (なお、このパラメータは <phrase os="sles;sled">&sle; 12</phrase> <phrase os="osuse">&opensuse; Leap</phrase> 以前のバージョンでは有効化されていませんでした) 。</para>
    <para>これは通常、プロセスがトランザクションの一部として I/O を実行しているようなサーバ用途で、各トランザクションに必要な時間を予測できるようにするために必要な設定です。しかしながら、このような動作では問題が発生する場合があります:</para>
    <itemizedlist>
     <listitem>
      <para>改善したい性能指標が I/O の集中する単一のプロセスである場合。</para>
     </listitem>
     <listitem>
      <para>処理をできる限り素早く終わらせなければならず、かつ複数の I/O 発信源が存在するような場合。この場合は、 I/O スケジューラが不公平に扱うことによって、トランザクションをより素早く追わせることができるためです。各プロセスはいったんタイムスライスを入手すると、それをできる限り素早く処理して終わらせようとしますので、これによって全体の競合を防ぐことができるようになります。</para>
     </listitem>
    </itemizedlist>
    <para>この問題に対応する手段として、 2 つの方法が用意されています。具体的には、 <literal>target_latency</literal> を増やすか、もしくは <literal>low_latency</literal> を無効にするかです。なお、全てのチューニングパラメータに対して、チューニングによる変更前後で負荷が正しく処理されるかどうかを確認しておく必要があります。また、受け持つ負荷が各プロセスのピーク性能によって決まるのか、それとも公平性によって全体的に改善を行なわなければならないものなのかについても、慎重に確認しておくことをお勧めします。また、性能は下位に存在するストレージに依存して決まるものであり、一方の環境で正しいチューニングであっても、他方の環境では正しくないこともあります。</para>
    <para>下記の例は、 I/O の開始時には制御を行なわないものの、シンプルに説明することができる例です。この例では、 32 個のプロセスが小さいサイズの書き込みを並行して行なっています。 &productname; の既定値 (つまり <literal>low_latency</literal> が有効化されている場合) では、結果は下記のようになります:</para>
<screen>&prompt.root;echo 1 &gt; /sys/block/sda/queue/iosched/low_latency
&prompt.root;time ./dd-test.sh
10485760 bytes (10 MB) copied, 2.62464 s, 4.0 MB/s
10485760 bytes (10 MB) copied, 3.29624 s, 3.2 MB/s
10485760 bytes (10 MB) copied, 3.56341 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.56908 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.53043 s, 3.0 MB/s
10485760 bytes (10 MB) copied, 3.57511 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.53672 s, 3.0 MB/s
10485760 bytes (10 MB) copied, 3.5433 s, 3.0 MB/s
10485760 bytes (10 MB) copied, 3.65474 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.63694 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.90122 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.88507 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.86135 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.84553 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.88871 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 3.94943 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 4.12731 s, 2.5 MB/s
10485760 bytes (10 MB) copied, 4.15106 s, 2.5 MB/s
10485760 bytes (10 MB) copied, 4.21601 s, 2.5 MB/s
10485760 bytes (10 MB) copied, 4.35004 s, 2.4 MB/s
10485760 bytes (10 MB) copied, 4.33387 s, 2.4 MB/s
10485760 bytes (10 MB) copied, 4.55434 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.52283 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.52682 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.56176 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.62727 s, 2.3 MB/s
10485760 bytes (10 MB) copied, 4.78958 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.79772 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.78004 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.77994 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.86114 s, 2.2 MB/s
10485760 bytes (10 MB) copied, 4.88062 s, 2.1 MB/s

real    0m4.978s
user    0m0.112s
sys     0m1.544s</screen>
    <para>上記では、それぞれのプロセスがほぼ同等の時間で終わっていることに注意してください。ここでは <systemitem class="resource">CFQ</systemitem> のスケジューラに従い、 <literal>target_latency</literal> の値に適合するように調整が図られています。つまり、各プロセスはストレージに対して同じ程度のアクセス時間を与えられています。</para>
    <para>なお、初めのうちは比較的高速に動作していることにも注意してください。これはプロセスの起動タイミングが正確に一致していないためで、より複雑な例では、これを制御して行ないます。</para>
    <para>low_latency を無効化すると、下記のようになります:</para>
<screen>&prompt.root;echo 0 &gt; /sys/block/sda/queue/iosched/low_latency
&prompt.root;time ./dd-test.sh
10485760 bytes (10 MB) copied, 0.813519 s, 12.9 MB/s
10485760 bytes (10 MB) copied, 0.788106 s, 13.3 MB/s
10485760 bytes (10 MB) copied, 0.800404 s, 13.1 MB/s
10485760 bytes (10 MB) copied, 0.816398 s, 12.8 MB/s
10485760 bytes (10 MB) copied, 0.959087 s, 10.9 MB/s
10485760 bytes (10 MB) copied, 1.09563 s, 9.6 MB/s
10485760 bytes (10 MB) copied, 1.18716 s, 8.8 MB/s
10485760 bytes (10 MB) copied, 1.27661 s, 8.2 MB/s
10485760 bytes (10 MB) copied, 1.46312 s, 7.2 MB/s
10485760 bytes (10 MB) copied, 1.55489 s, 6.7 MB/s
10485760 bytes (10 MB) copied, 1.64277 s, 6.4 MB/s
10485760 bytes (10 MB) copied, 1.78196 s, 5.9 MB/s
10485760 bytes (10 MB) copied, 1.87496 s, 5.6 MB/s
10485760 bytes (10 MB) copied, 1.9461 s, 5.4 MB/s
10485760 bytes (10 MB) copied, 2.08351 s, 5.0 MB/s
10485760 bytes (10 MB) copied, 2.28003 s, 4.6 MB/s
10485760 bytes (10 MB) copied, 2.42979 s, 4.3 MB/s
10485760 bytes (10 MB) copied, 2.54564 s, 4.1 MB/s
10485760 bytes (10 MB) copied, 2.6411 s, 4.0 MB/s
10485760 bytes (10 MB) copied, 2.75171 s, 3.8 MB/s
10485760 bytes (10 MB) copied, 2.86162 s, 3.7 MB/s
10485760 bytes (10 MB) copied, 2.98453 s, 3.5 MB/s
10485760 bytes (10 MB) copied, 3.13723 s, 3.3 MB/s
10485760 bytes (10 MB) copied, 3.36399 s, 3.1 MB/s
10485760 bytes (10 MB) copied, 3.60018 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.58151 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.67385 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.69471 s, 2.8 MB/s
10485760 bytes (10 MB) copied, 3.66658 s, 2.9 MB/s
10485760 bytes (10 MB) copied, 3.81495 s, 2.7 MB/s
10485760 bytes (10 MB) copied, 4.10172 s, 2.6 MB/s
10485760 bytes (10 MB) copied, 4.0966 s, 2.6 MB/s

real    0m3.505s
user    0m0.160s
sys     0m1.516s</screen>
    <para>上記では、各プロセスに対して公平な時間が割り当てられていないことから、処理にかかる時間が大きくばらついていることに注意してください。プロセスによっては高速に動作して終わっているものもありますので、これによって全体もより高速に処理できていることになり、見かけ上のスループットも多角になっています。また、マシン側のリソース状況や接続されているストレージの種類に依存して性能が決まるため、全てのシステムで同じような結果になるとは限らないことにも注意してください。</para>
    <para>また、どの環境でも確実に性能を向上することのできるチューニングオプションというものも、存在していないことに注意してください。状況によって最適なチューニングは異なりますし、負荷ごとに要件も異なりますので、これによって設定内容も異なることになります。</para>
   </example>
  </sect2>

  <sect2 xml:id="sec-tuning-io-schedulers-noop">
   <title><systemitem class="resource">NOOP</systemitem></title>
   <para>届いた I/O リクエストをそのまま流すだけの単純なスケジューラです。他のスケジューラとの比較のために用意されていて、たとえばチューニングによって性能が悪化していないことを確認するために使用します。</para>
   <para>このスケジューラは、デバイス側で I/O スケジューリングを実施するような、たとえばインテリジェント型ストレージやマルチパス環境などでお勧めです。ホスト側でより複雑なスケジューラを選択してしまうと、ホスト側とデバイス側のスケジューリングが競合してしまい、性能が落ちてしまうことがあるからです。このようなストレージデバイスの場合は、デバイス側で I/O のスケジュールを実施するのが適切です。</para>
   <para>同様の理由により、このスケジューラは仮想マシンにもお勧めです。</para>
   <para>また、 <systemitem class="resource">NOOP</systemitem> スケジューラは、機械的な動作のない、 SSD のようなデバイスに対しても便利な仕組みです。ただし SSD の場合、通常は <systemitem class="resource">DEADLINE</systemitem> I/O スケジューラのほうが適切です。しかしながら、 <systemitem class="resource">NOOP</systemitem> はオーバーヘッドの少ない仕組みであることから、特定の負荷に対しては性能を向上させることになります。</para>
  </sect2>

  <sect2 xml:id="sec-tuning-io-schedulers-deadline">
   <title><systemitem class="resource">DEADLINE</systemitem></title>
   <para><systemitem class="resource">DEADLINE</systemitem> は遅延時間の削減を重視したスケジューラです。それぞれの I/O リクエストには期限が設定されます。通常はセクタ番号順に並べ替えられてキュー (読み込みおよび書き込み) 内に保存されますが、 <systemitem class="resource">DEADLINE</systemitem> アルゴリズムでは期限順に並べ替えられた 2 種類の追加キュー (読み込みおよび書き込み) を管理します。リクエストの期限が切れない限り、 <quote>セクタ順</quote> のキューを使用します。いったん期限切れのものが発生すると、期限切れのリクエストがなくなるまで、 <quote>期限順</quote> のキューを使用して処理を行なうようになります。また、一般的にアルゴリズムは書き込みよりも読み込みを優先します。</para>
   <para>このスケジューラは、いくつかのスレッドが読み込みと書き込みを行なっているものの、公平性を重視しないような環境で、 <systemitem class="resource">CFQ</systemitem> I/O スケジューラよりも高い性能を提供します。たとえば SAN とデータベースから読み込みを行なっているプロセスがいくつか存在しているような場合 (特に <quote>TCQ</quote> ディスクを使用しているような場合) がそれにあたります。 <systemitem class="resource">DEADLINE</systemitem> スケジューラには、下記のようなチューナブルパラメータが用意されています:</para>
   <variablelist>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/writes_starved</filename></term>
     <listitem>
      <para>ディスクに対して書き込みを送信できるようになるまでに、どれだけの数の読み込みをディスクに送信できるようにするのかを制御します。たとえば <literal>3</literal> という値であれば、 1 回の書き込み要求に対して 3 回の読み込み処理を行なうことになります。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/read_expire</filename></term>
     <listitem>
      <para>ミリ秒単位で読み込み処理の期限 (現在時刻からの経過時間) を指定します。既定値は 500 です。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>デバイス名</replaceable>/queue/iosched/write_expire</filename></term>
     <listitem>
      <para><!-- NOTE: invalid explanation? it is for read_expire.. -->ミリ秒単位で書き込み処理の期限 (現在時刻からの経過時間) を指定します。既定値は 500 です。</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-tuning-io-barrier">
  <title>I/O バリアのチューニング</title>

  <para><remark>sknorr, 2014-07-24: This might need updating to include Btrfs.</remark> ほとんどのファイルシステム (xfs, ext3, ext4 など) では、 fsync やトランザクションのコミット時に、ディスクに対して書き込みバリアを送信します。書き込みバリアは書き込みの順序を保証するための仕組みで、これによって揮発性のあるディスクの書き込みキャッシュを安全に使用できるようにしています (ただし、これによって少しの性能劣化があります) 。お使いのディスクに何らかの方式によるバッテリーが搭載されている場合、バリアを無効化しても、安全に性能を改善することができます。</para>

  <para>書き込みバリア送信の無効化は、 <option>nobarrier</option> マウントオプションで行なうことができます。</para>

  <warning>
   <title>バリアの無効化によるデータ損失の危険性について</title>
   <para>電源障害時にキャッシュからディスクへの書き込みが正しく保証されない環境でバリアを無効化すると、ファイルシステムの破壊やデータ損失が発生することがあります。</para>
  </warning>
 </sect1>

 <sect1 xml:id="cha-tuning-io-scsimq">
  <title>SCSI に対する既定での blk-mq I/O パスの有効化</title>

  <para>ブロックマルチキュー (blk-mq) は複数のキューを利用するブロック I/O キューイング機構です。 blk-mq では CPU ごとにソフトウエアキューを用意して、 I/O リクエストをため込みます。このソフトウエアキューは 1 つ以上のハードウエア発信キューに割り当てられます。このような構造により、 blk-mq はロック競合を防ぐことができるようになっています。 blk-mq では、 1 秒あたりに多くの I/O 操作 (IOPS) を行なうことのできるデバイスで、性能改善に役立ちます。 blk-mq は NVM Express デバイスなど、いくつかのデバイスに対しては既定で有効化されています。</para>

  <para>なお、現時点では blk-mq は I/O スケジューリングに対応していません (CFQ, deadline のどちらも非対応です) 。この I/O スケジューリングの機能が欠けていることによって、回転型のディスクを使用している場合、明確な性能劣化が派生します。そのため、 SCSI デバイスに対しては、既定で blk-mq が無効化されるようになっています。</para>

  <para>通常のハードディスクではなく、高速な SCSI デバイス (例: SSD) をお持ちの場合は、 SCSI に対して blk-mq への切り替えをご検討ください。切り替えは、カーネルのコマンドラインオプションに <literal>scsi_mod.use_blk_mq=1</literal> を追加することで行なうことができます。</para>
 </sect1>
</chapter>
