<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "generic-entities.ent">
%entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-tuning-memory">
<!-- fs 2015-05-28
     No longer supported: http://bugzilla.suse.com/show_bug.cgi?id=874971

 <sect1 id="cha-tuning-memory-numa">
  <title>Non-uniform memory access (NUMA)</title>

  <para>
   Another increasingly important role of the VM is to provide good NUMA
   allocation strategies. NUMA stands for non-uniform memory access, and
   most of today's multi-socket servers are NUMA machines. NUMA is a
   secondary concern to managing swapping and caches in terms of
   performance, and there are lots of documents about improving NUMA memory
   allocations. One particular parameter interacts with page reclaim:
  </para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/vm/zone_reclaim_mode</filename>
    </term>
    <listitem>
     <para>
      This parameter controls whether memory reclaim is performed on a local
      NUMA node even if there is plenty of memory free on other nodes. This
      parameter is automatically turned on on machines with more pronounced
      NUMA characteristics.
     </para>
     <para>
      If the VM caches are not being allowed to fill all of memory on a NUMA
      machine, it could be because of zone_reclaim_mode being set. Setting to 0
      will disable this behavior.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
-->
 <title>メモリ管理サブシステムのチューニング</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
    <para>カーネルのメモリ管理動作を理解してチューニングするには、まず動作に関する概要と、他のサブシステムとの連携を理解するところから始めるのがよいでしょう。</para>
 <para><remark>sknorr, 2014-08-21: Using VM for "virtual memory manager" is really dangerous - everyone knows that VM means "virtual machine" and half of the people reading this chapter might skip this prargraph and be completely confused. At least use VMM or shorten "memory management subsystem" to "memory subsystem", the latter of which I think should be short enough. </remark> メモリ管理サブシステムは仮想メモリマネージャとも呼ばれ、下記では <quote>VM</quote> と略しています。 VM の役割は、カーネル全体とユーザプログラムに対する物理メモリ (RAM) の割り当て管理です。それだけではなく、ユーザプロセスに対して、仮想メモリ環境を提供する責任も負っています (Linux 拡張付きの POSIX API を介して管理します) 。最後に、 VM はメモリが枯渇した場合に、キャッシュを解放したり <quote>匿名</quote> メモリをスワップアウトしたりすることで、メモリを空ける処理も行います。</para>
 <para>VM の調査やチューニングに際して理解しておくべき最重要事項は、キャッシュの管理方法についてです。 VM のキャッシュの基本的なゴールは、スワップやファイルシステムの操作(ネットワークファイルシステムを含みます) を行うことによって生成される I/O のコストを、最小化することにあります。これは I/O を排除するか、もしくはよりよいパターンで I/O を送信することによって達成します。</para>
 <para>空きメモリは、必要に応じてキャッシュとして使用されます。キャッシュや匿名メモリとして使用できるメモリ領域が大きければ大きいほど、より効率的にキャッシュやスワップを制御できるようになります。しかしながら、いったんメモリが枯渇してしまった場合は、キャッシュを解放するか、メモリをスワップアウトする必要があります。</para>
 <para>適切な負荷範囲であれば、性能を改善するにあたって最初にやるべきことは、メモリを増やしてキャッシュの効率を上げ、キャッシュの解放やスワップアウトの頻度を減らすのがよいでしょう。次にやるべきことは、カーネルのパラメータを変更して、キャッシュの管理方法を変えることです。</para>
 <para>最後に、負荷それ自身についても調査してチューニングする必要があります。アプリケーション側で多くのプロセスやスレッドを動作させることができる場合、各プロセスがファイルシステム内の独自の領域を利用していると、 VM の効率が落ちてしまいます。また、メモリによるオーバーヘッドも増えてしまいます。アプリケーション側で独自のバッファやキャッシュを持っている場合、そのキャッシュを大きくしてしまうと、 VM 側に割り当てることのできるサイズが小さくなってしまいます。しかしながら、プロセスやスレッドの数を増やすことで、 I/O の重複度合いやパイプライン処理の機会が増えることがありますので、これによってマルチコア環境で性能が向上することもあります。従って、最適な結果を得るためには、実験が必要ということになります。</para>
 <sect1 xml:id="cha-tuning-memory-usage">
  <title>メモリの用途</title>

  <para>メモリの割り当ては <quote>ピン済み</quote> (<quote>回収不可</quote> と呼ばれることもあります), <quote>回収可</quote>, <quote>スワップ可</quote> に分類することができます。</para>

  <sect2 xml:id="cha-tuning-memory-usage-anonymous">
   <title>匿名メモリ</title>
   <para>匿名メモリはプログラムのヒープやスタックメモリ (例: <literal>&gt;malloc()</literal>) として使用されるメモリで、 <literal>mlock</literal> のような特殊ケースや、利用可能なスワップ領域が存在しないような場合を除いて、回収可能なメモリとして位置づけられます。また、匿名メモリは回収される前にスワップに書き込まなければなりません。また、スワップの I/O (ページのスワップインおよびスワップアウト) は、割り当てとアクセスのパターンにより、ページキャッシュの I/O よりも効率が落ちる傾向があります。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-usage-pagecache">
   <title>ページキャッシュ</title>
   <para>ファイルデータのキャッシュです。ディスクやネットワークからファイルが読み込まれると、その内容がページキャッシュ内に保存されます。ページキャッシュ内の内容が最新のものであれば、ディスクやネットワークへのアクセスは不要となります。また、 tmpfs や共有メモリセグメントについても、ページキャッシュとしてカウントします。</para>
   <para>ファイルに対して書き込みが行われる場合、ディスクやネットワークに書き戻される (つまりライトバックキャッシュを作成する) 前に、ページキャッシュにも保存が行われます。まだディスクやネットワークに書き込まれていない新しいデータが存在する場合、そのページは <quote>dirty</quote> であるとされます。逆に dirty ではないページは、 <quote>clean</quote> であるとされます。 clean やページキャッシュのページは、メモリが枯渇した場合、単純に解放するだけで回収することができます。 dirty なページの場合は、回収できるようにするためにまず clean にしなければなりません。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-usage-buffercache">
   <title>バッファキャッシュ</title>
   <para>ブロックデバイス (例: /dev/sda) に対するページキャッシュの一種です。ファイルシステムでは、inode テーブルやアロケーションビットマップなど、ディスク内のメタデータにアクセスする際にバッファキャッシュを使用します。バッファキャッシュはページキャッシュと同様に回収することができます。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-usage-bufferheads">
   <title>バッファヘッド</title>
   <para>バッファヘッドは小さな補助構造で、一般的にページキャッシュへのアクセス時に割り当てられるものです。ページキャッシュやバッファキャッシュが clean であれば、これらも一般に容易に回収することができます。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-usage-writeback">
   <title>ライトバック</title>
   <para>アプリケーションがファイルに書き込みを行う場合、ページキャッシュが dirty となるほか、バッファキャッシュについても必要に応じて dirty となります。 dirty なメモリの量が指定したページ数 (バイト単位で <emphasis>vm.dirty_background_bytes</emphasis> で指定します) を越えるか、メモリの全体量との比率が指定した値 ( <emphasis>vm.dirty_background_ratio</emphasis> ) を越えるか、もしくは指定した時間 ( <emphasis>vm.dirty_expire_centisecs</emphasis> ) より長く dirty であり続けた場合、カーネルは最初に dirty になったページのファイルからページの書き戻しを始めます。なお、 bytes の設定と ratio のパラメータは相互に排他な仕組みであり、一方を変更すると他方を上書きすることになります。また、 flusher スレッドは裏で書き戻しを行う仕組みであるため、アプリケーションは通常通り動作し続けることができます。ただし、 I/O の速度よりもアプリケーションがページを dirty にする速度のほうが早い場合、 dirty なデータが致命的な水準 ( <emphasis>vm.dirty_bytes</emphasis> もしくは <emphasis>vm.dirty_ratio</emphasis> ) を越えると、アプリケーションの速度が抑制され、 dirty なデータが過剰に生み出されないようにします。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-usage-readahead">
   <title>先読み</title>
   <para>VM はファイルのアクセスパターンを監視し、必要に応じて先読みを行おうとします。先読みはその名前の通り、まだ要求されていない範囲のものをファイルシステムからページキャッシュにページを読み込むものです。これは、より少ない回数でより大きな I/O を送信できるようにする (これによって効率化を図る) ためのものです。また、 I/O をパイプライン化する (アプリケーションの実行と同時に I/O を実施する) ためのものでもあります。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-usage-vfs">
   <title>VFS キャッシュ</title>
   <para/>
   <sect3 xml:id="cha-tuning-memory-usage-vfs-inode">
    <title>inode キャッシュ</title>
    <para>これは各ファイルシステムに対する inode の構造をメモリ内にキャッシュしておくものです。この中にはファイルサイズやパーミッション、所有者情報やファイルデータに対するポインタなどを保持しています。</para>
   </sect3>
   <sect3 xml:id="cha-tuning-memory-usage-vfs-dir-entry">
    <title>ディレクトリエントリキャッシュ</title>
    <para>これはシステム内でのディレクトリエントリのメモリ内キャッシュです。この中には名前 (ファイル名) のほか、それが指し示す inode と、子エントリが含まれます。このキャッシュは、ディレクトリ構造をたどる場合と、名前でファイルにアクセスする場合に使用されるものです。</para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-tuning-memory-optimize">
  <title>メモリ使用量の削減</title>

  <para/>

  <sect2 xml:id="cha-tuning-memory-optimize-malloc">
   <title>malloc (匿名) 利用の削減</title>
   <para>&productname; &productnumber; で動作しているアプリケーションは、以前のバージョンに比べてより多くのメモリを割り当てることができます。これは <systemitem class="resource">glibc</systemitem> がユーザスペースメモリの割り当てに際して、既定の動作を変更したことによるものです。これらのパラメータについて、詳しくは <link xlink:href="http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html"/> (英語) をお読みください。</para>
   <para>以前のバージョンの動作に戻したい場合、 M_MMAP_THRESHOLD の値を 128*1024 に設定する必要があります。これはアプリケーション側から mallopt() を利用することによって実現できるほか、アプリケーションの起動前に MALLOC_MMAP_THRESHOLD 環境変数を設定しても実現することができます。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-optimize-overhead">
   <title>カーネルのメモリオーバーヘッドの削減</title>
   <para>回収可能なカーネルメモリ (上述のとおりキャッシュなど) については、メモリの枯渇時に自動的に解放が行われます。その他のカーネルメモリについては容易に縮小することができますが、カーネルに与えられた負荷の特性によって決まります。</para>
   <para>ユーザスペースの負荷要件を減らす (プロセスを減らす、ファイルやソケットを減らすなど) ことで、カーネルのメモリ使用量も削減することができます。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-optimize-cgoups">
   <title>メモリコントローラ (メモリ cgroup)</title>
   <para>メモリ cgroup の機能が不要である場合は、カーネルのコマンドラインに cgroup_disable=memory を追加することで、無効化を行うことができます。これにより、カーネルが使用するメモリを少しだけ削減することができます。また、メモリ cgroup が利用できるにもかかわらず設定していない場合、少しだけメモリのオーバーヘッドが存在するため、少しだけ性能改善をはかることもできます。</para>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-tuning-memory-vm">
  <title>仮想メモリマネージャ (VM) のチューニングパラメータ</title>

  <para>VM をチューニングする場合、パラメータが実際の処理に反映されるまでには、しばらく時間がかかるものがあることに注意してください。また、負荷が 1 日を通して変化するような場合、時間帯によっては異なる振る舞いになることもあります。それだけでなく、特定の環境でスループットを改善できるパラメータが、別の環境ではスループットを悪化させてしまうようなこともあります。</para>

  <sect2 xml:id="cha-tuning-memory-vm-reclaim">
   <title>回収比率に関するパラメータ</title>
   <variablelist>
    <varlistentry>
     <term><filename>/proc/sys/vm/swappiness</filename></term>
     <listitem>
      <para>この変数は、ページキャッシュやその他のキャッシュに比べて、カーネルがどれだけの比率で匿名メモリを積極的にスワップアウトさせるかを指定するものです。この値を増やすことで、スワップ量が増えることになります。既定値は <literal>60</literal> です。</para>
      <para>スワップの I/O は一般に、その他の I/O よりもずっと効率が落ちるものです。しかしながら、ページキャッシュのページによっては、あまり使用されない匿名メモリよりも、頻繁にアクセスされるものがあります。ここでは適切な比率を設定してください。</para>
      <para>速度が低下した際にスワップの動作が観測できた場合、このパラメータを減らしてみることをお勧めします。また、多くの I/O 動作が存在する環境で、システム内のページキャッシュの量が比較的少ない場合や、動作しているものの休眠中の巨大なアプリケーションが存在するような環境では、この値を増やすことで性能を改善できるかもしれません。</para>
      <para>ただし、データのスワップアウト量が増えれば増えるほど、必要とされた際にスワップアウトされたデータを取り戻すのに時間がかかることに注意してください。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/vfs_cache_pressure</filename></term>
     <listitem>
      <para>この変数は、ページキャッシュやスワップに比べて、カーネルが VFS キャッシュに使用しているメモリの回収傾向を制御するためのものです。この値を増やすと、 VFS キャッシュの回収比率を高めることができます。</para>
      <para>ただし、試してみる以外の方法では、適切な値を推測することは難しい値でもあります。 <command>slabtop</command> コマンド (<systemitem class="resource">procps</systemitem> パッケージ内に含まれています) を使用することで、カーネル側で使用しているメモリオブジェクトを多い順に並べることができます。このコマンド内で、 vfs キャッシュは "dentry" と "*_inode_cache" として表示されます。ページキャッシュに比べてこれらのメモリが巨大になっている場合、この圧力値を増やしてみることをお勧めします。これにより、スワップ処理を減らすことにもなります。既定値は <literal>100</literal> です。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/min_free_kbytes</filename></term>
     <listitem>
      <para>この変数は、 <quote>アトミックな</quote> (回収を待つことができない) 割り当てなど、特別に予約しておく必要のあるメモリ量を制御します。これは通常、メモリ使用率について注意深くチューニングしている場合を除いて、減らすべきではない値です (通常はサーバ用途ではなく、組み込み用途で設定すべき値です) 。もしもログ内に <quote>ページ割り当て失敗</quote> に関するメッセージとスタックトレースが頻繁に現れているような場合、 min_free_kbytes をエラーが出なくなる程度まで増やしてみることをお勧めします。このようなメッセージが頻繁に現れたりしていなければ、特に気にする必要はありません。既定値は搭載されている RAM の量に従って決められます。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/watermark_scale_factor</filename></term>
     <listitem>
      <para>大まかに言うと、空きメモリには高／低／最小の水準が設定されています。低い水準に達した場合、 <command>kswapd</command> が動作して裏でメモリの回収を行うようになります。この回収作業は、高いほうの水準に達するまで続けられます。最小の水準に達した場合、アプリケーションの動作は一時的に停止させられます。</para>
      <para><literal>watermark_scale_factor</literal> は、 kswapd が動き出す際のノードもしくはシステム内のメモリ量を表す値と、そこから kswapd が休眠状態に戻るまでにどれだけの量のメモリを解放するのかを表す値です。単位は 10,000 の対する比率で、既定値の 10 は 水準間の長さがノード／システム内で利用できるメモリの 0.1% 分であることを示します。最大値は 1000 で、 10% 分を表します。</para>
      <para>直接的な回収処理で処理速度が遅くなってしまっている場合、 <filename>/proc/vmstat</filename> 内の <literal>allocstall</literal> の値が増えますが、この場合はこのパラメータを変更することで、問題を回避できるかもしれません。同様に <command>kswapd</command> が早く休眠状態になってしまう場合、 <literal>kswapd_low_wmark_hit_quickly</literal> の値が増えますが、この場合はアプリケーションの一時停止を回避するために空いているページ数が、少なすぎることを表しています。</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-vm-writeback">
   <title>ライトバック (書き戻し) 関連のパラメータ</title>
   <para><!-- NOTE: wrong for openSUSE? &productname; 10 --> 以前の &productname; バージョンからのライトバック関連の主な変更点として、ファイルに結びつけられた mmap() メモリが、即時に dirty なメモリとして判断されるようになった (つまりライトバックの対象となる) ことがあげられます。以前のバージョンでは、 munmap() でマップが解除された場合や msync() システムコールが呼び出された場合、もしくはメモリへの圧力が大きい場合にのみ書き戻しが行われていました。</para>
   <para>アプリケーションによっては、このような書き戻し動作への変更を期待していないものもあり、場合によっては性能が低下してしまうことがあります。</para>
   <variablelist>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_background_ratio</filename></term>
     <listitem>
      <para>この変数は空き領域や回収可能なメモリの全体量の割合を表すものです。この割合以上に dirty なページキャッシュが存在していると、ライトバックスレッドが dirty なメモリを書き込み始めるようになります。既定値は <literal>10</literal> (%) です。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_background_bytes</filename></term>
     <listitem>
      <para>この変数は、裏で動作するカーネルのライトバックスレッドが、その書き込みを始める割合を表すものです。 <filename>dirty_background_bytes</filename> は <filename>dirty_background_ratio</filename> に対応するもので、一方を設定すると他方が自動的に <literal>0</literal> に設定されます。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_ratio</filename></term>
     <listitem>
      <para><filename>dirty_background_ratio</filename> に似た意味を持つ割合値です。ここで指定した割合を超過すると、ページキャッシュに対して書き込みを行いたいアプリケーションの動作が一時的に止められ、カーネルのライトバックスレッドが dirty なメモリを書き込んで clean に戻すまで待機するようになります。既定値は <literal>20</literal> (%) です。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_bytes</filename></term>
     <listitem>
      <para>この変数は <filename>dirty_ratio</filename> と同じようなチューニングパラメータですが、ここでは割合ではなくバイト単位でサイズを指定します。 <filename>dirty_ratio</filename> と <filename>dirty_bytes</filename> は同じチューニングパラメータであることから、一方を設定すると他方が自動的に <literal>0</literal> になります。 <filename>dirty_bytes</filename> に設定可能な最小値は 2 ページ分 (ただしバイト単位) で、それより小さい値を設定しようとしても、それは無視されて元の値に戻ってしまいます。</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_expire_centisecs</filename></term>
     <listitem>
      <para>この変数は、ここで設定した値よりも長い時間 dirty であり続けたメモリが存在した場合、次回のライトバックスレッドの起床時に書き込みが行われるようになるものです。ここでの期限設定はファイルの inode に設定された最終更新日時を基準にします。そのため、同じファイルに対して発生した複数の dirty ページが存在した場合、この期限を超過するとそれら全てが書き込まれるようになります。</para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para><filename>dirty_background_ratio</filename> と <filename>dirty_ratio</filename> は、いずれもページキャッシュのライトバック動作を設定するためのものです。これらの値を増やした場合、システム内に dirty なメモリがより多く、かつ長く保持されることになります。システム内により多くの dirty なメモリを保持できる環境であれば、ライトバックの I/O を減らして、より最適な I/O パターンでの書き込みを増やすために、これらの値を活用できる場合があります。ただし dirty なメモリが多く存在していると、メモリを回収する必要がある場合や、ディスクに書き込む必要が発生した場合に一貫性を確保するタイミング ( <quote>同期ポイント</quote> ) で、遅延が発生する場合があります。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-vm-writetiming" os="sles;sled">
   <title>&sle; 12 と &sle; 11 における I/O 書き込みのタイミングの違いについて</title>
   <para>システム側では、ディスクに書き込むべきファイルデータをどれだけ保持しておくのかを、システムのメモリサイズに応じて制限する必要があります。これにより、 I/O を完了させるのに必要なデータ構造を常に割り当てることができるようにしています。 dirty になりうるメモリの最大量と、その書き込みの必要性は、 <literal>vm.dirty_ratio</literal> ( <filename>/proc/sys/vm/dirty_ratio</filename> ) で制御することができます。既定値はそれぞれ下記のとおりです:</para>
<screen>SLE-11-SP3:     vm.dirty_ratio = 40
SLE-12:         vm.dirty_ratio = 20</screen>
   <para>&sle; 12 で低い割合が設定されていることの利点は、メモリが枯渇した場合のページ回収と割り当てをできる限り素早く処理し、古い dirty ページを素早く見つけて廃棄することができる点にあります。もう 1 つの利点としては、システム内の全てのデータは同期しておかなければならず、既定では &sle; 11 SP3 よりも &sle; 12 のほうがより早く終わることになる点です。ほとんどの処理では、アプリケーション側から <literal>fsync()</literal> が呼ばれたり、その制限に引っかかるほど素早く dirty になることがありませんので、このような変更が行われていることに気がつくことはありません。</para>
   <para>もしも例外的なアプリケーションをお使いの場合で、この変更による影響を受けてしまっているような場合、書き込み時に予期せずアプリケーションの動きが遅くなってしまうことがあります。この場合、 dirty なデータの速度が制限されていることを確認するため、 <literal>/proc/<replaceable>アプリケーションのプロセス_ID</replaceable>/stack</literal> を監視しておき、アプリケーションが <literal>balance_dirty_pages_ratelimited</literal> 内で時間を費やしていることを確認してください。このような現象に当てはまる場合で、かつそれが問題となっている場合、 <literal>vm.dirty_ratio</literal> を 40 に戻すことで、従来の &sle; 11 SP3 の動作に戻すことをお勧めします。</para>
   <important>
    <para>ただし、全体としての I/O スループットはこの設定とは無関係であることに注意してください。違いは I/O がキュー内に存在するタイミングだけです。</para>
   </important>
   <para>下記の例は <command>dd</command> コマンドを利用して、メモリの 30% 分をディスクに書き込むことで、 <literal>vm.dirty_ratio</literal> の変更による影響を受けられるようにした場合の例です:</para>
<screen>&prompt.root;MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`
&prompt.root;sysctl vm.dirty_ratio=40
&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s
&prompt.root;sysctl vm.dirty_ratio=20
dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s</screen>
   <para>なお、このパラメータは、コマンドの処理が完了するまでの時間と、デバイスへの見かけ上の書き込み速度に影響を与えていることに注意してください。 <literal>dirty_ratio=40</literal> では、多くのデータがキャッシュされ、カーネルによって裏でディスクへの書き込みが行われています。また、両方の実行例でデバイスの I/O 速度は同じであることにも注意してください。もう一度、確認の目的で <command>dd</command> コマンドを実行し、今回は終了前にデータ同期を行ってみます:</para>
<screen>&prompt.root;sysctl vm.dirty_ratio=40
&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s
&prompt.root;sysctl vm.dirty_ratio=20
&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s</screen>
   <para>上記のとおり、 <literal>dirty_ratio</literal> の設定変更による影響はほとんど発生しておらず、誤差の範囲内であることがわかります。このことから、 <literal>dirty_ratio</literal> の設定は直接的な I/O の性能に影響を与えることはありませんが、同期せずに書き込みを行っている場合、見かけ上の性能には影響があることに注意してください。</para>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-vm-readahead">
   <title>先読み関連のパラメータ</title>
   <variablelist>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>ブロックデバイス</replaceable>/queue/read_ahead_kb</filename></term>
     <listitem>
      <para>1 つもしくは複数のプロセスがファイルを順次読み込みしている場合、カーネルはそれらのデータを前もって読み込み (先読みし) 、プロセスがデータを待つ時間を減らすように処理を行います。先読みのデータ量は、どれだけ <emphasis>順に</emphasis> I/O を行っているのかに従って、動的に計算されます。このパラメータは、カーネルが先読みを行う際、 1 ファイルあたりの最大データ量を設定するためのものです。巨大なファイルの順次読み込みが十分に早いものであるとは思えない場合、この値を増やしてみることをお勧めします。ただし、大きくしすぎると、先読みスラッシングと呼ばれる現象が発生し、先読みで使用したページキャッシュが、使用される前に回収されてしまうことがあります。また、意味のない I/O が発生して速度が落ちてしまうこともあります。既定値は <literal>512</literal> [KB] です。</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cha-tuning-memory-vm-thp">
   <title>Transparent HugePage 関連のパラメータ</title>
    <para>Transparent HugePages (THP) は動的な huge page の割り当て方法で、プロセス側から要求によって割り当てたり、後から <command>khugepaged</command> カーネルスレッドを介して遅延割り当てを行ったりするためのものです。この方式は <literal>hugetlbfs</literal> の使用とは区別されていて、こちらは割り当てと使用を手作業で管理する方式です。連続したメモリのアクセスパターンが存在する負荷の場合、 THP によって大きく性能を改善できる可能性があります。連続したメモリのアクセスパターンで一括処理を行うと、ページフォルトを 1000 倍減少させることもできてしまいます。</para>
   <para>逆に THP が望ましくない場合もあります。たとえばメモリ内のあちこちをアクセスするようなパターンの場合、 メモリの使用量が増える結果になってしまうため、かえって性能が落ちてしまいます。たとえばフォルトごとに 4 KB ではなく、フォルト発生時点で 2 MB のメモリを使用してしまうことがあり、最終的にはページの回収を早める結果になってしまいます。<phrase os="sles;sled"> &sle; 12 SP2 よりも古いバージョンでは、 THP の割り当てに伴ってアプリケーション側が長い時間待機させられてしまうことがありましたので、これが THP を無効化する理由にもなっていました。このような推奨は <phrase os="sles;sled">&sle; 12 SP3</phrase> で改められています。 </phrase></para>
   <para>THP の動作は <option>transparent_hugepage=</option> のカーネルパラメータか、 sysfs 経由で設定することができます。たとえばカーネルのパラメータに <option>transparent_hugepage=never</option> を追加して grub2 の設定を再構築し、システムを再起動します。すると、下記のように入力して実行することで、 THP が無効化されていることを確認できます:</para>
    <screen>&prompt.root;cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</screen>
     <para>無効化されている場合、上記の例のように <literal>never</literal> が [] で囲まれて表示されます。 <literal>always</literal> に設定すると THP を常に使用するようになりますが、割り当てが失敗した場合は <command>khugepaged</command> に従います。 <literal>madvise</literal> に設定すると、アプリケーション側で明示的に指定されたアドレス空間に THP を割り当てるだけになります。</para>
     <variablelist>
      <varlistentry>
       <term><filename>/sys/kernel/mm/transparent_hugepage/defrag</filename></term>
       <listitem>
        <para>このパラメータは、 THP を割り当てる際にアプリケーションがどれだけの努力を行うのかを制御するものです。 <literal>always</literal> は <phrase os="sles;sled">&sle; 12 SP1 およびそれ以前</phrase> <phrase os="osuse">&opensuse; 42.1 およびそれ以前</phrase> の THP に対応するリリースでの既定値でした。もしも THP が利用できない場合、アプリケーションはメモリのデフラグを実施しようとします。つまり THP が利用できない場合、メモリがフラグメントされていると、アプリケーションの動作が潜在的に一時的ながら停止する可能性があることになります。</para>
        <para>値に <literal>madvise</literal> を設定すると、 THP の割り当てリクエストでは、アプリケーション側から明示的に要求された場合にのみ、デフラグを実施します。こちらが <phrase os="sles;sled">&sle; 12 SP2</phrase> <phrase os="osuse">&opensuse; 42.2</phrase> およびそれ以降のバージョンでの既定値となります。</para>
        <para><literal>defer</literal> は <phrase os="sles;sled">&sle; 12 SP2</phrase> <phrase os="osuse">&opensuse; 42.2</phrase> およびそれ以降で利用できるようになった値で、 THP が利用できない場合には小さなページを使用して処理するように動作します。これにより <command>kswapd</command> と <command>kcompactd</command> の各カーネルスレッドを起床させ、裏でデフラグを行うことで、後から <command>khugepaged</command> が THP を割り当てる動作になります。</para>
       <para>最後の値である <literal>never</literal> は、 THP が利用できない場合には単純に小さなページを利用して処理を行うもので、それ以外の処理は行わない意味になります。</para>
       </listitem>
      </varlistentry>
     </variablelist>
  </sect2>
  <sect2 xml:id="cha-tuning-memory-vm-khugepaged">
   <title>khugepaged のパラメータ</title>
   <para>khugepaged は <literal>transparent_hugepage</literal> の値が <literal>always</literal> もしくは <literal>madvise</literal> である場合に自動的に開始され、 <literal>never</literal> である場合には自動的に停止します。通常は低頻度で動作するものではありますが、動作をチューニングすることもできます。</para>
   <variablelist>
     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/defrag</filename></term>
      <listitem>
       <para>値に 0 を設定すると、フォルトの時点で THP が使用されていても <command>khugepaged</command> を無効化します。これは遅延に敏感なアプリケーションにとっては重要な設定で、 THP による利点を受けるものの、 <command>khugepaged</command> がアプリケーションのメモリ使用を更新しようとする際に、一時的に動作が止まってしまう事象を防ぐことができるからです。</para>
      </listitem>
     </varlistentry>

     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan</filename></term>
      <listitem>
      <para>この変数は、 <command>khugepaged</command> が 1 回の処理でスキャンするページ数を制御するためのものです。スキャン処理では小さいページを検出して、 THP で再割り当てができないかどうかを確認します。この値を増やすことで、 CPU の使用率が上がるものの、裏でより高速に THP を割り当てることができるようになります。</para>
      </listitem>
    </varlistentry>

     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs</filename></term>
      <listitem>
      <para>この変数は、 <command>khugepaged</command> が 1 回の処理を完了した後に待機する時間を設定するためのもので、 CPU の使用率が過剰に上がらないようにするためのものです。この値を小さくすると、 CPU の使用率が上がる代わりに、裏でより高速に THP を割り当てることができるようになります。</para>
      </listitem>
     </varlistentry>

     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs</filename></term>
      <listitem>
       <para>この変数は、 <command>khugepaged</command> が <command>kswapd</command> と <command>kcompactd</command> が動作するのを待っている間、裏で THP が割り当てに失敗した際に休眠する時間を指定するためのものです。</para>
      </listitem>
    </varlistentry>
   </variablelist>

   <para><command>khugepaged</command> に対するその他のパラメータは、性能チューニングにあたってはほとんど用途のないものではありますが、 <filename>/usr/src/linux/Documentation/vm/transhuge.txt</filename> ファイル (英語) で詳しく説明しています。</para>
  </sect2>
  <sect2 xml:id="cha-tuning-memory-vm-more">
   <title>その他の VM パラメータ</title>
   <para>VM に関連するチューニングパラメータに関する完全な一覧については、 <filename>/usr/src/linux/Documentation/sysctl/vm.txt</filename> ファイル (英語, <systemitem class="resource">kernel-source</systemitem> パッケージ内に含まれています) をお読みください。</para>
  </sect2>
 </sect1>
<!-- fs 2015-05-28
     No longer supported: http://bugzilla.suse.com/show_bug.cgi?id=874971

 <sect1 id="cha-tuning-memory-numa">
  <title>Non-uniform memory access (NUMA)</title>

  <para>
   Another increasingly important role of the VM is to provide good NUMA
   allocation strategies. NUMA stands for non-uniform memory access, and
   most of today's multi-socket servers are NUMA machines. NUMA is a
   secondary concern to managing swapping and caches in terms of
   performance, and there are lots of documents about improving NUMA memory
   allocations. One particular parameter interacts with page reclaim:
  </para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/vm/zone_reclaim_mode</filename>
    </term>
    <listitem>
     <para>
      This parameter controls whether memory reclaim is performed on a local
      NUMA node even if there is plenty of memory free on other nodes. This
      parameter is automatically turned on on machines with more pronounced
      NUMA characteristics.
     </para>
     <para>
      If the VM caches are not being allowed to fill all of memory on a NUMA
      machine, it could be because of zone_reclaim_mode being set. Setting to 0
      will disable this behavior.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
-->
 <sect1 xml:id="cha-tuning-memory-monitoring">
  <title>VM の動作監視</title>

  <para>VM の動作の監視には、下記のようなシンプルなツールを使用することができます:</para>

  <orderedlist spacing="normal">
   <listitem>
    <para>vmstat: このツールは、 VM が現在何をしているのかをわかりやすく表示することができます。詳しくは <xref linkend="sec-util-multi-vmstat"/> をお読みください。</para>
   </listitem>
   <listitem>
    <para><filename>/proc/meminfo</filename> : このファイルは、メモリの使途を分解して示しているファイルです。詳しくは <xref linkend="sec-util-memory-meminfo"/> をお読みください。</para>
   </listitem>
   <listitem>
    <para><command>slabtop</command> : このツールは、カーネルの slab メモリについて、その使用状況を詳細に表示することができます。 buffer_head, dentry, inode_cache, ext3_inode_cache などの値が主なキャッシュです。このコマンドは <systemitem class="resource">procps</systemitem> パッケージ内に含まれています。</para>
   </listitem>
   <listitem>
    <para><filename>/proc/vmstat</filename> : このファイルには VM の内部動作を詳しく分解して表示したものが含まれています。含まれている情報は実装に依存して決まるものであり、環境によって表示される内容が異なります。いくつかの項目は <filename>/proc/meminfo</filename> でも表示されていますが、それ以外の項目はユーティリティを使用してわかりやすく表示するのがよいでしょう。また、情報を最大限に活用するため、このファイルは変化率を観察するために長時間監視しておくことをお勧めします。他の情報源からは導き出すのが難しい主な情報は下記のとおりです:</para>
    <variablelist>
     <varlistentry>
      <term><literal>pgscan_kswapd_*, pgsteal_kswapd_*</literal></term>
      <listitem>
       <para>これらのレポートには、システムが起動してからスキャンしたページ数の合計と、 <command>kswapd</command> が回収したページ数の合計が書かれています。これらの値の比率は回収効率として解釈することができ、この値が低い場合は、システムがメモリを回収するのに苦労していて、おそらく使用する前に回収されてしまっていることを表しています。処理が軽い場合は、あまり気にする必要はありません。</para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>pgscan_direct_*, pgsteal_direct_*</literal></term>
      <listitem>
        <para>これらのレポートには、アプリケーションが直接スキャンしたページ数の合計と、回収したページ数の合計が書かれています。これは <literal>allocstall</literal> のカウンタ値と相関しています。これらのイベントが多く発生している場合、プロセスの動作が一時的に止まっていることを示すことから、 <command>kswapd</command> での処理より深刻な問題となります。 <command>kswapd</command> での処理が重く、 <literal>pgpgin</literal> , <literal>pgpout</literal> の値が高いか、もしくは <literal>pswapin</literal> や <literal>pswpout</literal> の値が高い場合は、システムが過剰にスラッシングしている (実際に使用される前に回収されてしまっている) ことを表しています。</para>
        <para>より詳しい情報を得たい場合は、トレースポイントを使用してください。</para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>thp_fault_alloc, thp_fault_fallback</literal></term>
      <listitem>
       <para>これらのカウンタは THP がアプリケーションから直接割り当てられた回数と、 THP が利用できずに小さいページを使用した回数を表しています。 thp_fault_fallback の値が大きい場合でも、アプリケーションが TLB の圧力に敏感でない限り、有害ではありません。</para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>thp_collapse_alloc, thp_collapse_alloc_failed</literal></term>
      <listitem>
       <para>これらのカウンタは、 <command>khugepaged</command> が割り当てた THP の回数と、 THP が利用できずに小さなページを使用した回数を示しています。 <!-- NOTE: not fallback, but failed? --> failed の割合が高い場合、システムがフラグメント状態にあり、アプリケーション側が許可したメモリ使用量であるにもかかわらず、 THP が使用されていないことを表しています。アプリケーションが TLB の圧力に敏感である場合にのみ、問題となる項目です。</para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>compact_*_scanned, compact_stall, compact_fail, compact_success</literal></term> <listitem>
       <para>これらのカウンタは THP が有効化され、システムがフラグメントしている場合に増えるものです。 <literal>compact_stall</literal> は THP の割り当てに際してアプリケーションの動作が一時的に停止した場合に増加します。それ以外のカウンタは、スキャンしたページの数と、成功もしくは失敗したデフラグイベントの数を表しています。</para>
      </listitem>
     </varlistentry>
    </variablelist>
   </listitem>
  </orderedlist>
 </sect1>
</chapter>
