<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-libvirt-host">
 <title>Preparing the &vmhost;</title>
 <para>
  Before you can install guest virtual machines, you need to prepare the
  &vmhost; to provide the guests with the resources that they need for their
  operation. Specifically, you need to configure:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>Networking</emphasis> so that guests can make use of the network
    connection provided the host.
   </para>
  </listitem>
  <listitem>
   <para>
    A <emphasis>storage pool</emphasis> reachable from the host so that the
    guests can store their disk images.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="libvirt-host-network">
  <title>Configuring Networks</title>

  <para>&bmguest; に対してネットワーク機能を提供するには、下記の 2 種類の方法があります:</para>

  <itemizedlist>
   <listitem>
    <para>
     A <emphasis>network bridge</emphasis>. This is the default and recommended
     way of providing the guests with network connection.
    </para>
   </listitem>
   <listitem>
    <para>
     A <emphasis>virtual network</emphasis> with forwarding enabled.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="libvirt-networks-bridged">
   <title>ネットワークブリッジ</title>
   <para>ネットワークブリッジは、 &vmguest; に対してレイヤ 2 スイッチの機能を提供します。レイヤ 2 スイッチは、ポート間のパケット転送の際にレイヤ 2 イーサネットパケットを利用し、 MAC アドレスをベースにして宛先を判断します。これにより、 &vmhost; から &vmguest; に対して、レイヤ 2 アクセスのネットワークを提供することになります。これは、 &vmguest; の仮想的なイーサネットケーブルがイーサネットハブに接続され、そこからホスト自身やホスト内で動作する他の &vmguest; に接続できる形態と同じになります。この設定は、 <emphasis>共有物理デバイス</emphasis> と称することもあります。</para>
   <para>ネットワークブリッジは、 &productname; を &kvm; または &xen; のハイパーバイザとして設定した際の既定の設定になっています。これは &vmguest; と &vmhost; の LAN を単純に接続できるため、お勧めの方法でもあります。</para>
   <sect3 xml:id="libvirt-networks-bridged-yast">
    <title>&yast; を利用したネットワークブリッジの管理</title>
    <para>本章では、 &yast; を利用してネットワークブリッジを追加または削除する方法を説明しています。</para>
    <sect4 xml:id="libvirt-networks-bridged-yast-add">
     <title>ネットワークブリッジの追加</title>
     <para>&vmhost; にネットワークブリッジを追加するには、下記の手順を行ないます:</para>
     <procedure>
      <step>
       <para><menuchoice><guimenu>&yast;</guimenu><guimenu>システム</guimenu><guimenu>ネットワークの設定</guimenu></menuchoice> を起動します。</para>
      </step>
      <step>
       <para><guimenu>概要</guimenu> タブに移動して <guimenu>追加</guimenu> を押します。</para>
      </step>
      <step>
       <para><guimenu>デバイスの種類</guimenu> では <guimenu>ブリッジ</guimenu> を選択し、 <guimenu>設定名</guimenu> の項目にはブリッジデバイスのインターフェイス名を入力します。あとは <guimenu>次へ</guimenu> を押します。</para>
      </step>
      <step>
       <para><guimenu>アドレス</guimenu> のタブでは、 DHCP 経由でアドレスを取得するか、もしくは固定で IP アドレスを設定するかを選択し、必要であれば IP アドレスやサブネットマスク、ホスト名などをそれぞれ入力します。</para>
       <para><guimenu>可変 IP アドレス</guimenu> は、ブリッジが DHCP サーバに接続されているような場合のみ有用です。</para>
       <para>
        If you intend to create a virtual bridge that has no connection to a
        real network device, use <guimenu>Statically assigned IP
        Address</guimenu>. In this case, it is a good idea to use addresses
        from the private IP address ranges, for example,
        <literal>192.168.0.0/16</literal>, <literal>172.16.0.0/12</literal>, or
        <literal>10.0.0.0/8</literal>.
       </para>
       <para>ホストシステムとは接続しないゲスト間のみのネットワーク接続を作成する場合は、 IP アドレスを <literal>0.0.0.0</literal> 、サブネットマスクを <literal>255.255.255.255</literal> に設定します。これにより、 IP アドレスを設定しない特殊なネットワークを構成することができます。</para>
      </step>
      <step>
       <para><guimenu>ブリッジ接続デバイス</guimenu> のタブでは、ネットワークブリッジに含めたいネットワークデバイスを選択します。</para>
      </step>
      <step>
       <para><guimenu>次へ</guimenu> を押すと <guimenu>概要</guimenu> タブに戻りますので、設定内容を確認して <guimenu>OK</guimenu> を押します。これで &vmhost; 内にネットワークブリッジが作成され、有効化されます。</para>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="libvirt-networks-bridged-yast-rm">
     <title>ネットワークブリッジの削除</title>
     <para>既存のネットワークブリッジを削除するには、下記の手順を行ないます:</para>
     <procedure>
      <step>
       <para><menuchoice><guimenu>&yast;</guimenu><guimenu>システム</guimenu><guimenu>ネットワークの設定</guimenu></menuchoice> を起動します。</para>
      </step>
      <step>
       <para><guimenu>概要</guimenu> タブで削除したいブリッジデバイスを選択します。</para>
      </step>
      <step>
       <para><guimenu>Delete</guimenu> を押してブリッジを削除し、 <guimenu>OK</guimenu> を押します。</para>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="libvirt-networks-bridged-add-brctl">
    <title>コマンドラインを利用したネットワークブリッジの管理</title>
    <para>本章では、コマンドラインを利用してネットワークブリッジを追加もしくは削除する方法を説明しています。</para>
    <sect4 xml:id="libvirt-networks-bridged-add-brctl-add">
     <title>ネットワークブリッジの追加</title>
     <para>&vmhost; 内にネットワークブリッジを追加するには、下記の手順を行ないます:</para>
     <procedure>
      <step>
       <para>ネットワークブリッジを作成したい &vmhost; にログインし、 &rootuser; になります。</para>
      </step>
      <step>
       <para>まずは新しいブリッジの名前を選択します。下記の例では、 <replaceable>virbr_test</replaceable> という名前で作成するものとします。</para>
<screen>&prompt.root;ip link add name <replaceable>virbr_test</replaceable> type bridge</screen>
      </step>
      <step>
       <para>&vmhost; でブリッジが作成されたことを確認します:</para>
<screen>&prompt.root;bridge vlan
[...]
virbr_test  1 PVID Egress Untagged
</screen>
       <para><literal>virbr_test</literal> という名前が現われていますが、この時点ではどの物理ネットワークインターフェイスにも結びつけられていません。</para>
      </step>
      <step>
       <para>ネットワークブリッジを起動して、ブリッジにネットワークインターフェイスを追加します:</para>
<screen>
&prompt.root;ip link set virbr_test up
&prompt.root;ip link set eth1 master virbr_test
</screen>
       <important>
        <title>ネットワークインターフェイスを使用していてはならない件について</title>
        <para>ここで指定できるネットワークインターフェイスは、他のネットワークブリッジで使用されているものであってはなりません。</para>
       </important>
      </step>
      <step>
       <para>なお、必要であれば STP (<link xlink:href="https://ja.wikipedia.org/wiki/%E3%82%B9%E3%83%91%E3%83%8B%E3%83%B3%E3%82%B0%E3%83%84%E3%83%AA%E3%83%BC%E3%83%97%E3%83%AD%E3%83%88%E3%82%B3%E3%83%AB">スパニングツリープロトコル</link>) を有効化することもできます:</para>
<screen>&prompt.root;bridge link set dev virbr_test cost 4</screen>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="libvirt-networks-bridged-add-brctl-del">
     <title>ネットワークブリッジの削除</title>
     <para>コマンドラインを利用して &vmhost; のネットワークブリッジを削除するには、下記の手順を行ないます:</para>
     <procedure>
      <step>
       <para>ネットワークブリッジを削除したい &vmhost; にログインし、 &rootuser; になります。</para>
      </step>
      <step>
       <para>まずは既存のネットワークブリッジの一覧を表示させて、名前を確認します:</para>
<screen>&prompt.root;bridge vlan
[...]
virbr_test  1 PVID Egress Untagged
</screen>
      </step>
      <step>
       <para>ブリッジを削除します:</para>
<screen>&prompt.root;ip link delete dev virbr_test</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="sec-xen-net-vlan">
    <title>VLAN インターフェイスの使用</title>
    <para>状況によっては、 2 台の &vmhost; 間や &vmguest; 間で、プライベートな接続が必要となる場合があります。たとえば &vmguest; を異なるネットワークセグメント内にあるホストに移行させる必要がある場合や、特定の &vmguest; のみが接続できるプライベートブリッジが必要となる場合 (異なる &vmhost; 内にあってもかまいません) などがそれにあたります。このような状況下では、 VLAN ネットワークを作成するのが最も簡単です。</para>
    <para>VLAN インターフェイスは一般に &vmhost; 内で設定を行ないます。この VLAN インターフェイスは異なる &vmhost; システム間の相互接続として使用できるほか、 &vmguest; のみが接続されたブリッジへの物理インターフェイスとしても設定することができます。このほか、 &vmhost; では IP アドレスを設定せずに VLAN を物理インターフェイスに接続することもできてしまいます。これにより、 &vmguest; から &vmhost; に接続できないように設定できることになります。</para>
    <para>まずは &yast; を起動して <menuchoice><guimenu>システム</guimenu><guimenu>ネットワークの設定</guimenu></menuchoice> を選択します。あとは下記の手順で VLAN インターフェイスを設定します:</para>
    <procedure>
     <title>&yast; を利用した VLAN インターフェイスの設定</title>
     <step>
      <para><guimenu>追加</guimenu> を押して新しいネットワークインターフェイスを作成します。</para>
     </step>
     <step>
      <para><guimenu>インターフェイスの追加設定</guimenu><!-- NOTE: is this exist?, changed anyway --> 内の <guimenu>デバイスの種類</guimenu> では、 <guimenu>VLAN</guimenu> を選択します。</para>
     </step>
     <step>
      <para><guimenu>VLAN ID</guimenu><!-- NOTE: is this wrong?, changed anyway --> では VLAN の ID を指定します。なお、 VLAN ID <literal>1</literal> は通常、管理用に使用します。</para>
     </step>
     <step>
      <para><guimenu>次へ</guimenu> を押します。</para>
     </step>
     <step>
      <para><guimenu>VLAN の実インターフェイス</guimenu> では、 VLAN デバイスの接続先となるインターフェイスを選択します。一覧に必要なインターフェイスが表示されていない場合は、いったんキャンセルしてから、表示させたいインターフェイスを設定してください (このとき、 IP アドレスは指定しなくてかまいません) 。</para>
     </step>
     <step>
      <para>さらに VLAN デバイスへの IP アドレス設定方法を選択します。</para>
     </step>
     <step>
      <para><guimenu>次へ</guimenu> を押して設定を完了してください。</para>
     </step>
    </procedure>
    <para>VLAN インターフェイスをブリッジの物理インターフェイスとして使用することもできます。これにより、特定の &vmhost; と &vmguest; のみを接続することができるようになりますので、そのネットワークを介して &bvmguest; を移行できるようになります。</para>
    <para>場合によっては、 &yast; で IP アドレスの設定を無くすことができない場合があります。たとえば &vmguest; <!-- NOTE: is this wrong? changed anyway -->のみを接続するようなネットワークがそれにあたります。このような場合は、 IP アドレスに <literal>0.0.0.0</literal> を、サブネットマスクに <literal>255.255.255.255</literal> をそれぞれ指定してください。これにより IP アドレス無しでの設定ができるようになります。</para>
   </sect3>
  </sect2>

  <sect2 xml:id="libvirt-networks-virtual">
   <title>仮想ネットワーク</title>
   <para>&libvirt; が管理する仮想ネットワークはブリッジ型のネットワークと似ていますが、一般的には &vmhost; とのレイヤ 2 接続は行ないません。 &vmhost; の物理ネットワークへの接続は、レイヤ 3 転送を利用して実現します。これは &vmhost; 側でパケット転送の機能を提供するもので、レイヤ 2 ブリッジ型ネットワークとは異なる方式になります。この種類の仮想ネットワークでは &vmguest; に対して DHCP や DNS のサービスを提供することもできます。 &libvirt; の仮想ネットワーク機能に関する詳細は、 <link xlink:href="https://libvirt.org/formatnetwork.html"/> にある <citetitle>Network XML format</citetitle> の章をお読みください。</para>
   <para>&productname; で標準的な方法で &libvirt; のインストールを行なうと、 <literal>default</literal> という名前の仮想ネットワークが作成され、 DHCP と DNS の機能がそれぞれ提供されるようになります。また、この仮想ネットワークにはアドレス変換 (NAT) 機能が提供され、 &vmhost; の物理ネットワークに接続できるようになります。これはあらかじめ設定済みの形で提供されますが、管理者側で有効化する必要があります。 &libvirt; でサポートされている転送モードの詳細について、詳しくは <link xlink:href="https://libvirt.org/formatnetwork.html#elementsConnect"/> にある <citetitle>Network XML format</citetitle> ドキュメンテーションをお読みください。</para>
   <para>&libvirt; が管理する仮想ネットワークは幅広い用途に対応していて、必要な機能のほとんどに対応していますが、ラップトップなどの無線接続や動的な (間欠的な) 接続の場合には不十分です。また仮想ネットワークは、仮想ネットワークと &vmhost; のネットワークとの間でパケット転送を行なうことから、 &vmhost; の接続しているネットワーク内で IP アドレス数が不足しているような環境に有用でもあります。しかしながら、サーバ用途の場合は、 &vmguest; を &vmhost; の LAN に接続するネットワークブリッジ型の構成のほうが便利ではあります。</para>
   <warning>
    <title>転送モードの有効化について</title>
    <para>&libvirt; の仮想ネットワークに対して転送モードを有効化するには、まず <filename>/proc/sys/net/ipv4/ip_forward</filename> と <filename>/proc/sys/net/ipv6/conf/all/forwarding</filename> をそれぞれ 1 に設定する必要があります。これにより、 &vmhost; をルータとして動作させることができるようになります。なお、 &vmhost; のネットワークを再起動してしまうと、これらの値はリセットされ転送機能が無効化されてしまいます。再起動後も転送モードを有効化したい場合は、 &vmhost; 内の <filename>/etc/sysctl.conf</filename> ファイルを編集して、下記の内容を追記してください:</para>
<screen>net.ipv4.ip_forward = 1</screen>
<screen>net.ipv6.conf.all.forwarding = 1</screen>
   </warning>
   <sect3 xml:id="libvirt-networks-virtual-vmm">
    <title>&vmm; を利用した仮想ネットワークの管理</title>
    <para>&vmm; を利用することで、仮想ネットワークの作成や設定、操作などを行なうことができます。</para>
    <sect4 xml:id="libvirt-networks-virtual-vmm-define">
     <title>仮想ネットワークの作成</title>
     <procedure>
      <step>
       <para>まずは &vmm; を起動します。利用可能な接続が表示されますので、設定したい仮想ネットワークの名前を選択して <guimenu>詳細</guimenu> を押します。</para>
      </step>
      <step>
       <para><guimenu>接続の詳細</guimenu> ウインドウで <guimenu>仮想ネットワーク</guimenu> タブを選択します。ここには現在の接続で利用可能な全ての仮想ネットワークが表示されるほか、右側には選択した仮想ネットワークの詳細が表示されます。</para>
       <figure>
        <title>接続の詳細</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="libvirt_vmm_conndetails.png" width="45%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="libvirt_vmm_conndetails.png" width="45%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para>新しい仮想ネットワークを追加するには、 <guimenu>追加</guimenu> を押します。</para>
      </step>
      <step>
       <para>新しい仮想ネットワークの名前を指定して <guimenu>ルーティング</guimenu> を選択します。</para>
       <figure>
        <title>仮想ネットワークの作成</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="libvirt_vmm_vnet_name.png" width="45%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="libvirt_vmm_vnet_name.png" width="45%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para>IPv4 アドレスの範囲を指定したい場合は、対応するオプションを選択して <guimenu>ネットワーク</guimenu> 以下のテキストボックスに値を入力します。</para>
       <figure>
        <title>仮想ネットワークの作成</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="libvirt_vmm_vnet_ipv4.png" width="45%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="libvirt_vmm_vnet_ipv4.png" width="45%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para>
        &libvirt; can provide your virtual network with a DHCP server. If you
        need it, activate <guimenu>Enable DHCPv4</guimenu>, then type the start
        and end IP addresses of a range to be made assignable.
       </para>
      </step>
      <step>
       <para>新しい仮想ネットワークに対してスタティック (固定の) ルーティングを設定したい場合は、対応するオプションを選択してネットワークとゲートウエイアドレスを入力します。</para>
      </step>
      <step>
       <para><guimenu>完了</guimenu> を押して進めます。</para>
      </step>
      <step>
       <para>IPv6 関連のオプション (アドレス範囲や DHCPv6 サーバ、スタティック (固定の) ルーティングなど) を指定したい場合は、 <guimenu>Enable IPv6</guimenu><!-- NOTE: different? --> を選択して対応する項目に入力します。</para>
      </step>
      <step>
       <para><guimenu>完了</guimenu> を押して進めます。</para>
      </step>
      <step>
       <para>あとは孤立したネットワークを設定するか、転送型のネットワークを設定するかを選択します。</para>
       <figure>
        <title>仮想ネットワークの作成</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="libvirt_vmm_vnet_type.png" width="45%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="libvirt_vmm_vnet_type.png" width="45%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
       <para>転送型のネットワークの場合は、パケットの転送先となるネットワークインターフェイスを指定します。このとき、下記のいずれかの転送モードを選択することができます: <guimenu>NAT</guimenu> (Network Address Translation; ネットワークアドレス変換) は仮想的なネットワークアドレス領域を特定の IP アドレスに割り当てるための方式で、 1 つの IP アドレスを複数のゲストで共有することができるようになります。 <guimenu>ルーティング</guimenu> は仮想ネットワーク側からのパケットを、そのまま &vmhost; の物理ネットワークに転送する方式です。</para>
      </step>
      <step>
       <para>IPv6 アドレス領域を指定していない場合は、ここで仮想マシン間内部の IPv6 の有効化を設定することができます。</para>
      </step>
      <step performance="optional">
       <para>また、必要であれば DNS ドメイン名を設定することもできます。</para>
      </step>
      <step>
       <para><guimenu>完了</guimenu> を押すと新しい仮想ネットワークを作成することができます。 &vmhost; 側では新しい仮想ネットワークブリッジである <literal>virbr<replaceable>X</replaceable></literal> が作成され、これが新しい仮想ネットワークになります。これは <command>bridge link</command> コマンドで確認することができます。 &libvirt; では自動的に iptables のルールを追加して、新しく作成した <emphasis>virbr<replaceable>X</replaceable></emphasis> でのゲスト間の通信可否を設定します。</para>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-vmm-start">
     <title>仮想ネットワークの起動</title>
     <para>停止済みの仮想ネットワークを起動するには、下記の手順で行ないます:</para>
     <procedure>
      <step>
       <para>まずは &vmm; を起動します。利用可能な接続が表示されますので、設定したい仮想ネットワークの名前を選択して <guimenu>詳細</guimenu> を押します。</para>
      </step>
      <step>
       <para><guimenu>接続の詳細</guimenu> ウインドウ内では <guimenu>仮想ネットワーク</guimenu> タブを選択します。ここには現在の接続で利用可能な、全ての仮想ネットワークが一覧表示されます。</para>
      </step>
      <step>
       <para>仮想ネットワークを起動するには、 <guimenu>開始</guimenu> を押します。</para>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-vmm-stop">
     <title>仮想ネットワークの停止</title>
     <para>起動済みの仮想ネットワークを停止するには、下記の手順で行ないます:</para>
     <procedure>
      <step>
       <para>まずは &vmm; を起動します。利用可能な接続が表示されますので、設定したい仮想ネットワークの名前を選択して <guimenu>詳細</guimenu> を押します。</para>
      </step>
      <step>
       <para><guimenu>接続の詳細</guimenu> ウインドウ内では <guimenu>仮想ネットワーク</guimenu> タブを選択します。ここには現在の接続で利用可能な、全ての仮想ネットワークが一覧表示されます。</para>
      </step>
      <step>
       <para>停止させたい仮想ネットワークを選択して、 <guimenu>停止</guimenu> を押します。</para>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-vmm-delete">
     <title>仮想ネットワークの削除</title>
     <para>&vmhost; から仮想ネットワークを削除するには、下記の手順で行ないます:</para>
     <procedure>
      <step>
       <para>まずは &vmm; を起動します。利用可能な接続が表示されますので、設定したい仮想ネットワークの名前を選択して <guimenu>詳細</guimenu> を押します。</para>
      </step>
      <step>
       <para><guimenu>接続の詳細</guimenu> ウインドウ内では <guimenu>仮想ネットワーク</guimenu> タブを選択します。ここには現在の接続で利用可能な、全ての仮想ネットワークが一覧表示されます。</para>
      </step>
      <step>
       <para>削除したい仮想ネットワークを選択して <guimenu>削除</guimenu> を押します。</para>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-vmm-nsswitch">
<!-- fate#316628 -->
     <title>NAT ネットワークでの <command>nsswitch</command> を利用した IP アドレスの取得 (KVM)</title>
     <itemizedlist>
      <listitem>
       <para>&vmhost; 側に libvirt-nss をインストールします。これは libvirt 向けに NSS サポートを提供するものです:</para>
<screen>&prompt.sudo;zypper in libvirt-nss</screen>
      </listitem>
      <listitem>
       <para><filename>/etc/nsswitch.conf</filename> に <literal>libvirt</literal> を追記します:</para>
<screen>...
hosts:  files libvirt mdns_minimal [NOTFOUND=return] dns
...</screen>
      </listitem>
      <listitem>
       <para>NSCD を起動している場合は、再起動します:</para>
<screen>&prompt.sudo;systemctl restart nscd</screen>
      </listitem>
     </itemizedlist>
     <para>これでホスト側からゲストを名前で接続できるようになります。</para>
     <para>なお、 NSS モジュールの機能は完全ではありません。 <command>dnsmasq</command> が提供する <filename>/var/lib/libvirt/dnsmasq/*.status</filename> ファイルを読み込んで、 JSON レコード形式で記述されているホスト名と IP アドレスを検出します。ホスト名の変換は、 <command>dnsmasq</command> の動作している libvirt 管理下のブリッジ型ネットワークの &vmhost; でのみ動作します。</para>
     <para>詳しくは <link xlink:href="http://wiki.libvirt.org/page/NSS_module"/> をお読みください。</para>
    </sect4>
   </sect3>
   <sect3 xml:id="libvirt-networks-virtual-virsh">
    <title><command>virsh</command> を利用した仮想ネットワークの管理</title>
    <para>&libvirt; が提供する仮想ネットワークは、 <command>virsh</command> コマンドラインツールで管理することができます。 <command>virsh</command> で利用可能なネットワーク関連のコマンドを一覧表示するには、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh help network
Networking (help keyword 'network'):
    net-autostart                  ネットワークの自動起動
    net-create                     XML ファイルによるネットワークの作成
    net-define                     define (but don't start) a network from an XML file
    net-destroy                    ネットワークの強制停止
    net-dumpxml                    XML 形式のネットワーク情報
    net-edit                       ネットワークの XML 設定の編集
    net-event                      Network Events
    net-info                       ネットワーク情報
    net-list                       ネットワークの一覧表示
    net-name                       ネットワーク UUID からネットワーク名への変換
    net-start                      停止状態の（定義済み）ネットワークの起動
    net-undefine                   undefine an inactive network
    net-update                     既存のネットワーク設定の一部分の更新
    net-uuid                       ネットワーク名からネットワーク UUID への変換
    net-port-list                  list network ports
    net-port-create                create a network port from an XML file
    net-port-dumpxml               network port information in XML
    net-port-delete                delete the specified network port</screen>
    <para><command>virsh</command> の特定のコマンドに対するヘルプを表示したい場合は、 <command>virsh help <replaceable>コマンド</replaceable></command> のように入力して実行してください。</para>
<screen>&prompt.sudo;virsh help net-create
  名前
    net-create - XML ファイルによるネットワークの作成

  形式
    net-create &lt;file&gt;

  詳細
    ネットワークを作成します。

  オプション
    [--file] &lt;string&gt;  ネットワーク の XML 記述を含むファイル</screen>
    <sect4 xml:id="libvirt-networks-virtual-virsh-create">
     <title>ネットワークの作成</title>
<!-- 2016-02-25 tbazant
     http://wiki.libvirt.org/page/VirtualNetworking#Creating_a_virtual_network
     http://libvirt.org/formatnetwork.html
     http://libvirt.org/sources/virshcmdref/html/sect-net-create.html
     http://libvirt.org/sources/virshcmdref/html/
     -->
     <para>新しい <emphasis>動作中</emphasis> の仮想ネットワークを作成するには、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-create <replaceable>定義ファイル名.xml</replaceable></screen>
     <para>ここで、 <replaceable>定義ファイル名.xml</replaceable> には、 &libvirt; が受け入れ可能な XML 形式で記述された、仮想ネットワークの設定ファイルを指定します。</para>
     <para>有効化せずに新しい仮想ネットワークを追加したい場合は、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-define <replaceable>定義ファイル名.xml</replaceable></screen>
     <para>下記には、様々な種類の仮想ネットワークの設定例を示しています。</para>
     <example xml:id="ex-libvirt-net-nat">
      <title>NAT-Based Network</title>
      <para>
       The following configuration allows &vmguest;s outgoing connectivity if
       it is available on the &vmhost;. In the absence of &vmhost; networking, it
       allows guests to talk directly to each other.
      </para>
<screen>
&lt;network&gt;
&lt;name&gt;vnet_nated&lt;/name&gt;<co xml:id="vnet-xml-name"/>
&lt;bridge name="virbr1"/&gt;<co xml:id="vnet-xml-bridge"/>
 &lt;forward mode="nat"/&gt;<co xml:id="vnet-xml-forward"/>
 &lt;ip address="192.168.122.1" netmask="255.255.255.0"&gt;<co xml:id="vnet-xml-ip"/>
  &lt;dhcp&gt;
   &lt;range start="192.168.122.2" end="192.168.122.254"/&gt;<co xml:id="vnet-xml-dhcp"/>
   &lt;host mac="52:54:00:c7:92:da" name="host1.testing.com" \
    ip="192.168.1.23.101"/&gt;<co xml:id="vnet-xml-dhcp-host"/>
   &lt;host mac="52:54:00:c7:92:db" name="host2.testing.com" \
    ip="192.168.1.23.102"/&gt;
   &lt;host mac="52:54:00:c7:92:dc" name="host3.testing.com" \
    ip="192.168.1.23.103"/&gt;
  &lt;/dhcp&gt;
 &lt;/ip&gt;
&lt;/network&gt;
</screen>
      <calloutlist>
       <callout arearefs="vnet-xml-name">
        <para>新しい仮想ネットワークの名前です。</para>
       </callout>
       <callout arearefs="vnet-xml-bridge">
        <para>
         The name of the bridge device used to construct the virtual network.
         When defining a new network with a &lt;forward&gt; mode of <literal>"nat"</literal> or
         <literal>"route"</literal> (or an isolated network with no &lt;forward&gt; element),
         &libvirt; will automatically generate a unique name for the bridge
         device if none is given.
        </para>
       </callout>
       <callout arearefs="vnet-xml-forward">
        <para>
         Inclusion of the &lt;forward&gt; element indicates that the virtual
         network will be connected to the physical LAN. The
         <literal>mode</literal> attribute specifies the forwarding method. The
         most common modes are <literal>"nat"</literal> (Network Address Translation, the default),
         <literal>"route"</literal> (direct forwarding to the physical network, no address
         translation), and <literal>"bridge"</literal> (network bridge configured outside of
         &libvirt;). If the &lt;forward&gt; element is not specified, the
         virtual network will be isolated from other networks. For a complete
         list of forwarding modes, see
         <link xlink:href="http://libvirt.org/formatnetwork.html#elementsConnect"/>.
        </para>
       </callout>
       <callout arearefs="vnet-xml-ip">
        <para>ネットワークブリッジに設定する IP アドレスとネットマスクです。</para>
       </callout>
       <callout arearefs="vnet-xml-dhcp">
        <para>
         Enable DHCP server for the virtual network, offering IP addresses
         ranging from the specified <literal>start</literal> and
         <literal>end</literal> attributes.
        </para>
       </callout>
       <callout arearefs="vnet-xml-dhcp-host">
        <para>指定は任意ですが、 &lt;host&gt; 要素を設定することで、内蔵の DHCP サーバに対して割り当てるべき IP アドレスを設定することもできます。この &lt;host&gt; 要素では、 IPv4 の場合は割当先のホストの MAC アドレスと DHCP サーバが割り当てるべき IP アドレス、そしてホスト名をそれぞれ設定します。 IPv6 の場合は IPv4 と少し異なり、 IPv6 では使用しない <literal>mac</literal> 属性が存在しない代わりに、 <literal>name</literal> 属性でホストを識別します。 DHCPv6 の場合、 <literal>name</literal> 属性は対象の純粋なホスト名である必要があります。なお、 IPv4 でも <literal>mac</literal> の代わりにホスト名を使用することができます。</para>
       </callout>
      </calloutlist>
     </example>
     <example>
      <title>ルーティング型のネットワーク</title>
      <para>下記の設定は、仮想ネットワーク内のトラフィックを LAN にそのまま (NAT を適用せずに) 転送する場合の例を示しています。 IP アドレスの範囲は、 &vmhost; 側のネットワークルータであらかじめ設定されていなければなりません。</para>
<screen>
&lt;network&gt;
 &lt;name&gt;vnet_routed&lt;/name&gt;
 &lt;bridge name="virbr1"/&gt;
 &lt;forward mode="route" dev="eth1"/&gt;<co xml:id="vnet-xml-route"/>
 &lt;ip address="192.168.122.1" netmask="255.255.255.0"&gt;
  &lt;dhcp&gt;
   &lt;range start="192.168.122.2" end="192.168.122.254"/&gt;
  &lt;/dhcp&gt;
 &lt;/ip&gt;
&lt;/network&gt;
</screen>
      <calloutlist>
       <callout arearefs="vnet-xml-route">
        <para>この例では、ゲスト側のトラフィックは &vmhost; 内の <systemitem>eth1</systemitem> デバイスを介して送信されます。</para>
       </callout>
      </calloutlist>
     </example>
     <example>
      <title>孤立したネットワーク</title>
      <para>この設定は、プライベートネットワークとして完全に孤立させる設定例となります。ゲスト同士や &vmhost; との間では通信ができるものの、 LAN 内のマシンからはは全く接続できなくなります。これは XML の設定内に &lt;forward&gt; 要素が存在していないためです。</para>
<screen>&lt;network&gt;
 &lt;name&gt;vnet_isolated&lt;/name&gt;
 &lt;bridge name="virbr3"/&gt;
 &lt;ip address="192.168.152.1" netmask="255.255.255.0"&gt;
  &lt;dhcp&gt;
   &lt;range start="192.168.152.2" end="192.168.152.254"/&gt;
  &lt;/dhcp&gt;
 &lt;/ip&gt;
 &lt;/network&gt;
</screen>
     </example>
     <example>
      <title>&vmhost; にある既存のブリッジの使用</title>
      <para>この設定は、 &vmhost; 内にある既存のネットワークブリッジ <literal>br0</literal> を使用する場合の例です。 &vmguest; は物理ネットワークに直接接続される形になります。 &vmguest; の IP アドレスは物理ネットワークと同じサブネットでなければなりませんが、その代わり、接続に関する制限は送受信ともになくなります。</para>
<screen>&lt;network&gt;
        &lt;name&gt;host-bridge&lt;/name&gt;
        &lt;forward mode="bridge"/&gt;
        &lt;bridge name="br0"/&gt;
&lt;/network&gt;
</screen>
     </example>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-virsh-list">
     <title>ネットワークの一覧表示</title>
     <para>&libvirt; で利用可能な仮想ネットワークの一覧を表示するには、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-list --all

 名前                 状態       自動起動      永続
----------------------------------------------------------
 crowbar              動作中     はい (yes)    はい (yes)
 vnet_nated           動作中     はい (yes)    はい (yes)
 vnet_routed          動作中     はい (yes)    はい (yes)
 vnet_isolated        停止状態   はい (yes)    はい (yes)</screen>
<!-- fate#316628: Tool to obtain IP address allocated (DHCP)
      for bridged network in KVM -->
     <para>利用可能なドメインの一覧を表示するには、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh list
 Id    名前                           状態
----------------------------------------------------
 1     nated_sles12sp3                動作中
 ...</screen>
     <para>動作中のドメインのインターフェイス一覧を表示したい場合は、 <option>domifaddr <replaceable>ドメイン</replaceable></option> のように入力して実行します。このとき、インターフェイス名を直接指定して実行することもできます。既定では IP アドレスと MAC アドレスがそれぞれ出力されます:</para>
<screen>&prompt.sudo;virsh domifaddr nated_sles12sp3 --interface vnet0 --source lease
 名前       MAC アドレス          Protocol     Address
-------------------------------------------------------------------------------
 vnet0      52:54:00:9e:0d:2b    ipv6         fd00:dead:beef:55::140/64
 -          -                    ipv4         192.168.100.168/24</screen>
     <para>指定したドメインに関連づけられた全ての仮想インターフェイスの概要情報を表示するには、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh domiflist nated_sles12sp3
Interface  Type       Source       Model       MAC
---------------------------------------------------------
vnet0      network    vnet_nated   virtio      52:54:00:9e:0d:2b</screen>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-virsh-info">
     <title>ネットワークの関する詳細の取得</title>
     <para>ネットワークに関する詳細情報を取得するには、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-info vnet_routed
名前:           vnet_routed
UUID:           756b48ff-d0c6-4c0a-804c-86c4c832a498
起動中:         はい (yes)
永続:           はい (yes)
自動起動:       はい (yes)
ブリッジ:         virbr5</screen>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-virsh-start">
     <title>ネットワークの起動</title>
     <para>設定済みではあるものの、現在起動していないネットワークを起動したい場合は、まず対象の名前 (もしくは識別子や UUID) を判断します。具体的には、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-list --inactive
 名前                 状態        自動起動      永続
----------------------------------------------------------
 vnet_isolated        停止状態    はい (yes)    はい (yes)</screen>
     <para>あとは下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-start vnet_isolated
ネットワーク vnet_isolated を起動しました</screen>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-virsh-stop">
     <title>ネットワークの停止</title>
     <para>動作中のネットワークを停止するには、まず対象の名前 (もしくは識別子や UUID) を判断します。具体的には、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-list --inactive
 名前                 状態        自動起動      永続
----------------------------------------------------------
 vnet_isolated        動作中      はい (yes)    はい (yes)</screen>
     <para>あとは下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-destroy vnet_isolated
ネットワーク vnet_isolated は強制停止されました</screen>
    </sect4>
    <sect4 xml:id="libvirt-networks-virtual-virsh-undefine">
     <title>ネットワークの削除</title>
     <para>停止したネットワークの設定を &vmhost; から完全に停止したい場合は、下記のように入力して実行します:</para>
<screen>&prompt.sudo;virsh net-undefine vnet_isolated
ネットワーク vnet_isolated の定義が削除されました</screen>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="libvirt-host-storage">
  <title>Configuring a Storage Pool</title>

  <para>
   When managing a &vmguest; on the &vmhost; itself, you can access the
   complete file system of the &vmhost; to attach or create virtual hard disks
   or to attach existing images to the &vmguest;. However, this is not possible
   when managing &vmguest;s from a remote host. For this reason, &libvirt;
   supports so called <quote>Storage Pools</quote>, which can be accessed from
   remote machines.
  </para>

  <tip>
   <title>CD/DVD ISO images</title>
   <para>
    To be able to access CD/DVD ISO images on the &vmhost; from remote clients, they
    also need to be placed in a storage pool.
   </para>
  </tip>

  <para>
   &libvirt; knows two different types of storage: volumes and pools.
  </para>

  <variablelist>
   <varlistentry>
    <term>Storage Volume</term>
    <listitem>
     <para>
      A storage volume is a storage device that can be assigned to a
      guest&mdash;a virtual disk or a CD/DVD/floppy image. Physically, it can
      be a block device&mdash;for example, a partition or a logical
      volume&mdash;or a file on the &vmhost;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Storage Pool</term>
    <listitem>
     <para>
      A storage pool is a storage resource on the &vmhost; that can be used for
      storing volumes, similar to network storage for a desktop machine.
      Physically it can be one of the following types:
     </para>
     <variablelist>
      <varlistentry>
       <term>File System Directory (<guimenu>dir</guimenu>)</term>
       <listitem>
        <para>
         A directory for hosting image files. The files can be either one of
         the supported disk formats (raw or qcow2), or ISO images.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Physical Disk Device (<guimenu>disk</guimenu>)</term>
       <listitem>
        <para>
         Use a complete physical disk as storage. A partition is created for
         each volume that is added to the pool.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Pre-Formatted Block Device (<guimenu>fs</guimenu>)</term>
       <listitem>
        <para>
         Specify a partition to be used in the same way as a file system
         directory pool (a directory for hosting image files). The only
         difference to using a file system directory is that &libvirt; takes
         care of mounting the device.
        </para>
       </listitem>
      </varlistentry>
<!-- <varlistentry>
      <term>GlusterFS device (<guimenu>gluster</guimenu>)</term>
      <listitem>
       <para>
        ...
       </para>
      </listitem>
     </varlistentry> -->
      <varlistentry>
       <term>iSCSI Target (iscsi)</term>
       <listitem>
        <para>
         Set up a pool on an iSCSI target. You need to have been logged in to
         the volume once before to use it with &libvirt;. Use the &yast;
         <guimenu>iSCSI Initiator</guimenu> to detect and log in to a
         volume<phrase os="sles">, see <xref linkend="book-storage"/> for
         details</phrase>. Volume creation on iSCSI pools is not supported,
         instead each existing Logical Unit Number (LUN) represents a volume.
         Each volume/LUN also needs a valid (empty) partition table or disk
         label before you can use it. If missing, use <command>fdisk</command>
         to add it:
        </para>
<screen>&prompt.sudo;fdisk -cu /dev/disk/by-path/ip-&wsIip;:3260-iscsi-iqn.2010-10.com.example:[...]-lun-2
Device contains neither a valid DOS partition table, nor Sun, SGI
or OSF disklabel
Building a new DOS disklabel with disk identifier 0xc15cdc4e.
Changes will remain in memory only, until you decide to write them.
After that, of course, the previous content won't be recoverable.

Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>LVM Volume Group (logical)</term>
       <listitem>
        <para>
         Use an LVM volume group as a pool. You may either use a predefined
         volume group, or create a group by specifying the devices to use.
         Storage volumes are created as partitions on the volume.
        </para>
        <warning>
         <title>Deleting the LVM-Based Pool</title>
         <para>
          When the LVM-based pool is deleted in the Storage Manager, the volume
          group is deleted as well. This results in a non-recoverable loss of
          all data stored on the pool!
         </para>
        </warning>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Multipath Devices (<guimenu>mpath</guimenu>)</term>
       <listitem>
        <para>
         At the moment, multipathing support is limited to assigning existing
         devices to the guests. Volume creation or configuring multipathing
         from within &libvirt; is not supported.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Network Exported Directory (<guimenu>netfs</guimenu>)</term>
       <listitem>
        <para>
         Specify a network directory to be used in the same way as a file
         system directory pool (a directory for hosting image files). The only
         difference to using a file system directory is that &libvirt; takes
         care of mounting the directory. Supported protocols are NFS and
         GlusterFS.
        </para>
       </listitem>
      </varlistentry>
<!-- <varlistentry>
      <term>Rados Block Device (<guimenu>rbd</guimenu>)</term>
      <listitem>
       <para>
        ...
       </para>
      </listitem>
     </varlistentry> -->
<!-- <varlistentry>
      <term>Sheepdog distributed storage system (<guimenu>sheepdog</guimenu>)</term>
      <listitem>
       <para>
        ...
       </para>
      </listitem>
     </varlistentry> -->
      <varlistentry>
       <term>SCSI Host Adapter (<guimenu>scsi</guimenu>)</term>
       <listitem>
        <para>
         Use an SCSI host adapter in almost the same way as an iSCSI target. We
         recommend to use a device name from
         <filename>/dev/disk/by-*</filename> rather than
         <filename>/dev/sd<replaceable>X</replaceable></filename>. The latter
         can change (for example, when adding or removing hard disks). Volume
         creation on iSCSI pools is not supported. Instead, each existing LUN
         (Logical Unit Number) represents a volume.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

  <warning>
   <title>Security Considerations</title>
   <para>
    To avoid data loss or data corruption, do not attempt to use resources such
    as LVM volume groups, iSCSI targets, etc., that are also used to build
    storage pools on the &vmhost;. There is no need to connect to these
    resources from the &vmhost; or to mount them on the
    &vmhost;&mdash;&libvirt; takes care of this.
   </para>
   <para>
    Do not mount partitions on the &vmhost; by label. Under certain
    circumstances it is possible that a partition is labeled from within a
    &vmguest; with a name already existing on the &vmhost;.
   </para>
  </warning>

  <sect2 xml:id="sec-libvirt-storage-virsh">
   <title>Managing Storage with <command>virsh</command></title>
   <para>
    Managing storage from the command line is also possible by using
    <command>virsh</command>. However, creating storage pools is currently not
    supported by &suse;. Therefore, this section is restricted to documenting
    functions such as starting, stopping, and deleting pools, and volume management.
   </para>
   <para>
    A list of all <command>virsh</command> subcommands for managing pools and
    volumes is available by running <command>virsh help pool</command> and
    <command>virsh help volume</command>, respectively.
   </para>
   <sect3 xml:id="sec-libvirt-storage-virsh-list-pools">
    <title>Listing Pools and Volumes</title>
    <para>
     List all pools currently active by executing the following command. To
     also list inactive pools, add the option <option>--all</option>:
    </para>
<screen>&prompt.user;virsh pool-list --details</screen>
    <para>
     Details about a specific pool can be obtained with the
     <literal>pool-info</literal> subcommand:
    </para>
<screen>&prompt.user;virsh pool-info <replaceable>POOL</replaceable></screen>
    <para>
     By default, volumes can only be listed per pool. To list all volumes from a
     pool, enter the following command.
    </para>
<screen>&prompt.user;virsh vol-list --details <replaceable>POOL</replaceable></screen>
    <para>
     At the moment <command>virsh</command> offers no tools to show whether a
     volume is used by a guest or not. The following procedure describes a way
     to list volumes from all pools that are currently used by a &vmguest;.
    </para>
    <procedure xml:id="pro-libvirt-storage-virsh-list-vols">
     <title>Listing all Storage Volumes Currently Used on a &vmhost;</title>
     <step>
      <para>
       Create an XSLT style sheet by saving the following content to a file,
       for example, ~/libvirt/guest_storage_list.xsl:
      </para>
<screen>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;xsl:stylesheet version="1.0"
  xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
  &lt;xsl:output method="text"/&gt;
  &lt;xsl:template match="text()"/&gt;
  &lt;xsl:strip-space elements="*"/&gt;
  &lt;xsl:template match="disk"&gt;
    &lt;xsl:text&gt;  &lt;/xsl:text&gt;
    &lt;xsl:value-of select="(source/@file|source/@dev|source/@dir)[1]"/&gt;
    &lt;xsl:text&gt;&amp;#10;&lt;/xsl:text&gt;
  &lt;/xsl:template&gt;
&lt;/xsl:stylesheet&gt;</screen>
     </step>
     <step>
      <para>
       Run the following commands in a shell. It is assumed that the guest's
       XML definitions are all stored in the default location
       (<filename>/etc/libvirt/qemu</filename>). <command>xsltproc</command> is
       provided by the package
       <systemitem class="resource">libxslt</systemitem>.
      </para>
<screen>SSHEET="$HOME/libvirt/guest_storage_list.xsl"
cd /etc/libvirt/qemu
for FILE in *.xml; do
  basename $FILE .xml
  xsltproc $SSHEET $FILE
done</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-libvirt-storage-virsh-start-pools">
    <title>Starting, Stopping, and Deleting Pools</title>
    <para>
     Use the <command>virsh</command> pool subcommands to start, stop, or delete
     a pool. Replace <replaceable>POOL</replaceable> with the pool's name or
     its UUID in the following examples:
    </para>
    <variablelist>
     <varlistentry>
      <term>Stopping a Pool</term>
      <listitem>
<screen>&prompt.user;virsh pool-destroy <replaceable>POOL</replaceable></screen>
       <note>
        <title>A Pool's State Does not Affect Attached Volumes</title>
        <para>
         Volumes from a pool attached to &vmguest;s are always available,
         regardless of the pool's state (<guimenu>Active</guimenu> (stopped) or
         <guimenu>Inactive</guimenu> (started)). The state of the pool solely
         affects the ability to attach volumes to a &vmguest; via remote
         management.
        </para>
       </note>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Deleting a Pool</term>
      <listitem>
<screen>&prompt.user;virsh pool-delete <replaceable>POOL</replaceable></screen>
       <warning>
        <title>Deleting Storage Pools</title>
        <para>
         See <xref linkend="deleting-storage-pools"/>
        </para>
       </warning>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Starting a Pool</term>
      <listitem>
<screen>&prompt.user;virsh pool-start <replaceable>POOL</replaceable></screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Enable Autostarting a Pool</term>
      <listitem>
<screen>&prompt.user;virsh pool-autostart <replaceable>POOL</replaceable></screen>
       <para>
        Only pools that are marked to autostart will automatically be started
        if the &vmhost; reboots.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Disable Autostarting a Pool</term>
      <listitem>
<screen>&prompt.user;virsh pool-autostart <replaceable>POOL</replaceable> --disable</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec-libvirt-storage-virsh-add-volumes">
    <title>Adding Volumes to a Storage Pool</title>
    <para>
     <command>virsh</command> offers two ways to add volumes to storage pools:
     either from an XML definition with <literal>vol-create</literal> and
     <literal>vol-create-from</literal> or via command line arguments with
     <literal>vol-create-as</literal>. The first two methods are currently not
     supported by &suse;, therefore this section focuses on the subcommand
     <literal>vol-create-as</literal>.
    </para>
    <para>
     To add a volume to an existing pool, enter the following command:
    </para>
<screen>&prompt.user;virsh vol-create-as <replaceable>POOL</replaceable><co xml:id="co-vol-create-as-pool"/><replaceable>NAME</replaceable><co xml:id="co-vol-create-as-name"/> 12G --format<co xml:id="co-vol-create-as-capacity"/><replaceable>raw|qcow2</replaceable><co xml:id="co-vol-create-as-format"/> --allocation 4G<co xml:id="co-vol-create-as-alloc"/></screen>
    <calloutlist>
     <callout arearefs="co-vol-create-as-pool">
      <para>
       Name of the pool to which the volume should be added
      </para>
     </callout>
     <callout arearefs="co-vol-create-as-name">
      <para>
       Name of the volume
      </para>
     </callout>
     <callout arearefs="co-vol-create-as-capacity">
      <para>
       Size of the image, in this example 12 gigabytes. Use the suffixes k, M,
       G, T for kilobyte, megabyte, gigabyte, and terabyte, respectively.
      </para>
     </callout>
     <callout arearefs="co-vol-create-as-format">
      <para>
       Format of the volume. &suse; currently supports <literal>raw</literal>
       and <literal>qcow2</literal>.
      </para>
     </callout>
     <callout arearefs="co-vol-create-as-alloc">
      <para>
       Optional parameter. By default, <command>virsh</command> creates a sparse
       image file that grows on demand. Specify the amount of space that should
       be allocated with this parameter (4 gigabytes in this example). Use the
       suffixes k, M, G, T for kilobyte, megabyte, gigabyte, and terabyte,
       respectively.
      </para>
      <para>
       When not specifying this parameter, a sparse image file with no
       allocation will be generated. To create a non-sparse volume, specify the
       whole image size with this parameter (would be <literal>12G</literal> in
       this example).
      </para>
     </callout>
    </calloutlist>
    <sect4 xml:id="sec-libvirt-storage-virsh-add-volumes-clone">
     <title>Cloning Existing Volumes</title>
     <para>
      Another way to add volumes to a pool is to clone an existing volume. The
      new instance is always created in the same pool as the original.
     </para>
<screen>&prompt.user;virsh vol-clone <replaceable>NAME_EXISTING_VOLUME</replaceable><co xml:id="co-vol-clone-existing"/><replaceable>NAME_NEW_VOLUME</replaceable><co xml:id="co-vol-clone-new"/> --pool <replaceable>POOL</replaceable><co xml:id="co-vol-clone-pool"/></screen>
     <calloutlist>
      <callout arearefs="co-vol-clone-existing">
       <para>
        Name of the existing volume that should be cloned
       </para>
      </callout>
      <callout arearefs="co-vol-clone-new">
       <para>
        Name of the new volume
       </para>
      </callout>
      <callout arearefs="co-vol-clone-pool">
       <para>
        Optional parameter. &libvirt; tries to locate the existing volume
        automatically. If that fails, specify this parameter.
       </para>
      </callout>
     </calloutlist>
    </sect4>
   </sect3>
   <sect3 xml:id="sec-libvirt-storage-virsh-del-volumes">
    <title>Deleting Volumes from a Storage Pool</title>
    <para>
     To permanently delete a volume from a pool, use the subcommand
     <literal>vol-delete</literal>:
    </para>
<screen>&prompt.user;virsh vol-delete <replaceable>NAME</replaceable> --pool <replaceable>POOL</replaceable></screen>
    <para>
     <option>--pool</option> is optional. &libvirt; tries to locate the volume
     automatically. If that fails, specify this parameter.
    </para>
    <warning>
     <title>No Checks Upon Volume Deletion</title>
     <para>
      A volume will be deleted in any case, regardless of whether it is
      currently used in an active or inactive &vmguest;. There is no way to
      recover a deleted volume.
     </para>
     <para>
      Whether a volume is used by a &vmguest; can only be detected by using by
      the method described in
      <xref linkend="pro-libvirt-storage-virsh-list-vols"/>.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="libvirt-storage-virsh-attach-volumes">
    <title>Attaching Volumes to a &vmguest;</title>
    <para>
     After you create a volume as described in
     <xref linkend="sec-libvirt-storage-virsh-add-volumes"/>, you can
     attach it to a virtual machine and use it as a hard disk:
    </para>
<screen>&prompt.user;virsh attach-disk <replaceable>DOMAIN</replaceable> <replaceable>SOURCE_IMAGE_FILE</replaceable> <replaceable>TARGET_DISK_DEVICE</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.user;virsh attach-disk sles12sp3 /virt/images/example_disk.qcow2 sda2</screen>
    <para>
     To check if the new disk is attached, inspect the result of the
     <command>virsh dumpxml</command> command:
    </para>
<screen>&prompt.root;virsh dumpxml sles12sp3
[...]
&lt;disk type='file' device='disk'&gt;
 &lt;driver name='qemu' type='raw'/&gt;
 &lt;source file='/virt/images/example_disk.qcow2'/&gt;
 &lt;backingStore/&gt;
 &lt;target dev='sda2' bus='scsi'/&gt;
 &lt;alias name='scsi0-0-0'/&gt;
 &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
&lt;/disk&gt;
[...]</screen>
    <sect4>
     <title>Hotplug or Persistent Change</title>
     <para>
      You can attach disks to both active and inactive domains. The attachment
      is controlled by the <option>--live</option> and
      <option>--config</option> options:
     </para>
     <variablelist>
      <varlistentry>
       <term><option>--live</option></term>
       <listitem>
        <para>
         Hotplugs the disk to an active domain. The attachment is not saved in
         the domain configuration. Using <option>--live</option> on an inactive
         domain is an error.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--config</option></term>
       <listitem>
        <para>
         Changes the domain configuration persistently. The attached disk is
         then available after the next domain start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--live</option><option>--config</option></term>
       <listitem>
        <para>
         Hotplugs the disk and adds it to the persistent domain configuration.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <tip>
      <title><command>virsh attach-device</command></title>
      <para>
       <command>virsh attach-device</command> is the more generic form of
       <command>virsh attach-disk</command>. You can use it to attach other
       types of devices to a domain.
      </para>
     </tip>
    </sect4>
   </sect3>
   <sect3 xml:id="libvirt-storage-virsh-detach-volumes">
    <title>Detaching Volumes from a &vmguest;</title>
    <para>
     To detach a disk from a domain, use <command>virsh detach-disk</command>:
    </para>
<screen>&prompt.root;virsh detach-disk <replaceable>DOMAIN</replaceable> <replaceable>TARGET_DISK_DEVICE</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;virsh detach-disk sles12sp3 sda2</screen>
    <para>
     You can control the attachment with the <option>--live</option> and
     <option>--config</option> options as described in
     <xref linkend="libvirt-storage-virsh-attach-volumes"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-libvirt-storage-vmm">
   <title>Managing Storage with &vmm;</title>
   <para>
    The &vmm; provides a graphical interface&mdash;the Storage Manager&mdash;to
    manage storage volumes and pools. To access it, either right-click a
    connection and choose <guimenu>Details</guimenu>, or highlight a connection
    and choose <menuchoice> <guimenu>Edit</guimenu> <guimenu>Connection
    Details</guimenu> </menuchoice>. Select the <guimenu>Storage</guimenu> tab.
   </para>
   <informalfigure>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="virt_virt-manager_storage.png" width="60%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="virt_virt-manager_storage.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
   <sect3 xml:id="sec-libvirt-storage-vmm-addpool">
    <title>Adding a Storage Pool</title>
    <para>
     To add a storage pool, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Click <guimenu>Add</guimenu> in the bottom left corner. The dialog
       <guimenu>Add a New Storage Pool</guimenu> appears.
      </para>
     </step>
     <step>
      <para>
       Provide a <guimenu>Name</guimenu> for the pool (consisting of
       alphanumeric characters and <literal>_-.</literal>) and select a
       <guimenu>Type</guimenu>. Proceed with <guimenu>Forward</guimenu>.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="virt_virt-manager_storage_add.png" width="60%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="virt_virt-manager_storage_add.png" width="60%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       Specify the required details in the following window. The data that
       needs to be entered depends on the type of pool you are creating:
      </para>
      <variablelist>
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>dir</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: Specify an existing directory.
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>disk</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: The directory that hosts the
            devices. The default value <filename>/dev</filename> should usually
            fit.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Format</guimenu>: Format of the device's partition table.
            Using <guimenu>auto</guimenu> should usually work. If not, get the
            required format by running the command <command>parted</command>
            <option>-l</option> on the &vmhost;.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Source Path</guimenu>: Path to the device. It is
            recommended to use a device name from
            <filename>/dev/disk/by-*</filename> rather than the simple
            <filename>/dev/sd<replaceable>X</replaceable></filename>, since the
            latter can change (for example, when adding or removing hard
            disks). You need to specify the path that resembles the whole disk,
            not a partition on the disk (if existing).
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Build Pool</guimenu>: Activating this option formats the
            device. Use with care&mdash;all data on the device will be lost!
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>fs</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: Mount point on the &vmhost; file
            system.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Format: </guimenu> File system format of the device. The
            default value <literal>auto</literal> should work.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Source Path</guimenu>: Path to the device file. It is
            recommended to use a device name from
            <filename>/dev/disk/by-*</filename> rather than
            <filename>/dev/sd<replaceable>X</replaceable></filename>, because
            the latter can change (for example, when adding or removing hard
            disks).
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
<!-- <varlistentry>
       <term><emphasis role="bold">Type</emphasis> <guimenu>gluster</guimenu></term>
       <listitem>
        <itemizedlist>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: ...
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>-->
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>iscsi</guimenu></term>
        <listitem>
         <para>
          Get the necessary data by running the following command on the
          &vmhost;:
         </para>
<screen>&prompt.sudo;iscsiadm --mode node</screen>
         <para>
          It will return a list of iSCSI volumes with the following format. The
          elements in bold text are required:
         </para>
<screen><emphasis role="bold">IP_ADDRESS</emphasis>:PORT,TPGT <emphasis role="bold">TARGET_NAME_(IQN)</emphasis></screen>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: The directory containing the device
            file. Use <literal>/dev/disk/by-path</literal> (default) or
            <literal>/dev/disk/by-id</literal>.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Host Name</guimenu>: Host name or IP address of the iSCSI
            server.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Source Path</guimenu>: The iSCSI target name (IQN).
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>logical</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: In case you use an existing volume
            group, specify the existing device path. When building a new LVM
            volume group, specify a device name in the
            <filename>/dev</filename> directory that does not already exist.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Source Path</guimenu>: Leave empty when using an existing
            volume group. When creating a new one, specify its devices here.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Build Pool</guimenu>: Only activate when creating a new
            volume group.
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>mpath</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: Support for multipathing is
            currently limited to making all multipath devices available.
            Therefore, specify an arbitrary string here that will then be
            ignored. The path is required, otherwise the XML parser will fail.
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis><guimenu>netfs</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: Mount point on the &vmhost; file
            system.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Host Name</guimenu>: IP address or host name of the server
            exporting the network file system.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Source Path</guimenu>: Directory on the server that is
            being exported.
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
<!-- <varlistentry>
       <term><emphasis role="bold">Type</emphasis> <guimenu>rbd</guimenu></term>
       <listitem>
        <itemizedlist>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Name</guimenu>: ...
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>-->
       <varlistentry>
        <term><emphasis role="bold">Type</emphasis> <guimenu>scsi</guimenu></term>
        <listitem>
         <itemizedlist mark="bullet" spacing="normal">
          <listitem>
           <para>
            <guimenu>Target Path</guimenu>: The directory containing the device
            file. Use <literal>/dev/disk/by-path</literal> (default) or
            <literal>/dev/disk/by-id</literal>.
           </para>
          </listitem>
          <listitem>
           <para>
            <guimenu>Source Path</guimenu>: Name of the SCSI adapter.
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </varlistentry>
<!-- <varlistentry>
       <term><emphasis role="bold">Type</emphasis> <guimenu>rbd</guimenu></term>
       <listitem>
        <itemizedlist>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Name</guimenu>: ...
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>-->
      </variablelist>
      <note>
       <title>File Browsing</title>
       <para>
        Using the file browser by clicking <guimenu>Browse</guimenu> is not
        possible when operating remotely.
       </para>
      </note>
     </step>
     <step>
      <para>
       Click <guimenu>Finish</guimenu> to add the storage pool.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-libvirt-storage-vmm-manage">
    <title>Managing Storage Pools</title>
    <para>
     &vmm;'s Storage Manager lets you create or delete volumes in a pool. You
     may also temporarily deactivate or permanently delete existing storage
     pools. Changing the basic configuration of a pool is currently not
     supported by &suse;.
    </para>
    <sect4 xml:id="sec-libvirt-storage-vmm-manage-pool">
     <title>Starting, Stopping and Deleting Pools</title>
     <para>
      The purpose of storage pools is to provide block devices located on the
      &vmhost; that can be added to a &vmguest; when managing it from remote.
      To make a pool temporarily inaccessible from remote, click
      <guimenu>Stop</guimenu> in the bottom left corner of the Storage Manager.
      Stopped pools are marked with <guimenu>State: Inactive</guimenu> and are
      grayed out in the list pane. By default, a newly created pool will be
      automatically started <guimenu>On Boot</guimenu> of the &vmhost;.
     </para>
     <para>
      To start an inactive pool and make it available from remote again, click
      <guimenu>Start</guimenu> in the bottom left corner of the Storage
      Manager.
     </para>
     <note>
      <title>A Pool's State Does not Affect Attached Volumes</title>
      <para>
       Volumes from a pool attached to &vmguest;s are always available,
       regardless of the pool's state (<guimenu>Active</guimenu> (stopped) or
       <guimenu>Inactive</guimenu> (started)). The state of the pool solely
       affects the ability to attach volumes to a &vmguest; via remote
       management.
      </para>
     </note>
     <para>
      To permanently make a pool inaccessible, click <guimenu>Delete</guimenu>
      in the bottom left corner of the Storage Manager. You may only delete
      inactive pools. Deleting a pool does not physically erase its contents on
      &vmhost;&mdash;it only deletes the pool configuration. However, you need
      to be extra careful when deleting pools, especially when deleting LVM
      volume group-based tools:
     </para>
     <warning xml:id="deleting-storage-pools">
      <title>Deleting Storage Pools</title>
      <para>
       Deleting storage pools based on <emphasis>local</emphasis> file system
       directories, local partitions or disks has no effect on the availability
       of volumes from these pools currently attached to &vmguest;s.
      </para>
      <para>
       Volumes located in pools of type iSCSI, SCSI, LVM group or Network
       Exported Directory will become inaccessible from the &vmguest; if the
       pool is deleted. Although the volumes themselves will not be deleted,
       the &vmhost; will no longer have access to the resources.
      </para>
      <para>
       Volumes on iSCSI/SCSI targets or Network Exported Directory will become
       accessible again when creating an adequate new pool or when
       mounting/accessing these resources directly from the host system.
      </para>
      <para>
       When deleting an LVM group-based storage pool, the LVM group definition
       will be erased and the LVM group will no longer exist on the host
       system. The configuration is not recoverable and all volumes from this
       pool are lost.
      </para>
     </warning>
    </sect4>
    <sect4 xml:id="sec-libvirt-storage-vmm-manage-volume-add">
     <title>Adding Volumes to a Storage Pool</title>
     <para>
      &vmm; lets you create volumes in all storage pools, except in pools of
      types Multipath, iSCSI, or SCSI. A volume in these pools is equivalent to
      a LUN and cannot be changed from within &libvirt;.
     </para>
     <procedure>
      <step>
       <para>
        A new volume can either be created using the Storage Manager or while
        adding a new storage device to a &vmguest;. In either case, select a
        storage pool from the left panel, then click <guimenu>Create new
        volume</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Specify a <guimenu>Name</guimenu> for the image and choose an image
        format.
       </para>
       <para>
        &suse; currently only supports <literal>raw</literal> or
        <literal>qcow2</literal> images. The latter option is not available on
        LVM group-based pools.
       </para>
       <para>
        Next to <guimenu>Max Capacity</guimenu>, specify the amount maximum
        size that the disk image is allowed to reach. Unless you are working
        with a <literal>qcow2</literal> image, you can also set an amount for
        <guimenu>Allocation</guimenu> that should be allocated initially. If
        both values differ, a sparse image file will be created which grows on
        demand.
       </para>
       <para>
        For <literal>qcow2</literal> images, you can use a <guimenu>Backing
        Store</guimenu> (also called <quote>backing file</quote>) which
        constitutes a base image. The newly created <literal>qcow2</literal>
        image will then only record the changes that are made to the base
        image.
       </para>
      </step>
      <step>
       <para>
        Start the volume creation by clicking <guimenu>Finish</guimenu>.
       </para>
      </step>
     </procedure>
    </sect4>
    <sect4 xml:id="sec-libvirt-storage-vmm-manage-volume-delete">
     <title>Deleting Volumes From a Storage Pool</title>
     <para>
      Deleting a volume can only be done from the Storage Manager, by selecting
      a volume and clicking <guimenu>Delete Volume</guimenu>. Confirm with
      <guimenu>Yes</guimenu>.
     </para>
     <warning>
      <title>Volumes Can Be Deleted Even While in Use</title>
      <para>
       Volumes can be deleted even if they are currently used in an active or
       inactive &vmguest;. There is no way to recover a deleted volume.
      </para>
      <para>
       Whether a volume is used by a &vmguest; is indicated in the
       <guimenu>Used By</guimenu> column in the Storage Manager.
      </para>
     </warning>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
</chapter>
