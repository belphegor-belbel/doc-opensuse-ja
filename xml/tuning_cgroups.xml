<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "generic-entities.ent">
%entities;
<!-- custom prompt entities for the 'Controlling I/O with Proportional Weight Policy' section --><!ENTITY reader1 "reader1:/io-cgroup #">
<!ENTITY prompt.reader1 "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[reader1] host1:/io-cgroup # </prompt>">
<!ENTITY prompt.reader2 "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[reader2] host1:/io-cgroup # </prompt>">
<!ENTITY prompt.io-controller "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/io-cgroup # </prompt>">
<!ENTITY prompt.io-cgroup "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>host1:/io-cgroup # </prompt>">
<!ENTITY prompt.plain-root "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>host1:~ # </prompt>">
<!ENTITY prompt.blkio "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>">
<!ENTITY prompt.unified "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/sys/fs/cgroup/unified # </prompt>">
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-tuning-cgroups">
 <title>カーネルコントロールグループ</title>
 <info>
  <abstract>
   <para>カーネルコントロールグループ ( <quote>cgroups</quote> ) は、プロセスに対してハードウエアやシステムの資源を割り当てたり、制限したりするための仕組みです。この機能を利用することで、プロセスをツリー構造で管理することができるようになります。</para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-tuning-cgroups-overview">
  <title>概要</title>

  <para>それぞれのプロセスは正確に 1 つの管理用 cgroup に割り当てられます。 cgroup は階層構造型のツリー (木構造) として管理するもので、その構造の任意の箇所 (枝) もしくは 1 つのプロセスに対して、 CPU やメモリ、ディスクの I/O やネットワーク帯域などのリソース制限を割り当てます。</para>

  <para>&productname; では、 &systemd; が cgroup を利用してグループ内の全てのプロセスを管理しています。この場合、 &systemd; はグループをスライスと呼んでいます。 &systemd; には、 cgroup の設定を行うためのインターフェイスも用意されています。</para>

  <para><command>systemd-cgls</command> コマンドでは、階層構造を表示することができます。</para>

  <para>カーネルが提供する cgroup の API には 2 種類のものが存在します。これらは提供する cgroup 属性が異なるほか、コントローラの階層構造が異なります。 &systemd; は抽象化の仕組みによって、これらの違いを吸収しています。既定では &systemd; は <emphasis>ハイブリッド</emphasis> モードで動作し、 v1 API を利用してコントローラを制御します。 cgroup v2 は systemd 内でのみ使用されます。このほか <emphasis>統合</emphasis> モードとして、コントローラを v2 API で制御するモードもあります。なお、モードはいずれか 1 つを選択します。</para>

  <para>統合型のコントロールグループ階層構造を有効化したい場合は、 &grub; ブートローダの設定で、カーネルのコマンドラインパラメータに <option>systemd.unified_cgroup_hierarchy=1</option> を追加してください (&grub; の設定方法に関する詳細は、 <xref linkend="cha-grub2"/> をお読みください) 。</para>
 </sect1>
 <sect1 xml:id="sec-tuning-cgroups-accounting">
  <title>リソースアカウンティング</title>

  <para>複数の cgroup にプロセスをまとめることで、 cgroup ごとの資源消費データを取得できるようになります。</para>

  <para>このような機能をアカウンティングと呼びますが、この機能自身にも小さいながらオーバーヘッドが存在しています。このオーバーヘッドはその中での処理内容にも依存しますが、特定の 1 つのユニットに対してアカウンティングを有効にすると、同じスライスに含まれる全てのユニットだけでなく、親スライスやそこに直接含まれるユニットに対しても、この機能が有効化されることに注意してください。</para>

  <para>ユニット単位でアカウンティングを有効化したい場合は <literal>MemoryAccounting=</literal> のようなディレクティブを使用することができます。全てのユニットに対して有効化したい場合は、 <filename>/etc/systemd/system.conf</filename> ファイル内の <literal>DefaultMemoryAccounting=</literal> ディレクティブを設定してください。設定可能なディレクティブに関する詳細は、 <command>man systemd.resource-control</command> で表示されるマニュアルページ (英語) をお読みください。</para>
 </sect1>
 <sect1 xml:id="sec-tuning-cgroups-usage">
  <title>リソース制限の設定</title>

  <note>
   <title>暗黙のリソース消費について</title>
   <para>暗黙のうちに消費され、実行環境によって異なるリソースが存在することに注意してください。これにはたとえば、ライブラリやカーネル内のデータ構造のほか、利用しているユーティリティの fork() 処理の振る舞い、計算の効率性などがあります。このようなことから、実行環境を変えた場合は、リソース制限を再計算する必要があります。</para>
  </note>

  <para>cgroup に対する制限は、 <command>systemctl set-property</command> コマンドで設定します。書式は下記のとおりです:</para>

<screen>&prompt.root;<command>systemctl set-property [--runtime] <replaceable>名前</replaceable> <replaceable>プロパティ_1</replaceable>=<replaceable>値</replaceable> [<replaceable>プロパティ_2</replaceable>=<replaceable>値</replaceable>]</command></screen>

  <para>設定値は即時に適用されます。なお、必要であれば <option>--runtime</option> オプションを指定することもできます。このオプションを指定すると、再起動後には指定した制限が適用されなくなります。</para>

  <para>また、 <replaceable>名前</replaceable> には &systemd; のサービス名やスコープ名、もしくはスライス名を指定します。</para>

  <para>プロパティの一覧と詳細については、 <command>man systemd.resource-control</command> で表示されるマニュアルページをお読みください。</para>
 </sect1>
 <sect1 xml:id="sec-tuning-cgroups-tasksmax">
  <title><literal>TasksMax</literal> を利用した fork ボムの防止</title>

  <para>&systemd; では、ユニットごとやスライスごとにタスク数の制限を設定することができます。 &systemd; の提供元では、ユニットごとのタスク数制限の既定値が設定されています (カーネル全体での制限 (詳しくは <command>/usr/sbin/sysctl kernel.pid_max</command> を参照) に対して 15% に設定しています) 。また、各ユーザのスライスはカーネル全体での制限の 33% になっています。ただし、 &slea; では異なる設定になっています。</para>

  <sect2 xml:id="sec-tasksmax-defaults">
   <title>現時点での既定の <literal>TasksMax</literal> 値の検出</title>
   <para>しかしながら、全ての用途に対して単一の制限を適用するのは現実的ではありません。 &productname; では、システムユニットやユーザスライスに対する提供元の既定値を上書きするための独自設定ファイルが 2 つ用意され、いずれも <literal>infinity</literal> に設定されています。 <filename>/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf </filename> には、下記のような設定が書かれています:</para>
<screen>
[Manager]
DefaultTasksMax=infinity
</screen>
   <para>もう 1 つの存在である <filename>/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf </filename> には、下記のような設定が書かれています:</para>
<screen>[Slice]
TasksMax=infinity
</screen>
   <para>DefaultTasksMax の値を確認するには、 <command>systemctl</command> を下記のように入力して実行します:</para>
<screen>&prompt.user;<command>systemctl show --property DefaultTasksMax</command>
DefaultTasksMax=infinity</screen>
   <para><literal>infinity</literal> は無制限の意味です。特に要件がなければ既定値を変更する必要はありませんが、必要であれば設定を変更してください。</para>
  </sect2>

  <sect2 xml:id="sec-edit-taskmax-default">
   <title><literal>DefaultTasksMax</literal> 値の設定</title>
   <para>グローバルな <literal>DefaultTasksMax</literal> の値を変更したい場合は、設定を上書きするための新しい設定ファイル <filename>/etc/systemd/system.conf.d/90-system-tasksmax.conf</filename> を作成して対応してください。この設定ファイルには、下記のような内容を記述します (下記の例では、 systemd のユニットごとに最大 256 個までのタスク制限を設定します):</para>
<screen>
[Manager]
DefaultTasksMax=256
</screen>
   <para>新しい設定を読み込んで、設定が反映されたことを確認します:</para>
<screen>&prompt.sudo;<command>systemctl daemon-reload</command>
&prompt.user;<command>systemctl show --property DefaultTasksMax</command>
DefaultTasksMax=256
</screen>
   <para>設定値はお使いのシステムの要件に合わせて指定してください。また、特定のサービスに限定して制限を高くすることもできます。たとえば MariaDB で設定を変更したい場合、まずは現在の設定値を確認します:</para>
<screen>
&prompt.user;<command>systemctl status mariadb.service</command>
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset&gt;
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para>Tasks 以下には現在動作中のタスク数 (30 個) と上限 (256 個) が示されています。負荷の高いデータベースシステムとしては不十分な値であることから、たとえば MariaDB のみを 8192 個までに拡大してみることにします。</para>
<screen>&prompt.sudo;<command>systemctl set-property mariadb.service TasksMax=8192</command>
&prompt.user;<command>systemctl status mariadb.service</command> 
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab&gt;
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─50-TasksMax.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta&gt;
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta&gt;
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para><command>systemctl set-property</command> コマンドは、 <filename>/etc/systemd/system/mariadb.service.d/50-TasksMax.conf</filename> という名前の上書き用設定ファイルを作成して、新しい制限を設定します。ここには既存のユニットファイルに対する上書き値のみを保存します。もちろん 8192 でなくてもかまいません。お使いのシステムの負荷状況に合わせて設定してください。</para>
  </sect2>

  <sect2>
   <title>ユーザに対する既定の <literal>TasksMax</literal> 制限</title>
   <para>ユーザに対する既定の制限値は十分に高く設定されています。これは、ユーザセッションではより多くのリソースを必要とするためです。独自の制限を設定したい場合は、 <filename>/etc/systemd/system/user-.slice.d/40-user-taskmask.conf</filename> のような設定ファイルを作成し、その中に設定値を記述してください。下記の例では、タスクの最大値を 16284 に設定しています:</para>
<screen>
[Slice]
TasksMax=16284
</screen>
   <note>
    <title>ファイル名の冒頭に付与する数値について</title>
    <para>上書き用の設定ファイルを作成する場合、そのファイル名の冒頭には数値を指定する必要があります。その数値の設定方法に関する詳細は、 <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in"/> をお読みください。</para>
   </note>
   <para>あとは systemd に対して設定値の再読み込みを指示し、設定が変更されたことを確認します:</para>
<screen>&prompt.sudo;<command>systemctl daemon-reload</command>
&prompt.user;<command>systemctl show --property TasksMax user-1000.slice</command>
TasksMax=16284
</screen>
   <para>具体的にどのような設定値にすべきかについては、システムの用途と搭載されているリソースのほか、他のリソース設定によっても異なります。 <literal>TasksMax</literal> の値が少なすぎる場合は <emphasis>Failed to fork (Resources temporarily unavailable)</emphasis> (fork に失敗した (リソースが一時的に利用できなくなっている)) や <emphasis>Can't create thread to handle new connection</emphasis> (新しい接続を処理するためのスレッドが作成できない), <emphasis>Error: Function call 'fork' failed with error code 11, 'Resource temporarily unavailable'</emphasis> (エラーコード 11 (リソースが一時的に利用できなくなっている) で fork の関数呼び出しが失敗した) などのエラーが発生します。</para>
   <para>systemd でのシステムリソースの制限の設定方法について、詳しくは<literal>systemd.resource-control (5)</literal> をお読みください。</para>
  </sect2>
 </sect1>

<!-- BEGIN NEW VERSION -->
<sect1 xml:id="sec-control-io-cgroups">
<title>カーネルコントロールグループ</title>
<para>
This section introduces using the Linux kernel's block I/O controller to
prioritize or throttle I/O operations. This leverages the means provided by
systemd to configure cgroups, and discusses probable pitfalls when dealing
with proportional I/O control.
</para>

<sect2 xml:id="sec-cgroups-prerequisites">
<title>Prerequisites</title>
<para>
The following subsections describe steps that you must take in advance when
you design and configure your system, since those aspects cannot be changed
during runtime.
</para>

<sect3 xml:id="sec-cgroups-filesystems">
<title>Filesystem</title>
<para>
You should use a cgroup-writeback-aware filesystem (otherwise writeback charging is not possible).
The recommended SLES filesystems added support in the following upstream
releases:
<!-- [2] is https://www.susecon.com/doc/2015/sessions/TUT20066.pdf, slide 7, maybe there's better SLES docs
cjs: that slide is old! We must find something newer.
-->
</para>
   <itemizedlist mark="bullet" spacing="normal">
	   <listitem>
     <para>
      Btrfs (v4.3)
      </para>
      </listitem>
	   <listitem>
     <para>
      Ext4 (v4.3)
      </para>
      </listitem>
	   <listitem>
     <para>
      XFS (v5.3)
      </para>
      </listitem>
   </itemizedlist>
<para>
As of &productname;&nbsp;15&nbsp;SP3, any of the named filesystems can be used.
</para>
</sect3>

<sect3 xml:id="sec-cgroups-unified-hierarchy">
<title>Unified cgroup hierarchy</title>
<para>
To properly account writeback I/O, it is necessary to have equal I/O
and memory controller cgroup hierarchies, and to use the cgroup v2 I/O
controller. Together, this means that one has to use the
<emphasis>unified</emphasis> cgroup hierarchy. This has to be requested
explicitly in SLES by passing a kernel command line option,
<option>systemd.unified_cgroup_hierarchy=1</option>.
</para>
</sect3>

<sect3 xml:id="sec-cgroups-block-io-scheduler">
<title>Block I/O scheduler</title>
<para>
The throttling policy is implemented higher in the stack, therefore it
does not require any additional adjustments.
The proportional I/O control policies have two different implementations:
the BFQ controller, and the cost-based model.
We describe the BFQ controller here. In order to exert its proportional
implementation for a particular device, we must make sure that BFQ is the
chosen scheduler. Check the current scheduler:
</para>
<screen>
&prompt.user;<command>cat /sys/class/block/<replaceable>sda</replaceable>/queue/scheduler</command>
mq-deadline kyber bfq [none]
</screen>
<para>
Switch the scheduler to BFQ:
</para>
<screen>
 &prompt.root;<command>echo bfq &gt; /sys/class/block/<replaceable>sda</replaceable>/queue/scheduler</command>
</screen>
<para>
You must specify the disk device (not a partition). The
optimal way to set this attribute is a udev rule specific to the device
(note that &slsa; ships udev rules that already enable BFQ for rotational
disk drives).
</para>
</sect3>

<sect3 xml:id="sec-cgroups-hierarchy">
<title>Cgroup hierarchy layout</title>
<para>
Normally, all tasks reside in the root cgroup and they compete against
each other. When the tasks are distributed into the cgroup tree the
competition occurs between sibling cgroups only.
This applies to the proportional I/O control; the throttling
hierarchically aggregates throughput of all descendants (see the following
diagram).
</para>
<!-- this is not code but a picture -->
<screen>
r
`-  a      IOWeight=100
    `- [c] IOWeight=300
    `-  d  IOWeight=100
`- [b]     IOWeight=200
</screen>
<para>
I/O is originating only from cgroups c and b. Even though c has a higher
weight, it will be treated with lower priority because it is level-competing
with b.
</para>
</sect3>
</sect2>

<sect2 xml:id="sec-cgroups-configure-control-quantities">
<title>Configuring control quantities</title>
<para>
You can apply the values to (long running) services permanently.
</para>
<screen>
&prompt.sudo;<command>systemctl set-property <replaceable>fast.service</replaceable> IOWeight=<replaceable>400</replaceable></command>
&prompt.sudo;<command>systemctl set-property <replaceable>slow.service</replaceable> IOWeight=<replaceable>50</replaceable></command>
&prompt.sudo;<command>systemctl set-property <replaceable>throttled.service</replaceable> IOReadBandwidthMax="<replaceable>/dev/sda 1M</replaceable>"</command>
</screen>

<para>
Alternatively, you can apply I/O control to individual commands, for
example:
</para>
<screen>
&prompt.sudo;<command>systemd-run --scope -p IOWeight=<replaceable>400</replaceable> <replaceable>high_prioritized_command</replaceable></command>
&prompt.sudo;<command>systemd-run --scope -p IOWeight=<replaceable>50</replaceable> <replaceable>low_prioritized_command</replaceable></command>
&prompt.sudo;<command>systemd-run --scope -p IOReadBandwidthMax="<replaceable>/dev/sda 1M</replaceable>" <replaceable>dd if=/dev/sda of=/dev/null bs=1M count=10</replaceable></command>
</screen>
</sect2>

<sect2 xml:id="sec-cgroups-control-behavior">
<title>I/O control behavior and setting expectations</title>
<para>
 The following list items describe I/O control behavior, and what you
 should expect under various conditions.
</para>
<itemizedlist>
<listitem>
<para>
I/O control works best for direct I/O operations (bypassing page cache),
the situations where the actual I/O is decoupled from the caller (typically
writeback via page cache) may manifest variously. For example, delayed I/O
control or even no observed I/O control (consider little bursts or competing
workloads that happen to never "meet", submitting I/O at the same time, and
saturating the bandwidth). For these reasons, the resulting ratio of
I/O throughputs does not strictly follow the ratio of configured weights.
</para>
</listitem>
<listitem>
<para>
systemd performs scaling of configured weights (to adjust for narrower BFQ
weight range), hence the resulting throughput ratios also differ.
</para>
</listitem>
<listitem>
<para>
The writeback activity depends on the amount of dirty pages, besides the
global sysctl knobs (<filename>vm.dirty_background_ratio</filename> and
<filename>vm.dirty_ratio</filename>)). Memory limits of individual cgroups
come into play when the dirty limits are distributed among cgroups, and
this in turn may affect I/O intensity of affected cgroups.
</para>
</listitem>
<listitem>
<para>
Not all storages are equal. The I/O control happens at the I/O scheduler
layer, which has ramifications for setups with devices stacked on these
that do no actual scheduling. Consider device mapper logical volumes
spanning multiple physical devices, MD RAID, or even Btrfs RAID.
I/O control over such setups may be challenging.
</para>
</listitem>
<listitem>
<para>
There is no separate setting for proportional I/O control of reads and
writes.
</para>
</listitem>
<listitem>
<para>
Proportional I/O control is only one of the policies that can interact with
each other (but responsible resource design perhaps avoids that).
</para>
</listitem>
<listitem>
<para>
The I/O device bandwidth is not the only shared resource on the I/O path.
Global filesystem structures are involved, which is relevant
when I/O control is meant to guarantee certain bandwidth; it won’t and it
may even lead to priority inversion (prioritized cgroup waiting for a
transaction of slower cgroup).
</para>
</listitem>
<listitem>
<para>
So far, we have been discussing only explicit I/O of filesystem data, but
swap-in and swap-out can also be controlled. Although if such a need
arises, it usually points out to improperly provisioned memory (or memory limits).
</para>
</listitem>
</itemizedlist>
</sect2>
</sect1>
<!-- END NEW VERSION -->

 <sect1>
  <title>さらなる情報</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>カーネルのドキュメンテーション (<systemitem>kernel-source</systemitem> パッケージ内):  <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v1</filename> および <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst</filename> の各ファイル</para>
   </listitem>
   <listitem>
    <para>プロパティの一覧と詳細については、 <command>man systemd.resource-control</command> で表示されるマニュアルページをお読みください。</para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/604609/"/>: Brown, Neil: Control Groups Series (2014 年, 7 部構成)</para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/243795/"/>: Corbet, Jonathan: Controlling memory use in containers (2007 年)</para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/236038/"/>: Corbet, Jonathan: Process containers (2007 年)</para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
