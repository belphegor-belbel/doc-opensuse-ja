<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
<!-- custom prompt entities for the 'Controlling I/O with Proportional Weight Policy' section --><!ENTITY reader1 "reader1:/io-cgroup #">
<!ENTITY prompt.reader1 "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[reader1] host1:/io-cgroup # </prompt>">
<!ENTITY prompt.reader2 "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[reader2] host1:/io-cgroup # </prompt>">
<!ENTITY prompt.io-controller "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/io-cgroup # </prompt>">
<!ENTITY prompt.io-cgroup "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>host1:/io-cgroup # </prompt>">
<!ENTITY prompt.plain-root "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>host1:~ # </prompt>">
<!ENTITY prompt.blkio "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>">
<!ENTITY prompt.unified "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/sys/fs/cgroup/unified # </prompt>">
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-tuning-cgroups">

 <title>カーネルコントロールグループ</title>
 <info>
  <abstract>
   <para>カーネルコントロールグループ ( <quote>cgroups</quote> ) は、プロセスに対してハードウエアやシステムの資源を割り当てたり、制限したりするための仕組みです。この機能を利用することで、プロセスをツリー構造で管理することができるようになります。</para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-tuning-cgroups-overview">
  <title>概要</title>
  <para>それぞれのプロセスは正確に 1 つの管理用 cgroup に割り当てられます。 cgroup は階層構造型のツリー (木構造) として管理するもので、その構造の任意の箇所 (枝) もしくは 1 つのプロセスに対して、 CPU やメモリ、ディスクの I/O やネットワーク帯域などのリソース制限を割り当てます。</para>
  <para>&productname; では、 &systemd; が cgroup を利用してグループ内の全てのプロセスを管理しています。この場合、 &systemd; はグループをスライスと呼んでいます。 &systemd; には、 cgroup の設定を行なうためのインターフェイスも用意されています。</para>
  <para><command>systemd-cgls</command> コマンドでは、階層構造を表示することができます。</para>
  <para>本章は概要のみを説明しています。詳しい説明については、列挙されている参照先をお読みください。</para>
 </sect1>

 <sect1 xml:id="sec-tuning-cgroups-accounting">
  <title>Resource accounting</title>
  <para>
  The placement of processes into different cgroups can be used to obtain
  per-cgroup information of certain resource consumptions.
  </para>
  <para>
  The accounting has relatively small but non-zero overhead whose impact
  depends on the workload.
  Be aware that turning on accounting for one unit will also implicitly turn it
  on for all units directly contained in the same slice and for all its parent
  slices and the units directly contained therein.
  Therefore the accounting cost is not exclusive to the single unit.
  </para>
  <para>
  The accounting can be set on per-unit basis with directives such as
  <literal>MemoryAccounting=</literal> or globally for all units in
  <filename>/etc/systemd/system.conf</filename> with respective directive
  <literal>DefaultMemoryAccounting=</literal>.
  Refer to <literal>systemd.resource-control (5)</literal> for the exhaustive
  list of possible directives.
  </para>
 </sect1>

 <sect1 xml:id="sec-tuning-cgroups-usage">
  <title>リソース制限の設定</title>
  <note>
    <title>暗黙のリソース消費について</title>
    <para>暗黙のうちに消費され、実行環境によって異なるリソースが存在することに注意してください。これにはたとえば、ライブラリやカーネル内のデータ構造のほか、利用しているユーティリティの fork() 処理の振る舞い、計算の効率性などがあります。このようなことから、実行環境を変えた場合は、リソース制限を再計算する必要があります。</para>
  </note>
  <para><literal>cgroup</literal> に対する制限は、 <command>systemctl set-property</command> コマンドで設定します。書式は下記のとおりです:</para>
  <screen>&prompt.root;<command>systemctl set-property [--runtime] <replaceable>名前</replaceable> <replaceable>プロパティ_1</replaceable>=<replaceable>値</replaceable> [<replaceable>プロパティ_2</replaceable>=<replaceable>値</replaceable>]</command></screen>
  <para>必要であれば <option>--runtime</option> オプションを指定することもできます。このオプションを指定すると、再起動後には指定した制限が適用されなくなります。</para>
  <para>なお、 <replaceable>名前</replaceable> には &systemd; のスライス名やスコープ名、ソケット名やマウント名、スワップ名を指定します。また、プロパティには下記のものがあります:</para>
  <variablelist>
   <varlistentry>
    <term><literal>CPUQuota=</literal> <replaceable>パーセント値</replaceable></term>
    <listitem>
     <para>プロセスに対して CPU 時間の割り当てを行ないます。この値はパーセント単位で指定するため、末尾に <literal>%</literal> を付けて指定します。この設定を行なうと、 <literal>CPUAccounting=yes</literal> が設定されたものとして扱われます。</para>
     <para>例:</para>
     <screen>&prompt.root;<command>systemctl set-property user.slice CPUQuota=50%</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryLow=</literal> <replaceable>容量</replaceable></term>
    <listitem>
     <para>プロセスからの未使用メモリが指定した容量より少ない場合、メモリを他の用途に再利用しないようにします。 <replaceable>容量</replaceable> の値には K (キロ), M (メガ), G (ギガ), T (テラ) の各接頭辞を使用することができます。この設定を行なうと、 <literal>MemoryAccounting=yes</literal> が設定されたものとして扱われます。</para>
     <para>例:</para>
     <screen>&prompt.root;<command>systemctl set-property nginx.service MemoryLow=512M</command></screen>
     <note>
      <title>統合型のコントロールグループ階層構造について</title>
      <para>この設定は、統合型のコントロールグループ階層構造を使用している場合にのみ利用することができます。また、 <option>MemoryLimit=</option> の設定が無効化されます。統合型のコントロールグループ階層構造を使用するには、 &grub; ブートローダのカーネルコマンドラインパラメータに対して、 <option>systemd.unified_cgroup_hierarchy=1</option> を追加してください。なお、 &grub; の設定に関する詳細は、 <xref linkend="cha-grub2"/> をお読みください。</para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryHigh=</literal> <replaceable>容量</replaceable></term>
    <listitem>
     <para>この制限以上にメモリを使用した場合、プロセスからできる限り積極的にメモリを取り除こうとする動きをします。 <replaceable>容量</replaceable> の値には K (キロ), M (メガ), G (ギガ), T (テラ) の各接頭辞を使用することができます。この設定を行なうと、 <literal>MemoryAccounting=yes</literal> が設定されたものとして扱われます。たとえば下記のようになります:</para>
     <screen>&prompt.root;<command>systemctl set-property nginx.service MemoryHigh=2G</command></screen>
     <note>
      <title>統合型のコントロールグループ階層構造について</title>
      <para>この設定は、統合型のコントロールグループ階層構造を使用している場合にのみ利用することができます。また、 <option>MemoryLimit=</option> の設定が無効化されます。統合型のコントロールグループ階層構造を使用するには、 &grub; ブートローダのカーネルコマンドラインパラメータに対して、 <option>systemd.unified_cgroup_hierarchy=1</option> を追加してください。なお、 &grub; の設定に関する詳細は、 <xref linkend="cha-grub2"/> をお読みください。</para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryMax=</literal> <replaceable>容量</replaceable></term>
    <listitem>
     <para>メモリの最大値を設定することができます。プロセスがこの値を超えてメモリを確保した場合、プロセスは kill されます。 <replaceable>容量</replaceable> の値には K (キロ), M (メガ), G (ギガ), T (テラ) の各接頭辞を使用することができます。この設定を行なうと、 <literal>MemoryAccounting=yes</literal> が設定されたものとして扱われます。</para>
     <para>例:</para>
     <screen>&prompt.root;<command>systemctl set-property nginx.service MemoryMax=4G</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>DeviceAllow=</literal></term>
    <listitem>
     <para>読み込み ( <literal>r</literal> ), 書き込み ( <literal>w</literal> ), mknod ( <literal>m</literal> ) のアクセスを許可します。このコマンドの場合、デバイスノードの指定と、スペースを入れて <literal>r</literal> , <literal>w</literal>, <literal>m</literal> の一覧を指定する必要があります。</para>
     <para>例:</para>
     <screen>&prompt.root;<command>systemctl set-property system.slice DeviceAllow="/dev/sdb1 r"</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>DevicePolicy=</literal> <option>[auto|closed|strict]</option></term>
    <listitem>
     <para><literal>strict</literal> に設定した場合、 <literal>DeviceAllow</literal> に列挙したデバイスにのみアクセスを許可するようになります。 <literal>closed</literal> を指定すると、 <filename>/dev/null</filename> , <filename>/dev/zero</filename> , <filename>/dev/full</filename> , <filename>/dev/random</filename> , <filename>/dev/urandom</filename> などの標準疑似デバイスにもアクセスを許可するようになります。 <literal>auto</literal> を設定した場合は、 <literal>DeviceAllow</literal> でルールが設定されない場合、全てのデバイスへのアクセスが許可されるようになります。既定値は <literal>auto</literal> です。</para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>プロパティに対する詳細と完全な一覧については、 <command>man systemd.resource-control</command> で表示されるマニュアルページをお読みください。</para>
 </sect1>
 
 <sect1 xml:id="sec-tuning-cgroups-tasksmax">
  <title>TasksMax を利用した fork ボムの防止</title>
   <para>&systemd; 228 では <literal>DefaultTasksMax</literal> の制限が 512 に設定されています。これは systemd の各ユニットに対して、作成できるプロセス数の制限を行なうための仕組みで、それ以前のバージョンでは制限がありませんでした。このように設定を行なったのは、 fork によって作成される過剰なプロセスやスレッドによってシステム資源が枯渇してしまうのを防ぐ、セキュリティ改善のためのものです。</para> 
   <para>しかしながら、この既定値は全ての用途に対して適用可能なものではありません。また、 CPU やメモリの使用量に対する直接的な制限ではありませんので、 512 に設定してもシステムの安定性を保つことができるかどうかはわかりませんし、データベースシステムなどのように 512 個以上のプロセスを動作させなければならない場合もあります。 &systemd; 234 では、この値が 15% (カーネル側に設定されたプロセス数の最大値である 32768 の 15% 。詳しくは <command>cat /proc/sys/kernel/pid_max</command> をご覧ください) に設定されていますが、必要に応じて設定ファイルで変更することができます。具体的には、 <filename>/etc/systemd/system.conf</filename> ファイルを編集して設定を行なってください。なお、下記ではそれ以外の方法について説明しています。</para>

   <sect2 xml:id="sec-tasksmax-defaults">
    <title>現時点での既定の TasksMax 値の検出</title>
    <para>&productname; では、 systemd のユニットやユーザスライスに対する提供元の既定値を上書きするため、独自の設定ファイルが 2 つ用意され、いずれも <literal>infinity</literal> に設定されています。 <filename>/usr/lib/systemd/system.conf.d/20-suse-defaults.conf</filename> には、下記のような設定が書かれています:</para>
<screen>
[Manager]
DefaultTasksMax=infinity
</screen>
    <para>もう 1 つの存在である <filename>/usr/lib/systemd/system/user-.slice.d/20-suse-defaults.conf</filename> には、下記のような設定が書かれています:</para>
<screen>[Slice]
TasksMax=infinity
</screen>
    <para><literal>infinity</literal> は無制限の意味です。特に要件がなければ既定値を変更する必要はありませんが、必要であれば設定を変更してください。</para>
  </sect2>

  <sect2 xml:id="sec-edit-taskmax-default">
   <title>DefaultTasksMax 値の設定</title>
   <para>グローバルな <literal>DefaultTasksMax</literal> の値を変更したい場合は、設定を上書きするための新しい設定ファイル <filename>/etc/systemd/system.conf.d/90-system-tasksmax.conf</filename> を作成して対応してください。この設定ファイルには、下記のような内容を記述します (下記の例では、 systemd のユニットごとに最大 256 個までのタスク制限を設定します):</para>
<screen>
[Manager]
DefaultTasksMax=256
</screen>
  <para>新しい設定を読み込んで、設定が反映されたことを確認します:</para>
<screen>&prompt.sudo;systemctl daemon-reload
&prompt.user;systemctl show --property DefaultTasksMax
DefaultTasksMax=256
</screen>
  <para>設定値はお使いのシステムの要件に合わせて指定してください。また、特定のサービスに限定して制限を高くすることもできます。たとえば MariaDB で設定を変更したい場合、まずは現在の設定値を確認します:</para>
<screen>
&prompt.user;systemctl status mariadb.service
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset&gt;
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
  <para><quote>Tasks</quote> 以下には現在動作中のタスク数 (30 個) と上限 (256 個) が示されています。負荷の高いデータベースシステムとしては不十分な値であることから、たとえば MariaDB のみを 8192 個までに拡大してみることにします。設定値を変更するには、 <command>systemctl edit</command> コマンドを実行して、下記のような設定を記入します:</para>
<screen>&prompt.sudo;systemctl edit mariadb.service

[Service]
TasksMax=8192

&prompt.user;systemctl status mariadb.service 
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab&gt;
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─override.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta&gt;
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta&gt;
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para><command>systemctl edit</command> コマンドは <filename>/etc/systemd/system/mariadb.service.d/override.conf</filename> という名前の上書き用設定ファイルを作成します。ここには既存のユニットファイルに対する上書き値のみが保存されます。もちろん 8192 でなくてもかまいません。お使いの要件に合わせて設定してください。</para>
  </sect2>

  <sect2>
   <title>ユーザに対する既定の TasksMax 制限</title>
   <para>ユーザに対する既定の制限値は十分に高く設定されています。これは、ユーザセッションではより多くのリソースを必要とするためです。独自の制限を設定したい場合は、 <filename>/etc/systemd/system/user-.slice.d/user-taskmask.conf</filename> のような設定ファイルを作成し、その中に設定値を記述してください。下記の例では、タスクの最大値を 16284 に設定しています:</para>
<screen>
[Slice]
TasksMax=16284
</screen>
   <para>あとは systemd に対して設定値の再読み込みを指示し、設定が変更されたことを確認します:</para>
<screen>&prompt.sudo;systemctl daemon-reload

&prompt.user;systemctl show --property TasksMax user-.slice
TasksMax=16284

&prompt.user;systemctl show --property TasksMax user-1000.slice
TasksMax=16284
</screen>
   <para>具体的にどのような設定値にすべきかについては、システムの用途と搭載されているリソースのほか、他のリソース設定によっても異なります。 TasksMax の値が低すぎる場合は、<emphasis>Failed to fork (Resources temporarily unavailable)</emphasis> (fork に失敗した (リソースが一時的に利用できなくなっている)) や <emphasis>Can't create thread to handle new connection</emphasis> (新しい接続を処理するためのスレッドが作成できない), <emphasis>Error: Function call 'fork' failed with error code 11, 'Resource temporarily unavailable'</emphasis> (エラーコード 11 (リソースが一時的に利用できなくなっている) で fork の関数呼び出しが失敗した) などのエラーが発生します。</para>
   <para>systemd でのシステムリソースの制限の設定方法について、詳しくは<literal>systemd.resource-control (5)</literal> をお読みください。</para>
  </sect2>
 </sect1>

 <sect1>
  <title>比例型の重み付けポリシーによる I/O の制御</title>
  <para>本章では、 Linux カーネル内に存在する I/O コントローラを利用して、 I/O 処理の優先付けを行なうための方法について説明しています。 cgroup blkio サブシステムでは、ブロックデバイスに対する I/O 処理を制御したり監視したりすることができるほか、 cgroup の仮想ファイルシステム (擬似ファイルシステムとも呼ばれます) 内のファイルとして、 cgroup 向けのサブシステムパラメータを含むステートオブジェクトが提供されています。</para>
  <para>本章では、アクセスや帯域を制限するための擬似ファイルへの書き込み方法のほか、 I/O 処理の関する様々な情報を提供する擬似ファイルの読み込み方法を説明しています。また、 cgroup-v1 と cgroup-v2 の両方に対する例を示しています。</para>
  <para>まずはテスト用のディレクトリを作成し、その中にファイルを 2 つ作成して性能のテストを行ないます。ある程度のサイズのファイルを作成するため、本章では <command>yes</command> コマンドを使用します。下記の例ではテストディレクトリを作成し、その中に 537MB のテキストファイルを作成しています:</para> 
  <screen>&prompt.plain-root;mkdir /io-cgroup
&prompt.plain-root;cd /io-cgroup
&prompt.plain-root;yes this is a test file | head -c 537MB &gt; file1.txt
&prompt.plain-root;yes this is a test file | head -c 537MB &gt; file2.txt
</screen>
  <para>例を実行するために 3 つのコマンドシェルを開いてください。 2 つのシェルは読み込み側のプロセスを動作させるもので、残りの 1 つは I/O を制御するためのものです。下記の例では行頭に reader1/reader2 (読み込み側), io-controller (制御側) としてラベルを示しています。</para>

  <sect2>
   <title>cgroup-v1 での例</title>
   <para>下記の比例型重み付けポリシーファイルは読み込み側のプロセスに対して作用するもので、他のプロセスよりも優先的にアクセスができるようにするためのものです。</para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para><filename>blkio.weight</filename> (カーネルバージョン 4.20 もしくはそれ以前の古いブロックレイヤを使用していて、かつ CFQ I/O スケジューラを使用する場合にのみ利用できるファイルです)</para>
    </listitem>
    <listitem>
     <para><filename>blkio.bfq.weight</filename> (カーネルバージョン 5.0 もしくはそれ以降の blk-mq を使用していて、かつ BFQ I/O スケジューラを使用する場合にのみ利用できるファイルです)</para>
    </listitem>
   </itemizedlist>
   <para>まずはシェルのうちの 1 つを利用して、 I/O 制御を行なわない場合の <filename>file2.txt</filename> の読み込み性能を調べてみます:</para>
<screen> 
&prompt.io-controller;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.io-controller;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s
</screen>
   <para>次に、同じディスクに対して読み込み処理を同時に 2 つ動作させます:</para>
<screen>
&prompt.reader1;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s
</screen>
   <para>上記のとおり、 I/O 性能はおおよそ半分になっていることがわかります。次に、それぞれの読み込み側プロセスに適用できるよう、 2 種類の制御グループを作成します。このとき、 BFQ が適用されていることを確認しておいてください。また、 reader2 に対して異なる重み付けを行ないます:</para>
<screen>
&prompt.io-controller;cd /sys/fs/cgroup/blkio/
&prompt.blkio;mkdir reader1
&prompt.blkio;mkdir reader2
&prompt.blkio;echo 5220 &gt; reader1/cgroup.procs
&prompt.blkio;echo 5251 &gt; reader2/cgroup.procs
&prompt.blkio;cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
&prompt.blkio;cat reader1/blkio.bfq.weight
100
&prompt.blkio;echo 200 &gt; reader2/blkio.bfq.weight
&prompt.blkio;cat reader2/blkio.bfq.weight
200
</screen>
   <para>上記の設定で reader1 と reader2 を動作させると、 reader2 に対して優先的に性能が割り当てられるようになります:</para>
<screen>
&prompt.reader1;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s
</screen>
   <para>reader2 に対する重み付けの値を大きくすればするほど、より優先的に性能が割り当てられるようになります。さらに値を倍にしてみます:</para>
<screen>
&prompt.blkio;cat reader1/blkio.bfq.weight
100
&prompt.blkio;echo 400 &gt; reader2/blkio.bfq.weight
&prompt.blkio;cat reader2/blkio.bfq.weight
400
</screen>
   <para>これにより、 reader2 側がさらに高い性能を得るようになります:</para>
<screen>
&prompt.reader1;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s
</screen>
  </sect2>
  <sect2>
   <title>cgroup-v2 での例</title>
   <para>まずは前の章の手順に従ってテスト環境を準備してください。</para>
   <para>次にブロック IO コントローラ (cgroup-v1) の動作を停止します。これを行なうには、カーネルのパラメータに <option>cgroup_no_v1=blkio</option> を追加します。このパラメータが使用されていることを確認したあと、 IO コントローラ (cgroup-v2) が利用できるかどうかを確認します:</para>
<screen>
&prompt.io-controller;cat /proc/cmdline
BOOT_IMAGE=... cgroup_no_v1=blkio ...
&prompt.io-controller;cat /sys/fs/cgroup/unified/cgroup.controllers
io
</screen>
 <para>あとは IO コントローラを有効化します:</para>
<screen>
&prompt.io-controller;cd /sys/fs/cgroup/unified/
&prompt.unified;echo '+io' &gt; cgroup.subtree_control
&prompt.unified;cat cgroup.subtree_control
io
</screen>
<para>あとは cgroup-v1 と同じような手順を実行するだけです。ただし、ディレクトリのうちのいくつかが異なることに注意してください。まずはシェルのうちの 1 つを利用して、 I/O 制御を行なわない場合の <filename>file2.txt</filename> の読み込み性能を調べてみます (この例では、 SSD から読み込みを行なっています):</para>
<screen>
&prompt.unified;cd -
&prompt.io-controller;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.io-controller;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5633
[...]
</screen>
   <para>次に、同じディスクに対して読み込み処理を同時に 2 つ動作させて性能を確認します:</para>
<screen>
&prompt.reader1;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5633
[...]
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5703
[...]
</screen>
<para>上記のとおり、 I/O 性能はおおよそ半分になっていることがわかります。次に、それぞれの読み込み側プロセスに適用できるよう、 2 種類の制御グループを作成します。このとき、 BFQ が適用されていることを確認しておいてください。また、 reader2 に対して異なる重み付けを行ないます:</para>
<screen>
&prompt.io-controller;cd -
&prompt.unified;mkdir reader1
&prompt.unified;mkdir reader2
&prompt.unified;echo 5633 &gt; reader1/cgroup.procs
&prompt.unified;echo 5703 &gt; reader2/cgroup.procs
&prompt.unified;cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
&prompt.unified;cat reader1/io.bfq.weight
default 100
&prompt.unified;echo 200 &gt; reader2/io.bfq.weight
&prompt.unified;cat reader2/io.bfq.weight
default 200
</screen>
   <para>上記の設定で reader1 と reader2 を動作させると、 reader2 に対して優先的に性能が割り当てられるようになります:</para>
<screen>
&prompt.reader1;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1 of=/dev/null bs=4k
5633
[...]
&prompt.reader2;echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5703
[...]
</screen>
<para>reader2 に対する重み付けの値を大きくすればするほど、より優先的に性能が割り当てられるようになります。さらに値を倍にしてみます:</para>
<screen>
&prompt.reader2;echo 400 &gt; reader1/blkio.bfq.weight
&prompt.reader2;cat reader2/blkio.bfq.weight
400
&prompt.reader1;sync; echo 3 &gt; /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
[...]
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
[...]
</screen>
  </sect2>
  </sect1>
  
 <sect1>
  <title>さらなる情報</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>カーネルのドキュメンテーション (<systemitem>kernel-source</systemitem> パッケージ内):  <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v1</filename> および <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst</filename> の各ファイル</para>
   </listitem>
   <listitem>
    <para><link xlink:href="http://lwn.net/Articles/604609/"/>: Brown, Neil: Control Groups Series (2014 年, 7 部構成)</para>
   </listitem>
   <listitem>
    <para><link xlink:href="http://lwn.net/Articles/243795/"/>: Corbet, Jonathan: Controlling memory use in containers (2007 年)</para>
   </listitem>
   <listitem>
    <para><link xlink:href="http://lwn.net/Articles/236038/"/>: Corbet, Jonathan: Process containers (2007 年)</para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
